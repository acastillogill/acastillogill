---
title: "Regression trees: predicting property prices"
author: √Ångela Castillo-Gill
date: '2018-08-09'
slug: regression-trees-predicting-property-prices
categories:
  - R
  - RPART
tags: 
  - Kaggle
  - Home sales prices
  - Decision trees
  - Regression trees
draft: FALSE
summary: "What are features are closely linked with sales price?"
header: 
  image: "headers/final-plot-1.png"
  caption: "Regression tree to estimate property values. "
output:
  blogdown::html_page:
    toc: true
    number_sections: true
    toc_depth: 2
  fig_caption: true
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
x <-
  c("tidyverse",
    "knitr",
    "formatR",
    "stringr",
    "lubridate",
    "tidyr",
    "formattable",
    "grid",
    "gridExtra",
    "kableExtra",
    "here",
    "corrplot",
    "rms",
    "dummies",
    "sjmisc",
    "car",
    "DescTools",
    "gvlma",
    "MASS",
    "QuantPsyc",
    "Hmisc",
    "GGally",
    "lm.beta",
    "MuMIn",
    "broom",
    "corrplot",
    "rpart",
    "rpart.plot",
    "Metrics",
    "caret",
    "psych")

lapply(x, require, character.only = TRUE)


opts_chunk$set(echo = FALSE,
               warning = FALSE,
               error = FALSE,
               message = FALSE,
               collapse= TRUE,
               comment = NA,
               tidy = TRUE,
               cache=FALSE)
options(scipen = 999)
options(digits=4)
theme_set(theme_light())

```

# Summary

*To see the code used in this post, visit my [kernel on kaggle in R Markdown format](https://www.kaggle.com/adcastillogill/regression-trees-predicting-property-prices/code).* 

In this post I explore a property dataset from Ames, Iowa. The data describes a set of features for houses. My goal was to understand what characteristics make a house more valuable than others in Ames, Iowa. To do this, I first prepared the data by dealing with missing values in the dataset and created other variables to make results easier to understand. After preparing the data, I used regression trees to answer the question. The benefits of regression trees are that they produce an output that can be illustrated and easily interpreted. I found that the overall quality of properties is the most closely linked characteristic with sale prices. Other features such as living area and basement size were also key. An interesting finding was that there are three neighborhoods that have the most expensive houses. These are NorthRidge Heights, Northridge and, Stone Brook. 

# Question

Let's say that you have a house in Ames, Iowa [^2]. You're about to buy a house or some terrain to build your own house and you want to understand **what makes a house more valuable than others**. 

[^2]:That's where the dataset is from.

# Dataset description 





```{r reading data, results='hide'}
path <- here::here("static","data","property_train.csv")
#Read `train.csv` as homes
homes <- read.csv(path, stringsAsFactors = FALSE)
```


```{r counting variable types}
#is.numeric returns TRUE if the variable is numeric. 
#sapply iterates and returns a vector. 
#Which gives the indices that were TRUE
num_var <- which(sapply(homes, is.numeric))
#Count how many variables are numeric
length_num_var <- length(num_var)
#Return a vector with character variables
car_var <- which(sapply(homes, is.character))
#Count how many variables are characters
length_car_var <- length(car_var)
```

The dataset [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) was put together by [Dean De Cock.](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf) It has 79 explanatory variables describing 1,460 homes in Ames, Iowa. The codebook for all the variables can be [found here.](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) As I go along, I'll explain the most relevant ones. The dataset has `r length_num_var` numeric and `r length_car_var` character variables. 

## Missing values

As is more common than not, the dataset contains missing values. Missing values need to be dealt with because often regression (and other models) require complete observations. Dealing with missing data depends on *why the data are missing*. [This article](http://www.stat.columbia.edu/~gelman/arm/missing.pdf) explains four reasons why data could be missing. When the data are missing at random (MAR) or completely at random (MCAR), observations with missing values can be removed without introducing bias into the model. Sometimes, however, if the dataset is not too big and we don't want to lose observations, or even if it is big, yet we still don't want to remove observations, we can impute data. Imputing means replacing missing values by taking some educated guesses. [This article](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4) summarises how to impute data depending on why it is missing. If the data are not missing at random, then the imputation mechanism has to be modelled. 

```{r missigness-table}

#Which variables have missingness in data
homes%>%
  dplyr::select_if(function (x) any(is.na(x)))%>%
  dplyr::summarise_all(funs(sum(is.na(.))))%>%
  gather()%>%
  arrange(desc(value))->missing_columns

```

About `r nrow(missing_columns)` variables have missing values - based on the codebook, the reason why so many houses have pool quality (`PoolQC`) missing is because `NA`, means there is no pool. Since this variable is ordinal, I can revalue it to make it numerical and `0` will mean the property has no pool. Other features (`MiscFeature`), `Alley`, `Fence`, and fireplace quality (`FireplaceQu`) are missing because of similar reasons. We don't know why `LotFrontage` is missing but we will impute as the median for properties in the same neighborhood. I learned a lot about imputation and missing values from [Erik Bruin's kernel on Kaggle](https://www.kaggle.com/erikbruin/house-prices-lasso-xgboost-and-a-detailed-eda/code).

```{r missingness and recoding ordinal variables, results='hide'}

#Vector for ordinal values
ordinal_scale <- c("Ex"=5, "Gd"=4, "TA"=3, "Fa"=2, "Po"=1, "No"=0)

#Missingness PoolQC
homes$PoolQC[is.na(homes$PoolQC)] <- "No"

#Recode PoolQC
homes$PoolQC<-as.integer(plyr::revalue(homes$PoolQC, ordinal_scale))

#Missingness MiscFeature
homes$MiscFeature[is.na(homes$MiscFeature)] <- "No"

#Recode MiscFeature
homes$MiscFeature <- as.factor(homes$MiscFeature)

#Missingness Alley
homes$Alley[is.na(homes$Alley)] <- "No"

#Recode Alley
homes$Alley <- as.factor(homes$Alley)

#Missingness Fence
homes$Fence[is.na(homes$Fence)] <- "No"

#Checking if Fence is ordinal
homes%>%
  filter(!is.na(SalePrice))%>%
  group_by(Fence) %>%
  dplyr::summarise(median = median(SalePrice), counts=n())

#Fence is not ordinal

#Recode Fence
homes$Fence <- as.factor(homes$Fence)

#Missingness FireplaceQu
homes$FireplaceQu[is.na(homes$FireplaceQu)] <- "No"

#Checking if FireplaceQu is ordinal
homes%>%
  filter(!is.na(SalePrice))%>%
  group_by(FireplaceQu) %>%
  dplyr::summarise(median = median(SalePrice), counts=n())

#Recode FireplaceQu
homes$FireplaceQu<-as.integer(plyr::revalue(homes$FireplaceQu, ordinal_scale))


#LotFrontage is the linear feet of street connnected to property
#Missing values will be replaced by neighborhood average

for (i in 1:nrow(homes)){
        if(is.na(homes$LotFrontage[i])){
               homes$LotFrontage[i] <-
                 as.integer(median(homes$LotFrontage[homes$Neighborhood==homes$Neighborhood[i]], na.rm=TRUE)) 
        }
}

#Missingness GarageType
homes$GarageType[is.na(homes$GarageType)] <- "No"

#Recode GarageType
homes$GarageType <- as.factor(homes$GarageType)

#Checking if GarageFinish is ordinal
homes%>%
  filter(!is.na(SalePrice))%>%
  group_by(GarageFinish) %>%
  dplyr::summarise(median = median(SalePrice), counts=n())

#It is ordinal

#Missingness GarageFinish
homes$GarageFinish[is.na(homes$GarageFinish)] <- "No"

#GarageFinish ordinal vector
Finish <- c('No'=0, 'Unf'=1, 'RFn'=2, 'Fin'=3)

#Recode GarageFinish
homes$GarageFinish<-as.integer(plyr::revalue(homes$GarageFinish, Finish))
table(homes$GarageFinish)

#Missingness GarageQual
homes$GarageQual[is.na(homes$GarageQual)] <- "No"

#Recode GarageQual
homes$GarageQual<-as.integer(plyr::revalue(homes$GarageQual, ordinal_scale))

#Missingness GarageCond
homes$GarageCond[is.na(homes$GarageCond)] <- "No"

#Recode GarageCond
homes$GarageCond<-as.integer(plyr::revalue(homes$GarageCond, ordinal_scale))


#Missingness GarageYrBlt will be substituted for YearBuilt

#For all rows in homes
for (i in 1:nrow(homes)){
  #If observation i of column GarageYrBlt is NA
        if(is.na(homes$GarageYrBlt[i])){
          #Change observation for observation i of column YearBuilt
               homes$GarageYrBlt[i] <- homes$YearBuilt[i] 
        }
}

#Checking if BsmtExposure is ordinal
homes%>%
  filter(!is.na(SalePrice))%>%
  group_by(BsmtExposure) %>%
  dplyr::summarise(median = median(SalePrice), counts=n())

#Missingness BsmtExposure
homes$BsmtExposure[is.na(homes$BsmtExposure)] <- "None"

#It is ordinal, create vector substitute vector
exposure_ordinal <- c("Gd"= 4,"Av"= 3,"Mn"=2,"No"=1, "None"=0)

#Recode BsmtExposure
homes$BsmtExposure<-as.integer(plyr::revalue(homes$BsmtExposure, exposure_ordinal))
table(homes$BsmtExposure)

#Checking if BsmtFinType2 is ordinal
homes%>%
  filter(!is.na(SalePrice))%>%
  group_by(BsmtFinType2) %>%
  dplyr::summarise(median = median(SalePrice), counts=n())

#Missingness BsmtFinType2
homes$BsmtFinType2[is.na(homes$BsmtFinType2)] <- "No"

#It is ordinal, create vector substitute vector
FinType_ordinal <- c('No'=0, 'Unf'=1, 'LwQ'=2, 'Rec'=3, 'BLQ'=4, 'ALQ'=5, 'GLQ'=6)

#Recode BsmtFinType2
homes$BsmtFinType2<-as.integer(plyr::revalue(homes$BsmtFinType2, FinType_ordinal))
table(homes$BsmtFinType2)

#Missingness BsmtQual
homes$BsmtQual[is.na(homes$BsmtQual)] <- "No"

#Recode BsmtQual
homes$BsmtQual<-as.integer(plyr::revalue(homes$BsmtQual, ordinal_scale))

#Missingness BsmtCond
homes$BsmtCond[is.na(homes$BsmtCond)] <- "No"

#Recode BsmtCond
homes$BsmtCond<-as.integer(plyr::revalue(homes$BsmtCond, ordinal_scale))

#Missingness BsmtFinType1
homes$BsmtFinType1[is.na(homes$BsmtFinType1)] <- "No"

#Recode BsmtFinType1
homes$BsmtFinType1<-as.integer(plyr::revalue(homes$BsmtFinType1, FinType_ordinal))
table(homes$BsmtFinType1)

#Checking if MasVnrType is ordinal
homes%>%
  filter(!is.na(SalePrice))%>%
  group_by(MasVnrType) %>%
  dplyr::summarise(median = median(SalePrice), counts=n())

#Missingness MasVnrType
homes$MasVnrType[is.na(homes$MasVnrType)] <- "None"

#Create ordinality vector
mas_ordinality <- c('None'=0, 'BrkCmn'=0, 'BrkFace'=1, 'Stone'=2)

#Recode MasVnrType
homes$MasVnrType<-as.integer(plyr::revalue(homes$MasVnrType, mas_ordinality))
table(homes$MasVnrType)

#Missingness MasVnrArea
homes$MasVnrArea[is.na(homes$MasVnrArea)] <- 0

#Missingness Electrical
homes%>%
  group_by(Electrical)%>%
  dplyr::count()%>%
  arrange(desc(n))%>%
  dplyr::select(Electrical)%>%
  head(n=1)->replacement_Electrical

homes$Electrical[is.na(homes$Electrical)] <- unlist(replacement_Electrical)

#Recode Electrical
homes$Electrical <- as.factor(homes$Electrical)

#Recode MSZoning 
homes$MSZoning  <- as.factor(homes$MSZoning)

#Recode MSSubClass
homes$MSSubClass  <- as.factor(homes$MSSubClass)

#Recode Street     
homes$Street  <- as.factor(homes$Street)

#Recode LotShape   

homes$LotShape   <- as.factor(homes$LotShape)

#Recode LandContour   

homes$LandContour   <- as.factor(homes$LandContour)


#Recode Utilities 

homes$Utilities    <- as.factor(homes$Utilities)
#Looking at factors
table(homes$Utilities)
#When looking at levels, all observations except 1, fall into one level. Therefore this variable is not too helpful.
homes$Utilities <- NULL

#Recode LotConfig    

homes$LotConfig     <- as.factor(homes$LotConfig)

#Recode LandSlope  

homes$LandSlope     <- as.factor(homes$LandSlope)

#Recode Neighborhood    

homes$Neighborhood     <- as.factor(homes$Neighborhood)

#Recode Condition1   

homes$Condition1     <- as.factor(homes$Condition1)


#Recode Condition2 

homes$Condition2     <- as.factor(homes$Condition2)

#Recode BldgType    

homes$BldgType      <- as.factor(homes$BldgType)

#Recode HouseStyle 

homes$HouseStyle       <- as.factor(homes$HouseStyle)

#Recode RoofStyle    

homes$RoofStyle       <- as.factor(homes$RoofStyle)

#Recode RoofMatl   

homes$RoofMatl      <- as.factor(homes$RoofMatl)

#Recode Exterior1st 

homes$Exterior1st       <- as.factor(homes$Exterior1st)

#Recode Exterior2nd     

homes$Exterior2nd       <- as.factor(homes$Exterior2nd)

#Recode ExterQual 

homes$ExterQual<-as.integer(plyr::revalue(homes$ExterQual, ordinal_scale))

#Recode ExterCond    

homes$ExterCond<-as.integer(plyr::revalue(homes$ExterCond, ordinal_scale))

#Recode Foundation    

homes$Foundation      <- as.factor(homes$Foundation)

#Recode Heating 

homes$Heating       <- as.factor(homes$Heating)

#Recode HeatingQC    

homes$HeatingQC<-as.integer(plyr::revalue(homes$HeatingQC, ordinal_scale))

#Recode CentralAir   

boolean <- c("Y"=1,"N"=0)

homes$CentralAir<-as.integer(plyr::revalue(homes$CentralAir, boolean))

table(homes$CentralAir)

#Recode KitchenQual   


homes$KitchenQual<-as.integer(plyr::revalue(homes$KitchenQual, ordinal_scale))


#Recode Functional    

homes$Functional      <- as.factor(homes$Functional)

#Recode PavedDrive 

homes$PavedDrive       <- as.factor(homes$PavedDrive)

#Recode SaleType 
homes$SaleType        <- as.factor(homes$SaleType)

#Recode SaleCondition 

homes$SaleCondition     <- as.factor(homes$SaleCondition)
```


```{r create time variables}
homes<-homes%>%
  #Mutate to create `YearsSinceBuilt`, `YearsSinceGarageBuilt`, and `YearsSinceRemod`
  #It will be the difference of the present year - YearBuilt
  mutate(YearsSinceBuilt = year(Sys.Date())-YearBuilt,
         #Same for YearsSinceRemod
         YearsSinceRemod = year(Sys.Date())-YearRemodAdd,
         #Same for GarageYrBlt
         YearsSinceGarageBuilt = year(Sys.Date())-GarageYrBlt)%>%
  #Remove old variables, correlated ones, and ID
  dplyr::select(-GarageYrBlt,
                -YearRemodAdd,
                -YearBuilt,
                -GarageCars,
                -BsmtQual,
                -TotRmsAbvGrd,
                -Id)
```

```{r understanding-neighborhood-var, include=FALSE}
homes%>%
  group_by(Neighborhood)%>%
  summarise(Median=median(SalePrice))%>%
  arrange(desc(Median))%>%
  ggplot(aes(x=reorder(Neighborhood, Median), y=Median))+
  geom_col()+
  labs(title="Neighborhoods vs. Average SalePrice",
       x="Neighborhood")+
   scale_y_continuous(labels = scales::dollar_format(suffix = "", prefix = "$")) + coord_flip()
  
```

```{r binning_neighboorhood}

fancy <- c("NridgHt","NoRidge","StoneBr")
modern <- c("20","60","70","80","85","120")

homes <- homes%>%
  
mutate(Neighborhood_type=ifelse(Neighborhood %in% fancy, "Fancy","Not_fancy"),
       MS_type=ifelse(MSSubClass %in% modern, "Modern","Less_modern"))
                                      
homes <- homes%>%
  dplyr::select(-Neighborhood,
                -MSSubClass)
```

```{r export the data set for posts, eval=FALSE}
#Save RDS
saveRDS(homes, file=here::here("static","data", "homes.rds"))
```


## Correlation

Correlation, $Cor(X,Y)$, measures the strength of the linear relationship between two variables $X$ and $Y$. The correlation between `SalePrice` and another variable, let's say, `OverallQual`, is the covariance of the separately normalised data between the two variables.

```{r covariance-example, echo=TRUE}
cov(scale(homes$SalePrice), scale(homes$OverallQual))
```

Since covariance units are `OverallQual` * `SalePrice`, calculating the correlation is more helpful as is unit free. If we created a model with only one variable as the predictor of `SalePrice`, let's say, kitchen quality (`KitchenQual`) and normalised the data, the regression slope would be the correlation between the two variables.

```{r single-regression-normalised-data, echo=TRUE}
norm_fit <- lm(scale(SalePrice) ~ scale(KitchenQual), data = homes)
round(coefficients(norm_fit), digits = 2)
```

Now I'll look at all variables - here is the correlation matrix for variables that have a relationship stronger than 0.5 with `SalePrice`. 

```{r correlation-matrix, fig.retina=10, fig.cap="Variables are arranged in descending order according to the strength of the relationship with `SalePrice`."}
homes_num <- homes%>%
  select_if(is.numeric)
#Create tidy correlation matrix
cor_homes <- tidy(cor(homes_num))
#Arrange correlations with SalePrice in descending order
high_cor_names <- cor_homes%>%
  arrange(desc(SalePrice))%>%
  filter(abs(SalePrice)>0.5)%>%#Filter variables that have relationship >0.5
  pull(.rownames)#Pull names of variables
#Filter entire correlation matrix for the variables
high_cor <- cor(homes_num)[high_cor_names,high_cor_names]
#Create correlation matrix
corrplot.mixed(high_cor, tl.col="black", tl.pos = "lt",cl.ratio=0.1,number.cex=.6, cl.cex = .6, tl.cex = 0.6)

```

 There are 17 variables that have a correlation stronger than 0.5. When variables are highly correlated amongst each other, it's better to remove some of them as they don't necessarily add additional information and it could lead to multicollinearity. The correlation plot highlights some obvious pairs that are related with another, for example, `GarageArea` and `GarageCars`. Makes sense, a bigger garage can hold more cars. `X1stFlrSF` and `TotalBsmtSF`, the total area of the first floor and basement, this also seems reasonable since basements are underneath the same floor and would tend to have a similar area. `TotRmsAbvGrd` and `GrLivArea`, the total number of rooms and area above ground, again ok, more rooms would be linked to a bigger living area. Finally, `YearsSinceBuilt` and `YearsSinceGarageBuilt` since garages are usually built at the same time as the house.


# Regression trees - why use them?

The tool that I'm going to be using to answer the question is regression trees. They are also known as classification and regression trees (CART) or the recursing and partitioning (RPART) algorithm. The reason I'm choosing this tool or algorithm to answer the business question is because regression trees are interpretable and we can make easy-to-follow plots with them. It will be implemented with the `rpart` package in R. Rpart, builds a model in two stages:

**First stage**:

The variable which can best[^1] split the data into two groups is identified. The data are then separated into two groups and the whole process is repeated *recursively* or indefinitely until the sub-groups reach a minimum size, or until no further improvements can be made. When the split is made, similarity amongst the observations can more or less homogenous. This homogeneity is also called purity and it can be measured. The impurity measure of a node specifies how mixed the resulting subset is.

[^1]: The best tree is the smallest tree with highest cross-validated error.

**Second stage**:

The tree is trimmed back or prunned using cross-validation. We identify the lowest cross-validated error or the smallest within one standard error of the tree with lowest cross-validated error. 




```{r}
homes <- homes%>%
  mutate(SalePrice=SalePrice/1000)
```


```{r split-data, include=FALSE}
# Randomly assign rows to ids (1/2) represents train/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/test split will be approximately 70% / 30%
set.seed(1)
assignment <- sample(1:2, size = nrow(homes),
                     prob = c(0.7,0.3),
                     replace = TRUE)

# Create a train, validation and tests from the original data frame 
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_test <- homes[assignment == 2, ]   # subset the homes data frame to test indices only
```



```{r train the model, include=FALSE}
# Train the model
homes_model <- rpart(formula = SalePrice ~ ., 
                     data = homes_train, 
                     method = "anova")
```

## Variable importance

Using the `rpart` function, we are able to rank which variables are most predictive of `SalePrice`. The following plot ranks these variables in descending order. 

```{r variable-importance-table, fig.cap="Overall quality is the most predictive variable and the one that originally had the most correlation with `SalePrice`, it's followed by basement size and neighborhood type."}
#Look at variable importance

var_importance <- tidy(homes_model$variable.importance)

total_importance <- sum(var_importance$x)

var_importance%>%
  mutate(names=fct_reorder(names,x))%>%
  head(15)%>%
ggplot(aes(names,x/total_importance,fill=names))+
  geom_col()+
  scale_y_continuous(labels = scales::percent_format())+
  coord_flip()+
  labs(title="Top 15 most important variables for predicting `SalePrice`",
       y="",
       x="")+
  theme(legend.position = "none")
  
```


```{r initial-prediction, include=FALSE}
#Computing predicted values 
pred_base <- predict(object=homes_model,
                newdata = homes_test)

#Compute RMSE
rmse(actual=homes_test$SalePrice, #Actual values
     predicted = pred_base ) #Predicted values

#Compute MAE
mae(actual=homes_test$SalePrice, #Actual values
     predicted = pred_base ) #Predicted values
```


```{r tuning-hyperparameters, include=FALSE}
# Plot the "CP Table"
plotcp(homes_model)

# Print the "CP Table"
print(homes_model$cptable)

# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]

# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model, 
                         cp = cp_opt)
                          

```


```{r prediction, include=FALSE}
#Computing predicted values 
pred_opt <- predict(object=homes_model_opt,
                newdata = homes_test)

#Compute RMSE
rmse(actual=homes_test$SalePrice, #Actual values
     predicted = pred_opt) #Predicted values

#Compute MAE
mae(actual=homes_test$SalePrice, #Actual values
    predicted = pred_opt) #Predicted values
```

# Results

```{r final-plot, fig.cap="The tree with 10 nodes and 9 splits had the lowest performance metric `RMSE`."}

par(mar=c(1,1,1,1))
# Plot the optimized model
 rpart.plot(x = homes_model, type = 5, extra = 1, digits = 0)


```

The most influential variable for `SalePrice` was `OverallQual`. This variable "rates the overall material and finish of the house", values equal or above to 8 correspond to "very good", "excellent", and "very excellent". On one hand, for houses with a rating of 8 or above in `OverallQual`, the next most decisive variable is `TotalBsmtSF`, which is the "total square feet of basement area". If it's above 1,850, the house is classified depending on `Neighborhood_type`. Fancy houses are thoses that are in either of these neighborhoods `r fancy`. All other neighborhoods are not fancy. Houses below 1,850 feet are further classified depending on the `LotArea`.

On the other hand, houses with a rating below 8 for `OverallQual`, the same variable decides again classifying them above or equal to 7, which is "good". Either way, houses will be classified again by `GrLivArea`: "above grade living area in square feet", where smaller houses will depend on basement size (`TotalBsmtSF`) or on `MS_type`, which describes the type of dwelling. Modern dwellings include: split foyer, 1-story built in 1946 or newer. Less modern dwellings are 1945 or older in some cases.

In response to the question: what makes a house more valuable than others? Many variables in the tree are related to area: basement area, garage area, and total living area. Even the size of the lot plays an important role and is related to area. So an important one definitely is size. But more important than size in this case, the overall quality of the house is critical. 


# What does it mean?

It seems that having a property in very good condition or better pays a lot. Would it be worth to find a house in a somewhat fancy neighborhood and work on improving its finish and materials to increase quality. Who knows? That is a causal question. There is a strong relationship between `OverallQual` and `SalePrice` but correlation does not imply causation.
Size matters in all property dimensions: garage area, basement area, living area above ground, lot size. These are all important characteristics strongly linked to sales price. 
As a final note it's important to add that to a large extent the results of this analysis are limited Ames, Iowa, where this dataset is from.


