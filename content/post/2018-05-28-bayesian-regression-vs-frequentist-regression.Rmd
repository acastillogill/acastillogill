---
title: Bayesian Regression vs. Frequentist Regression
author: √Ångela Castillo-Gill
date: '2018-05-28'
slug: bayesian-regression-vs-frequentist-regression
categories: []
tags: []
---
```{r setup, include=FALSE}
x <-
  c("tidyverse",
    "bookdown",
    "fcuk",
    "knitr",
    "kableExtra",
    "formatR",
    "gridExtra")

lapply(x, require, character.only = TRUE)

opts_chunk$set(echo = TRUE)

```


So I thought, I need to get to the bottom of this, what is indeed the difference between Frequentist and Bayesian regression beyond the distinction of what probability, as a whole, means to both sides. 

#### Ordinary Least Squared (OLS) Errors Regression 

With OLS, our best estimate of the line that describes the relationship between the explanatory and response variable is ordinary least squared (OLS) estimates of alpha and beta to obtain the "fitted" values of predictions. What are those fitted values? They can also be called the "predicted" values. The equation for the model is:

$$Model: \hat{y_i}=\hat{\alpha}+\hat{\beta_i}*x_i$$

Then we have the residuals which are the difference between the observed and predicted values. 

$$Residuals: \hat{\epsilon_i}=y_i-\hat{y_i}$$

Finally, we have the mean squared error ($MSE$) or $\sigma^2$, which are the sum of the squared errors divided by the degrees of freedom.

$$MSE=\sum\limits_{n=1}^{n} \frac{\hat{\epsilon_i}}{n-2}$$

Squared errors we already know, but the degrees of freedom is the sample size minus the number of regression coefficients in the model. What are the regression coefficients in a model?

#### Bayesian Regression 

In Bayesian regression, the model is similar to OLS, but there is an additional assumption: errors are normally distributed and have constant variance.

$$Model:  \hat{Y_i}=\alpha+\beta x_i*\epsilon_i$$

$$Assumption: \epsilon_i\stackrel{iid}\sim{}N(0,\sigma^2)$$

Now, in Bayesian statistics there is something called **conjugate analysis**. 
Now here is where we need to brush up on priors, likelihoods and posteriors. Recall that a **prior** is a state of believe in a model before the current experiment, that **likelihood** is the probability of obtaining certain data given the model and that the posterior is the probability of the model given the data. 

$$Prior: p(Model)$$

$$Likelihood: p(Data|Model)$$

$$Posterior: p(Model|Data)$$

Using [Bayes' rule](https://en.wikipedia.org/wiki/Bayes%27_theorem), the posterior can also be written as: 
$$p(Model|Data)=\frac{p(Model\&Data)}{p(Data)}$$

$$=\frac{p(Data|Model)*p(Model)}{p(Data)}$$

$$=\frac{Likelihood*prior}{p(Data)}$$

$p(Data)$ is the sum of the $Likelihood*prior$ for all models:

$$p(Data)=\sum\limits_{Model=1}^{Model} p(Data|Model)*p(Model)$$

So in the end the posterior for a discrete case can be written as:

$$p(Model|Data)=\frac{p(Data|Model)*p(Model)}{\sum p(Data|Model)*p(Model)}$$

All that re-written for the continous case where instead of $Model$ we have $\theta$ meaning a probability distribution and instead of $Data$ we have $x$ (still data), leaves us with:

$$p(\theta|x)=\frac{p(x|\theta)p(\theta)}{\int p(x|\theta')p(\theta')d\theta'}$$

Now! Back to **conjugate analysis**. In Bayesian statistics, if the posterior and prior distributions belong to the same probability distribution families, they are called conjugate distributions. A list of conjugate distributions can be found [here](https://en.wikipedia.org/wiki/Conjugate_prior).
	
And back to **Bayesian regression**.

#### What is Bayesian linear regression?

#### What are application examples of Bayesian linear regression?

#### How is Bayesian linear regression different than frequentist linear regression?

- In Bayesian linear regression, instead of point estimate for each coefficient, there is a distribution. 

#### How is Bayesian linear regression similar to frequentist linear regression?

- Under the reference prior, the values of a 95% credible interval for a regression coefficient compared to a frequentist 95% confidence interval will be the same.
- Under the reference prior, the posterior mean for the regression coefficients is the same as the OLS estimate. 


#### In what cases could Bayesian linear regression be more helpful than frequentist linear regression?

#### What are the model assumptions of Bayesian Regression?

- Errors are normally distributed with constant variance. 

#### How do we determine what is an outlier in Bayesian Linear Regression?

My prediction in Bayes is $\hat{Y_i}=\alpha+\beta x_i*\epsilon_i$, if I rearrange that, then my unobserved deviation is $\epsilon_i=\hat{Y_i}-(\alpha+\beta x_i)$. 

Based on the posterior distribution for the parameters, we can find the posterior distribution of the deviation, a Student T distribution.

$t={\frac {\mu -{\bar {x}}}{s/{\sqrt {n}}}}$ 

Here the mean is the ordinary residual or observed value, minus the fitted value. The scale is a function of the levarage of that case. 

Based on the empirical rule, 95% of the errors will be within plus or minus two standard deviations from the mean. Under this logic, deviations that exceed $k$ standard deviation could be considered as potential outliers. 

```{r echo = FALSE}
#As the sample size increases, the expected number of points that deviate by k standard deviations also increases. Hint - remember that residuals are normally distributed and hence we can use the following command to find the probability that all NN points are within kk standard deviations of their predicted value.

#Question: what is the probability that we observe at least one outlier that is at least 3 standard deviations away from its predicted value?
k <- 3 
n <- 100
p <- (1-2*pnorm(-k))^n

```


#### What is the Bayesian Information Criterion (BIC) and what is it used for?

This is equivalent to using the P-value and R squared in frequentist statistics. 
There are other Bayesian criteria. 
$$BIC= -2*log(likelihood)+log(n)*\#Parameters$$
Where $n$ is the sample size. We choose the model that has the smallest BIC. 
BIC can also be written as 
$$BIC=n*log(1-R^2)+log(n)*\#parameters$$
We can increase the $R^2$ by adding another variable to the model, because this may result in overfitting, BIC penalizes the number of parameters inclding the intercept. 

We will have a tradeoff between the goodness of fit (the extent to which observed data matches the values expected by theory) $n*log(1-R^2)$ and the model complexity represented with $log(n)*\#parameters$. 

After backward elimination, the credible intervals are similar to the ones before but not the same. If the credible interval excludes zero, it suggests that we have created a parsimonious model (accomplishes the desired level of explanation or prediction with as few predictor variables as possible). 

There are other criteria for model selection include variations on the values of $k$, such as IIC. $$IIC=-2*log(likelihood)+k*\#parameters$$

#### How can I calculate the posterior probabilities of many models?

-Narrow intervals are not always better if they miss the truth!
Recall the the Bayes factor is the marginal likelihood of one model against the other:

$$BF[M_m:M_b]=\frac{marginal likelihood of model M_m}{marginal likelihood of model M_b}$$
which has odds against a base model $M_b$:
$$O[M_m:M_b]=\frac{p(M_m)}{p(M_b)}$$


#### How does Bayesian model averaging work?

Bayesian model averaging (BMA) multiplies the posterior probabilities of each model, by the predictions of the model. 

$$BMA predictions \hat{Y^*}=\sum {\hat{Y^*_m}*p(M_m|Data)}$$

#### What is stochastic exploration and what is it for?

Stochastic is when something has a random probability distribution or pattern that may be analysed statistically but may not be predicted precisely. 

What if we have A LOT of predictor variables? Here is where stochastic methods of implementing Bayesian model averaging may come in handy. 

#### What is Markov Chain Monte Carlo sampling and how does it work?

- There's a bunch of models but I am going to start at model 0. For models $i=1$ throught $I$ we will do the following steps. 

1. Randomly select another model (our proposed model). 
2. Next, we are going to compare it to our current model by looking at the posterior odds of the proposed model compared to our current model. We can use the odds if we don't have the posterior probabilities because odds can be expressed as the product of the Bayes factor.

$$BF[M^{*(i+1)}:M^{(i)}]*O[M^{*(i+1)}:M^{(i)}]>1$$

3. If the result of the comparison is more than, we accept the new model. If it is less than one, then we randomly decide to accept the proposed model. We then incremement $i$ by 1 and repeat until $i$ iterations of the model have been done. 

There are two reasons for using Markov Chain Monte Carlo when implementing Bayesian Model Averaging. 

- One, it can be used to sample models according to their posterior model probabilities, even when the posterior model probabilities are unknown. 

- Two, it is useful when there are so many predictors that not all models can be enumerated. 

#### Priors for Bayesian model uncertainty

- So far we have used BIC to determine Bayes' factors. 
- There is another prior distribution called Zellner's g prior. 

