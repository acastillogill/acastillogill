---
title: Predicting property prices
author: √Ångela Castillo-Gill
date: '2018-08-09'
slug: predicting-property-prices
categories:
  - R
  - EDA
tags: 
  - Kaggle
  - Home sales prices
draft: FALSE
summary: What factors most affect sales prices for homes?
output:
  blogdown::html_page:
    toc: true
    number_sections: true
    toc_depth: 2
  fig_caption: true
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
x <-
  c("tidyverse",
    "knitr",
    "formatR",
    "stringr",
    "lubridate",
    "tidyr",
    "formattable",
    "grid",
    "gridExtra",
    "kableExtra",
    "here",
    "corrplot",
    "rms",
    "dummies",
    "sjmisc",
    "car",
    "DescTools",
    "gvlma",
    "MASS",
    "QuantPsyc",
    "Hmisc",
    "GGally",
    "lm.beta",
    "MuMIn",
    "broom",
    "corrplot",
    "rpart",
    "rpart.plot",
    "Metrics")

lapply(x, require, character.only = TRUE)


opts_chunk$set(echo = FALSE,
               warning = FALSE,
               error = FALSE,
               message = FALSE,
               collapse= TRUE,
               comment = NA,
               tidy = TRUE)

#Clear Global Environment
rm(list = ls())
```

# Summary

*To see the code used in this post, visit my [kernel on kaggle in R Markdown format](https://www.kaggle.com/adcastillogill/exploring-kiva-loans).* 



# Purpose of this post



# The data

The dataset [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) was downloaded from Kaggle and put together by [Dean De Cock.](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf) It has 79 explanatory variables describing 1,460 homes in Ames, Iowa. The codebook for all the variables can be [found here.](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) As I go along, I'll explain the most relevant ones. 


```{r reading data, results='hide'}
path <- "/Volumes/TOSHIBAEXT/RStudio/blog/content/post/house_prices/"
#Read `train.csv` as homes
homes <- read.csv(paste0(path,"train.csv"), stringsAsFactors = FALSE)
```

First we will see how many numerical vs. categorical variables there are. 
```{r counting variable types}
#is.numeric returns TRUE if the variable is numeric. 
#sapply iterates and returns a vector. 
#Which gives the indices that were TRUE
num_var <- which(sapply(homes, is.numeric))
#Count how many variables are numeric
length_num_var <- length(num_var)
#Return a vector with character variables
car_var <- which(sapply(homes, is.character))
#Count how many variables are characters
length_car_var <- length(car_var)
```

Our dataset has `r length_num_var` numeric and `r length_car_var` character variables. Next, since we are interested in estimating sales prices `SalePrice`, we will recode character variables and see the most strongly correlated variables. There are 43 character variables available. I want to recode them where there is ordinality and where there isn't dummify them.

There are two numerical variables that are actually date related: `YearBuilt`and `YearRemodAdd` (remodelled date). It makes more sense to make two new variables that relate the build and remodelled dates with the present. In other words, I will create the years since built and years since remodelled date variables, this will help interpret the results better. 

## Missing values

As it more common than not, the dataset contains missing values. Missing values need to be dealt with because regression (and other models) requires complete observations. 

Dealing with missing data depends on *why the data are missing*. [This article](http://www.stat.columbia.edu/~gelman/arm/missing.pdf) explains four reasons why data could be missing. When the data are missing at random (MAR) or completely at random (MCAR), observations with missing values can be removed without introducing bias into the model.  

Sometimes, however, if the dataset is not too big and we don't want to lose observations, or even if it is big, yet we still don't want to remove observations, we can impute data. Imputing means replacing missing values by doing some educated guesses. [This article](https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4) summarises how to impute data depending on why it is missing. 

If the data are not missing at random, then the imputation mechanism has to modelled.

Let's look at which variables are missing:

```{r missigness-table}

#Which variables have missingness in data
homes%>%
  dplyr::select_if(function (x) any(is.na(x)))%>%
  dplyr::summarise_all(funs(sum(is.na(.))))%>%
  gather()%>%
  arrange(desc(value))->missing_columns

missing_columns%>%
#Make table
  kable(caption="Variables with missing values in descending order",
        #Add column names
        col.names = c("Variable","Number of NAs")) %>%
  #Style table
 kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

```

`r nrow(missing_columns)` variables have missing values. Based on the codebook, the reason why so many houses have `PoolQC` missing is because `NA`, means there is no pool. Since this variable is ordinal, I can revalue it to make it numerical and `0` will mean the property has no pool. `MiscFeature`, `Alley`, `Fence`, and `FireplaceQu` are missing because of similar reasons. We don't know why `LotFrontage` is missing but we will impute as the median for properties in the same neighborhood. [Erik Bruin's kernel on Kaggle](https://www.kaggle.com/erikbruin/house-prices-lasso-xgboost-and-a-detailed-eda/code) with this dataset, was a great guideline for what to do in each missing value case. 

```{r missingness and recoding ordinal variables, results='hide'}

#Vector for ordinal values
ordinal_scale <- c("Ex"=5, "Gd"=4, "TA"=3, "Fa"=2, "Po"=1, "No"=0)

#Missingness PoolQC
homes$PoolQC[is.na(homes$PoolQC)] <- "No"

#Recode PoolQC
homes$PoolQC<-as.integer(plyr::revalue(homes$PoolQC, ordinal_scale))

#Missingness MiscFeature
homes$MiscFeature[is.na(homes$MiscFeature)] <- "No"

#Recode MiscFeature
homes$MiscFeature <- as.factor(homes$MiscFeature)

#Missingness Alley
homes$Alley[is.na(homes$Alley)] <- "No"

#Recode Alley
homes$Alley <- as.factor(homes$Alley)

#Missingness Fence
homes$Fence[is.na(homes$Fence)] <- "No"

#Checking if Fence is ordinal
homes%>%
  filter(!is.na(SalePrice))%>%
  group_by(Fence) %>%
  dplyr::summarise(median = median(SalePrice), counts=n())

#Fence is not ordinal

#Recode Fence
homes$Fence <- as.factor(homes$Fence)

#Missingness FireplaceQu
homes$FireplaceQu[is.na(homes$FireplaceQu)] <- "No"

#Checking if FireplaceQu is ordinal
homes%>%
  filter(!is.na(SalePrice))%>%
  group_by(FireplaceQu) %>%
  dplyr::summarise(median = median(SalePrice), counts=n())

#Recode FireplaceQu
homes$FireplaceQu<-as.integer(plyr::revalue(homes$FireplaceQu, ordinal_scale))


#LotFrontage is the linear feet of street connnected to property
#Missing values will be replaced by neighborhood average

for (i in 1:nrow(homes)){
        if(is.na(homes$LotFrontage[i])){
               homes$LotFrontage[i] <-
                 as.integer(median(homes$LotFrontage[homes$Neighborhood==homes$Neighborhood[i]], na.rm=TRUE)) 
        }
}

#Missingness GarageType
homes$GarageType[is.na(homes$GarageType)] <- "No"

#Recode GarageType
homes$GarageType <- as.factor(homes$GarageType)

#Checking if GarageFinish is ordinal
homes%>%
  filter(!is.na(SalePrice))%>%
  group_by(GarageFinish) %>%
  dplyr::summarise(median = median(SalePrice), counts=n())

#It is ordinal

#Missingness GarageFinish
homes$GarageFinish[is.na(homes$GarageFinish)] <- "No"

#GarageFinish ordinal vector
Finish <- c('No'=0, 'Unf'=1, 'RFn'=2, 'Fin'=3)

#Recode GarageFinish
homes$GarageFinish<-as.integer(plyr::revalue(homes$GarageFinish, Finish))
table(homes$GarageFinish)

#Missingness GarageQual
homes$GarageQual[is.na(homes$GarageQual)] <- "No"

#Recode GarageQual
homes$GarageQual<-as.integer(plyr::revalue(homes$GarageQual, ordinal_scale))

#Missingness GarageCond
homes$GarageCond[is.na(homes$GarageCond)] <- "No"

#Recode GarageCond
homes$GarageCond<-as.integer(plyr::revalue(homes$GarageCond, ordinal_scale))


#Missingness GarageYrBlt will be substituted for YearBuilt

#For all rows in homes
for (i in 1:nrow(homes)){
  #If observation i of column GarageYrBlt is NA
        if(is.na(homes$GarageYrBlt[i])){
          #Change observation for observation i of column YearBuilt
               homes$GarageYrBlt[i] <- homes$YearBuilt[i] 
        }
}

#Checking if BsmtExposure is ordinal
homes%>%
  filter(!is.na(SalePrice))%>%
  group_by(BsmtExposure) %>%
  dplyr::summarise(median = median(SalePrice), counts=n())

#Missingness BsmtExposure
homes$BsmtExposure[is.na(homes$BsmtExposure)] <- "None"

#It is ordinal, create vector substitute vector
exposure_ordinal <- c("Gd"= 4,"Av"= 3,"Mn"=2,"No"=1, "None"=0)

#Recode BsmtExposure
homes$BsmtExposure<-as.integer(plyr::revalue(homes$BsmtExposure, exposure_ordinal))
table(homes$BsmtExposure)

#Checking if BsmtFinType2 is ordinal
homes%>%
  filter(!is.na(SalePrice))%>%
  group_by(BsmtFinType2) %>%
  dplyr::summarise(median = median(SalePrice), counts=n())

#Missingness BsmtFinType2
homes$BsmtFinType2[is.na(homes$BsmtFinType2)] <- "No"

#It is ordinal, create vector substitute vector
FinType_ordinal <- c('No'=0, 'Unf'=1, 'LwQ'=2, 'Rec'=3, 'BLQ'=4, 'ALQ'=5, 'GLQ'=6)

#Recode BsmtFinType2
homes$BsmtFinType2<-as.integer(plyr::revalue(homes$BsmtFinType2, FinType_ordinal))
table(homes$BsmtFinType2)

#Missingness BsmtQual
homes$BsmtQual[is.na(homes$BsmtQual)] <- "No"

#Recode BsmtQual
homes$BsmtQual<-as.integer(plyr::revalue(homes$BsmtQual, ordinal_scale))

#Missingness BsmtCond
homes$BsmtCond[is.na(homes$BsmtCond)] <- "No"

#Recode BsmtCond
homes$BsmtCond<-as.integer(plyr::revalue(homes$BsmtCond, ordinal_scale))

#Missingness BsmtFinType1
homes$BsmtFinType1[is.na(homes$BsmtFinType1)] <- "No"

#Recode BsmtFinType1
homes$BsmtFinType1<-as.integer(plyr::revalue(homes$BsmtFinType1, FinType_ordinal))
table(homes$BsmtFinType1)

#Checking if MasVnrType is ordinal
homes%>%
  filter(!is.na(SalePrice))%>%
  group_by(MasVnrType) %>%
  dplyr::summarise(median = median(SalePrice), counts=n())

#Missingness MasVnrType
homes$MasVnrType[is.na(homes$MasVnrType)] <- "None"

#Create ordinality vector
mas_ordinality <- c('None'=0, 'BrkCmn'=0, 'BrkFace'=1, 'Stone'=2)

#Recode MasVnrType
homes$MasVnrType<-as.integer(plyr::revalue(homes$MasVnrType, mas_ordinality))
table(homes$MasVnrType)

#Missingness MasVnrArea
homes$MasVnrArea[is.na(homes$MasVnrArea)] <- 0

#Missingness Electrical
homes%>%
  group_by(Electrical)%>%
  dplyr::count()%>%
  arrange(desc(n))%>%
  dplyr::select(Electrical)%>%
  head(n=1)->replacement_Electrical

homes$Electrical[is.na(homes$Electrical)] <- unlist(replacement_Electrical)

#Recode Electrical
homes$Electrical <- as.factor(homes$Electrical)

#Recode MSZoning 
homes$MSZoning  <- as.factor(homes$MSZoning)

#Recode MSSubClass
homes$MSSubClass  <- as.factor(homes$MSSubClass)

#Recode Street     
homes$Street  <- as.factor(homes$Street)

#Recode LotShape   

homes$LotShape   <- as.factor(homes$LotShape)

#Recode LandContour   

homes$LandContour   <- as.factor(homes$LandContour)


#Recode Utilities 

homes$Utilities    <- as.factor(homes$Utilities)
#Looking at factors
table(homes$Utilities)
#When looking at levels, all observations except 1, fall into one level. Therefore this variable is not too helpful.
homes$Utilities <- NULL

#Recode LotConfig    

homes$LotConfig     <- as.factor(homes$LotConfig)

#Recode LandSlope  

homes$LandSlope     <- as.factor(homes$LandSlope)

#Recode Neighborhood    

homes$Neighborhood     <- as.factor(homes$Neighborhood)

#Recode Condition1   

homes$Condition1     <- as.factor(homes$Condition1)


#Recode Condition2 

homes$Condition2     <- as.factor(homes$Condition2)

#Recode BldgType    

homes$BldgType      <- as.factor(homes$BldgType)

#Recode HouseStyle 

homes$HouseStyle       <- as.factor(homes$HouseStyle)

#Recode RoofStyle    

homes$RoofStyle       <- as.factor(homes$RoofStyle)

#Recode RoofMatl   

homes$RoofMatl      <- as.factor(homes$RoofMatl)

#Recode Exterior1st 

homes$Exterior1st       <- as.factor(homes$Exterior1st)

#Recode Exterior2nd     

homes$Exterior2nd       <- as.factor(homes$Exterior2nd)

#Recode ExterQual 

homes$ExterQual<-as.integer(plyr::revalue(homes$ExterQual, ordinal_scale))

#Recode ExterCond    

homes$ExterCond<-as.integer(plyr::revalue(homes$ExterCond, ordinal_scale))

#Recode Foundation    

homes$Foundation      <- as.factor(homes$Foundation)

#Recode Heating 

homes$Heating       <- as.factor(homes$Heating)

#Recode HeatingQC    

homes$HeatingQC<-as.integer(plyr::revalue(homes$HeatingQC, ordinal_scale))

#Recode CentralAir   

boolean <- c("Y"=1,"N"=0)

homes$CentralAir<-as.integer(plyr::revalue(homes$CentralAir, boolean))

table(homes$CentralAir)

#Recode KitchenQual   


homes$KitchenQual<-as.integer(plyr::revalue(homes$KitchenQual, ordinal_scale))


#Recode Functional    

homes$Functional      <- as.factor(homes$Functional)

#Recode PavedDrive 

homes$PavedDrive       <- as.factor(homes$PavedDrive)

#Recode SaleType 
homes$SaleType        <- as.factor(homes$SaleType)

#Recode SaleCondition 

homes$SaleCondition     <- as.factor(homes$SaleCondition)
```


## Variable creation

```{r create time variables}
homes<-homes%>%
  #Mutate to create `YearsSinceBuilt`, `YearsSinceGarageBuilt`, and `YearsSinceRemod`
  #It will be the difference of the present year - YearBuilt
  mutate(YearsSinceBuilt = year(Sys.Date())-YearBuilt,
         #Same for YearsSinceRemod
         YearsSinceRemod = year(Sys.Date())-YearRemodAdd,
         #Same for GarageYrBlt
         YearsSinceGarageBuilt = year(Sys.Date())-GarageYrBlt)%>%
  #Remove `YearsSinceBuilt`, `YearsSinceGarageBuilt`, and `YearsSinceRemod`
  dplyr::select(-YearBuilt,
         -YearRemodAdd,
         -GarageYrBlt,
         -Id)
```

## Correlation

Correlation, $Cor(X,Y)$, measures the strength of the linear relationship between two variables $X$ and $Y$.

The correlation between `SalePrice` and another variable, let's say, `OverallQual`, is the covariance of the separately normalised data between the two variables.

```{r covariance-example, echo=TRUE}
cov(scale(homes$SalePrice), scale(homes$OverallQual))
```

Since covariance units are `OverallQual` * `SalePrice`, calculating the correlation is instead more helpful since it is unit free. 

If we created a model with only variable as the predictor of `SalesPrice`, let's say, `KitchenQual` and normalised the data, the regression slope would be the correlation between the two variables.

```{r single-regression-normalised-data, echo=TRUE}
norm_fit <- lm(scale(SalePrice) ~ scale(KitchenQual), data = homes)
round(coefficients(norm_fit), digits = 2)
```

Here is the correlation matrix for variables that have a relationship stronger than 0.5 with `SalePrice`. 

```{r correlation_matrix}
#Select numeric variables
homes_num <- homes%>%
  select_if(is.numeric)
#Create tidy correlation matrix
cor_homes <- tidy(cor(homes_num))
#Arrange correlations with SalePrice in descending order
high_cor_names <- cor_homes%>%
  arrange(desc(SalePrice))%>%
  filter(abs(SalePrice)>0.5)%>%#Filter variables that have relationship >0.5
  pull(.rownames)#Pull names of variables
#Filter entire correlation matrix for the variables
high_cor <- cor(homes_num)[high_cor_names,high_cor_names]
#Create correlation matrix
corrplot.mixed(high_cor, tl.col="black", tl.pos = "lt",cl.ratio=0.1,number.cex=.8, cl.cex = .6, tl.cex = 0.8)
```

There are 17 variables that have a correlation stronger than 0.5. They are arranged in descending order. It is interesting to note the high correlation that exists amongst variables. The correlation plot highlights some obvious ones, `GarageArea` and `GarageCars`. Makes sense, a bigger garage can hold more cars. `X1stFlrSF` and `TotalBsmtSF`, the total area of the first floor and basement, this also seems reasonable since basements are underneath the same floor and would tend to have a similar area. `TotRmsAbvGrd` and `GrLivArea`, the total number of rooms and area above ground, again ok, more rooms would be linked to a bigger living area. Finally, `YearsSinceBuilt` and `YearsSinceGarageBuilt` since garages are usually built at the same time as the house.

Here is the codebook for all the variables featured in the correlation matrix in case they come up later and we need to interpret what they mean.

**Positive correlation**

- `OverallQual`: Rates the overall material and finish of the house
- `GrLivArea`: Above grade (ground) living area square feet.
- `ExterQual`: Exterior quality
- `KitchenQual`: Kitchen quality
- `GarageCars`: Size of garage in car capacity
- `GarageArea`: Size of garage in square feet
- `TotalBsmtSF`: Total square feet of basement area
- `X1stFlrSF`: First floor square feet
- `BsmtQual`: Height of basement
- `FullBath`: Full bathrooms above grade
- `GarageFinish`: Interior finish of the garage
- `TotRmsAbvGrd`: Total rooms above grade (does not include bathrooms)
- `FireplaceQu`: Fireplace quality

**Negative correlation**

- `YearsSinceRemod`: Years since remodel date (same as construction date if no remodelling or additions)
- `YearsSinceGarageBuilt`: Years since the garage was built 
- `YearsSinceBuilt`: Years since construction date


# Random forest time

## Splitting the data
```{r split-data, include=FALSE}
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15% 
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
                     prob = c(0.7,0.15,0.15),
                     replace = TRUE)

# Create a train, validation and tests from the original data frame 
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
```

## Training the model
```{r train the model, include=FALSE}
# Train the model
homes_model <- rpart(formula = SalePrice ~ ., 
                     data = homes_train, 
                     method = "anova")

# Look at the model output                      
print(homes_model)
```

##Revising variable importance
```{r variable-importance-table}
#Look at variable importance

var_importance <- tidy(homes_model$variable.importance)

total_importance <- sum(var_importance$x)

var_importance%>%
mutate(x=x/total_importance*100)%>%
kable(caption="Top 30 variables in order of importance",
        #Add column names
        col.names = c("Variable","Percentage")) %>%
  #Style table
 kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

## Evaluating the model

```{r prediction}
#Computing predicted values 
pred <- predict(object=homes_model,
                newdata = homes_test)

#Compute RMSE
rmse(actual=homes_test$SalePrice, #Actual values
     predicted = pred) #Predicted values

#Compute MAE
mae(actual=homes_test$SalePrice, #Actual values
     predicted = pred) #Predicted values
```

## Tuning hyperparameters

```{r tuning-hyperparameters}
# Plot the "CP Table"
plotcp(homes_model)

# Print the "CP Table"
print(homes_model$cptable)

# Retrieve optimal cp value based on cross-validated error
opt_index <- which.min(homes_model$cptable[, "xerror"])
cp_opt <- homes_model$cptable[opt_index, "CP"]

# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model, 
                         cp = cp_opt)
                          
# Plot the optimized model
rpart.plot(x = homes_model_opt, yesno = 2, type = 0, extra = 0)
```
## Checking the error on the optimised model
```{r prediction}
#Computing predicted values 
pred <- predict(object=homes_model_opt,
                newdata = homes_test)

#Compute RMSE
rmse(actual=homes_test$SalePrice, #Actual values
     predicted = pred) #Predicted values

#Compute MAE
mae(actual=homes_test$SalePrice, #Actual values
    predicted = pred) #Predicted values
```

# Techniques used

- I 

# Questions from this analysis

- 

