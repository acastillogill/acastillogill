---
title: "Regression trees: predicting property prices"
author: Ángela Castillo-Gill
date: '2018-08-09'
slug: regression-trees-predicting-property-prices
categories: []
tags: 
  - Kaggle
  - Home sale prices
  - Decision trees
  - Regression trees
description: "In this post I explore a property dataset from Ames, Iowa. The data describes a set of features for houses and includes sale price. My goal was to understand what features are linked with sale price for this specific dataset using regression trees. To do this, I first prepared the data by dealing with missing values and created other variables to better interpet the results. After preparing the data, I used regression trees to answer the question. One of the benefits of regression trees is that the output can be illustrated and easily interpreted. I found that the variable: overall quality is most closely linked to sale price. Other features such as living area and basement size are also important. I also found that neighborhoods NorthRidge Heights, Northridge and, Stone Brook have the most expensive houses."
draft: FALSE
banner: "banners/property-prices.png"
images: ["banners/property-prices.png"]
summary: "What are features are closely linked with sale price?"
header: 
  image: "banners/property-prices.png"
  caption: "Regression tree to estimate property values. "
output:
  blogdown::html_page:
    toc: true
    number_sections: true
    toc_depth: 2
  fig_caption: true
editor_options: 
  chunk_output_type: console
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>

<div id="TOC">
<ul>
<li><a href="#summary"><span class="toc-section-number">1</span> Summary</a></li>
<li><a href="#question"><span class="toc-section-number">2</span> Question</a></li>
<li><a href="#dataset-description"><span class="toc-section-number">3</span> Dataset description</a><ul>
<li><a href="#missing-values"><span class="toc-section-number">3.1</span> Missing values</a></li>
<li><a href="#correlation"><span class="toc-section-number">3.2</span> Correlation</a></li>
</ul></li>
<li><a href="#regression-trees---why-use-them"><span class="toc-section-number">4</span> Regression trees - why use them?</a><ul>
<li><a href="#variable-importance"><span class="toc-section-number">4.1</span> Variable importance</a></li>
</ul></li>
<li><a href="#results"><span class="toc-section-number">5</span> Results</a></li>
<li><a href="#conclusion"><span class="toc-section-number">6</span> Conclusion</a></li>
</ul>
</div>

<div id="summary" class="section level1">
<h1><span class="header-section-number">1</span> Summary</h1>
<p><em>To see the all the code used in this post, visit my <a href="https://github.com/acastillogill/acastillogill">GitHub repository for this site</a></em></p>
<ul>
<li>Objectives: To predict property prices using decision trees, specifically, the classification and regression algorithm.</li>
<li>Challenge: First time applying decision trees.</li>
<li>Data points: 118260</li>
<li>Language: R</li>
</ul>
</div>
<div id="question" class="section level1">
<h1><span class="header-section-number">2</span> Question</h1>
<p>You’re interested in buying or building a house in Ames, Iowa <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. You want to understand <strong>which characteristics are most closely linked to sale price</strong> since that can inform your purchasing decision. Perhaps you might even be able to find a less expensive house and modify certain features to increase the sale price.</p>
</div>
<div id="dataset-description" class="section level1">
<h1><span class="header-section-number">3</span> Dataset description</h1>
<p>The dataset <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a> was put together by <a href="https://ww2.amstat.org/publications/jse/v19n3/decock.pdf">Dean De Cock.</a> It has 79 explanatory variables describing 1,460 homes in Ames, Iowa with 38 numeric and 43 character variables. The codebook for all the variables can be <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data">found here.</a> As I go along, I’ll explain the most relevant ones.</p>
<div id="missing-values" class="section level2">
<h2><span class="header-section-number">3.1</span> Missing values</h2>
<p>Naturally, the dataset contains missing values. Missing values need to be dealt with because often regression (and other models) require complete observations. Dealing with missing data depends on <em>why the data are missing</em>. <a href="http://www.stat.columbia.edu/~gelman/arm/missing.pdf">This article</a> explains four reasons why data could be missing. When the data are missing at random (MAR) or completely at random (MCAR), observations with missing values can be removed without introducing bias into the model. Sometimes, however, if the dataset is not too big and we don’t want to lose observations, or even if it is big, yet we still don’t want to remove observations, we can impute data. Imputing means replacing missing values by making some educated guesses. <a href="https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4">This article</a> summarises how to impute data depending on why it is missing. If the data are not missing at random, then the imputation mechanism has to be modelled.</p>
<p>About 19 variables have missing values - based on the codebook, the reason why so many houses have pool quality (<code>PoolQC</code>) missing is because <code>NA</code>, means there is no pool. Since this variable is ordinal, I can revalue it to make it numerical and <code>0</code> will mean the property has no pool. Other features (<code>MiscFeature</code>), <code>Alley</code>, <code>Fence</code>, and fireplace quality (<code>FireplaceQu</code>) are missing because of similar reasons. We don’t know why <code>LotFrontage</code> is missing but we will impute the median for properties in the same neighborhood. I learned a lot about imputation and missing values from <a href="https://www.kaggle.com/erikbruin/house-prices-lasso-xgboost-and-a-detailed-eda/code">Erik Bruin’s kernel on Kaggle</a>.</p>
</div>
<div id="correlation" class="section level2">
<h2><span class="header-section-number">3.2</span> Correlation</h2>
<p>Correlation, <span class="math inline">\(Cor(X,Y)\)</span>, measures the strength of the linear relationship between two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The correlation between <code>SalePrice</code> and another variable, let’s say, <code>OverallQual</code>, is the covariance of the separately normalised data between the two variables.</p>
<pre class="r"><code>cov(scale(homes$SalePrice), scale(homes$OverallQual))</code></pre>
<pre><code>  [,1]</code></pre>
<p>[1,] 0.791</p>
<p>Since covariance units are <code>OverallQual</code> * <code>SalePrice</code>, calculating the correlation is more helpful as is unit free. If we created a model with only one variable as the predictor of <code>SalePrice</code>, let’s say, kitchen quality (<code>KitchenQual</code>) and normalised the data, the regression slope would be the correlation between the two variables.</p>
<pre class="r"><code>norm_fit &lt;- lm(scale(SalePrice) ~ scale(KitchenQual), data = homes)
round(coefficients(norm_fit), digits = 2)</code></pre>
<pre><code>   (Intercept) scale(KitchenQual) 
          0.00               0.66 </code></pre>
<p>Here is the correlation matrix for variables that have a relationship stronger than 0.5 with <code>SalePrice</code>. We can also see the two correlations (<code>SalePrice</code> &amp; <code>OverallQual</code>; <code>SalePrice</code> &amp; <code>KitchenQual</code>) mentioned above.</p>
<div class="figure"><span id="fig:correlation-matrix"></span>
<img src="/post/2018-08-09-predicting-property-prices_files/figure-html/correlation-matrix-1.png" alt="Variables are arranged in descending order according to the strength of the relationship with `SalePrice`." width="672" />
<p class="caption">
Figure 3.1: Variables are arranged in descending order according to the strength of the relationship with <code>SalePrice</code>.
</p>
</div>
<p>There are 17 variables that have a correlation stronger than 0.5 with `SalesPrice. When variables are highly correlated amongst each other, it’s better to remove some of them as they don’t necessarily add additional information and it could lead to <a href="https://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r">multicollinearity</a>. The correlation plot highlights some obvious pairs related to each other:</p>
<ul>
<li><code>GarageArea</code> and <code>GarageCars</code>: makes sense, a bigger garage can hold more cars.</li>
<li><code>X1stFlrSF</code> and <code>TotalBsmtSF</code>: the total area of the first floor and basement, this also seems reasonable since basements are underneath the same floor and would tend to have a similar area.</li>
<li><code>TotRmsAbvGrd</code> and <code>GrLivArea</code>: the total number of rooms and area above ground, again ok, more rooms would be linked to a bigger living area.</li>
<li><code>YearsSinceBuilt</code> and <code>YearsSinceGarageBuilt</code>: since garages are usually built at the same time as the house.</li>
</ul>
</div>
</div>
<div id="regression-trees---why-use-them" class="section level1">
<h1><span class="header-section-number">4</span> Regression trees - why use them?</h1>
<p>The tool I’m using to answer the question is regression trees. They are also known as classification and regression trees (CART) or the recursing and partitioning (RPART) algorithm. The reasons I’m choosing this tool or algorithm to answer the question is because a) I’ve never applied them to a dataset and b) regression trees are interpretable and allow for easy-to-follow plots that might come in handy. I will be using the <code>rpart</code> package in R. `rpart, builds a model in two stages:</p>
<p><strong>First stage</strong>:</p>
<p>The variable which can best<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> split the data into two groups is identified. The data are then separated into two groups and the whole process is repeated <em>recursively</em> or indefinitely until the sub-groups reach a minimum size, or until no further improvements can be made. When the split is made, similarity amongst the observations can more or less homogenous. This homogeneity is also called purity and it can be measured. The impurity measure of a node specifies how mixed the resulting subset is.</p>
<p><strong>Second stage</strong>:</p>
<p>The tree is trimmed back or prunned using cross-validation. We identify the lowest cross-validated error or the smallest within one standard error of the tree with lowest cross-validated error. In this case, the tree with seven splits and eight nodes is has the</p>
<pre class="r"><code># Train the model
homes_model &lt;- rpart(formula = SalePrice ~ ., data = homes_train, method = &quot;anova&quot;)</code></pre>
<div id="variable-importance" class="section level2">
<h2><span class="header-section-number">4.1</span> Variable importance</h2>
<p>Using the <code>rpart</code> function, we are able to rank which variables are most predictive of <code>SalePrice</code>. The following plot ranks these variables in descending order.</p>
<div class="figure"><span id="fig:variable-importance-table"></span>
<img src="/post/2018-08-09-predicting-property-prices_files/figure-html/variable-importance-table-1.png" alt="Overall quality (`OverallQual`) is the most predictive variable - it was also most correlated with `SalePrice`. It's followed by basement size (`TotalBsmtSF`) and neighborhood type." width="672" />
<p class="caption">
Figure 4.1: Overall quality (<code>OverallQual</code>) is the most predictive variable - it was also most correlated with <code>SalePrice</code>. It’s followed by basement size (<code>TotalBsmtSF</code>) and neighborhood type.
</p>
</div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:tuning-hyperparameters">Table 4.1: </span>Understanding the complexity parameter table: <code>CP</code> stands for complexity parameter, which can be thought as the minimum benefit a split <code>nsplit</code> can add to a tree and equivalent to the decrease of the <code>rel.error</code>. The <code>rel.error</code> stands for 1-RSquared, similar to linear regression, where it explains how much variability in the data is explained by the model. The <code>xerror</code> is the relative sum-of-squared errors in tenfold cross-validation. <code>xstd</code> is the variation in prediction across ten validation samples. We are going to choose the smallest tree (by splits) whose error is no more than one standard error above the error of the best model. The smallest error is 0.3242, adding one standard error: 0.3242+0.0373=0.3615. The model that has an <code>xerror</code> smaller than 0.3615 is the one with 4 splits with an <code>xerror</code> of 0.3604.
</caption>
<thead>
<tr>
<th style="text-align:right;">
CP
</th>
<th style="text-align:right;">
nsplit
</th>
<th style="text-align:right;">
rel.error
</th>
<th style="text-align:right;">
xerror
</th>
<th style="text-align:right;">
xstd
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.4458
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1.0000
</td>
<td style="text-align:right;">
1.0034
</td>
<td style="text-align:right;">
0.0965
</td>
</tr>
<tr>
<td style="text-align:right;">
0.1119
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.5542
</td>
<td style="text-align:right;">
0.5579
</td>
<td style="text-align:right;">
0.0545
</td>
</tr>
<tr>
<td style="text-align:right;">
0.0779
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0.4424
</td>
<td style="text-align:right;">
0.4670
</td>
<td style="text-align:right;">
0.0542
</td>
</tr>
<tr>
<td style="text-align:right;">
0.0394
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.3645
</td>
<td style="text-align:right;">
0.4001
</td>
<td style="text-align:right;">
0.0361
</td>
</tr>
<tr>
<td style="text-align:right;">
0.0268
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0.3251
</td>
<td style="text-align:right;">
0.3604
</td>
<td style="text-align:right;">
0.0357
</td>
</tr>
<tr>
<td style="text-align:right;">
0.0212
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
0.2715
</td>
<td style="text-align:right;">
0.3619
</td>
<td style="text-align:right;">
0.0410
</td>
</tr>
<tr>
<td style="text-align:right;">
0.0153
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
0.2503
</td>
<td style="text-align:right;">
0.3455
</td>
<td style="text-align:right;">
0.0378
</td>
</tr>
<tr>
<td style="text-align:right;">
0.0147
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
0.2351
</td>
<td style="text-align:right;">
0.3368
</td>
<td style="text-align:right;">
0.0372
</td>
</tr>
<tr>
<td style="text-align:right;">
0.0106
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
0.2204
</td>
<td style="text-align:right;">
0.3260
</td>
<td style="text-align:right;">
0.0373
</td>
</tr>
<tr>
<td style="text-align:right;">
0.0100
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.2098
</td>
<td style="text-align:right;">
0.3242
</td>
<td style="text-align:right;">
0.0373
</td>
</tr>
</tbody>
</table>
<div class="figure"><span id="fig:opt-plot"></span>
<img src="/post/2018-08-09-predicting-property-prices_files/figure-html/opt-plot-1.png" alt="This is the optimised model according to the criterion of choosing the smallest model within one standard error of the smallest `xerror`." width="672" />
<p class="caption">
Figure 4.2: This is the optimised model according to the criterion of choosing the smallest model within one standard error of the smallest <code>xerror</code>.
</p>
</div>
<p>Now I will compute two measures of error (MSE and RMSE) on both the baseline and optimised models using the test data. I will choose the model with the smallest MSE and RMSE on this unseen data.</p>
<pre class="r"><code>
#Compute RMSE baseline model
rmse(actual=homes_test$SalePrice, #Actual values
     predicted = pred_base ) #Predicted values</code></pre>
<p>[1] 44.35</p>
<pre class="r"><code>
#Compute MAE baseline model
mae(actual=homes_test$SalePrice, #Actual values
     predicted = pred_base ) #Predicted values</code></pre>
<p>[1] 29.38</p>
<pre class="r"><code>
#Compute RMSE optimised model
rmse(actual=homes_test$SalePrice, #Actual values
     predicted = pred_opt) #Predicted values</code></pre>
<p>[1] 49.59</p>
<pre class="r"><code>
#Compute MAE optimised model
mae(actual=homes_test$SalePrice, #Actual values
    predicted = pred_opt) #Predicted values</code></pre>
<p>[1] 35.49</p>
<p>It seems the baseline model with 10 splits resulted in a lower test MSE and RMSE than the optimised model.</p>
</div>
</div>
<div id="results" class="section level1">
<h1><span class="header-section-number">5</span> Results</h1>
<p>I chose the model with 10 splits and 11 nodes because it had the lowest performance metrics on unseen data. I was surprised because I had expected the smaller model to perform better.</p>
<div class="figure"><span id="fig:final-plot"></span>
<img src="/post/2018-08-09-predicting-property-prices_files/figure-html/final-plot-1.png" alt="The tree with 11 nodes and 10 splits had the lowest performance metrics." width="672" />
<p class="caption">
Figure 5.1: The tree with 11 nodes and 10 splits had the lowest performance metrics.
</p>
</div>
<p>The most influential variable for <code>SalePrice</code> was <code>OverallQual</code>. This variable “rates the overall material and finish of the house”, values equal or above to 8 correspond to “very good”, “excellent”, and “very excellent”. On one hand, for houses with a rating of 8 or above in <code>OverallQual</code>, the next most decisive variable is <code>TotalBsmtSF</code>, which is the “total square feet of basement area”. If it’s above 1850, the house is classified depending on <code>Neighborhood_type</code>. <code>Fancy</code> houses are in neighborhoods: NridgHt, NoRidge, StoneBr. All other neighborhoods are <code>Not_fancy</code>. Houses below 1850 feet are further classified depending on <code>LotArea</code>.</p>
<p>On the other hand, for houses with a rating below 8 for <code>OverallQual</code>, the same variable decides again classifying them above or equal to 7, which is “good”. Either way, houses will be classified again by <code>GrLivArea</code>: “above grade living area in square feet”, where smaller houses will depend on basement size (<code>TotalBsmtSF</code>) or on <code>MS_type</code>, which describes the type of dwelling. Modern dwellings include: split foyer, 1-story built in 1946 or newer. Less modern dwellings are 1945 or older in some cases.</p>
<p>In response to the question: what makes a house more valuable than others? <strong>The simple response is quality and area</strong>. Many variables in the tree are related to area: basement area, garage area, and total living area. Even the size of the lot plays an important role, so size is essential. But more important than size, in this case, is the overall quality of the house. It seems that having a property in good shape pays off. Would it be worth to find a house in a somewhat fancy neighborhood and work on improving its finish and materials to increase quality. Who knows? That is a causal question. There is a strong relationship between the variables I describe but correlation does not imply causation. As a final note it’s important to add that because the dataset is from Ames, Iowa, the results of this analysis are limited to that area.</p>
</div>
<div id="conclusion" class="section level1">
<h1><span class="header-section-number">6</span> Conclusion</h1>
<p>In this post I explored a property dataset from Ames, Iowa. The data described a set of features for houses and included sale price. My goal was to understand what features are linked with sale price for this specific dataset using regression trees. To do this, I first prepared the data by dealing with missing values and created other variables to better interpet the results. After preparing the data, I used regression trees to answer the question. One of the benefits of regression trees is that the output can be illustrated and easily interpreted. I found that the variable: overall quality is most closely linked to sale price. Other features such as living area and basement size are also important. I also found that neighborhoods NorthRidge Heights, Northridge and, Stone Brook have the most expensive houses.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>That’s where the dataset is from.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>The best tree is the smallest tree with highest cross-validated error.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
