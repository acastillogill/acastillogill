---
title: "Clustering: cereals, supermarket shelves, and sugar"
author: √Ångela Castillo-Gill
date: '2018-10-17'
slug: cluster-analysis-cereals
categories: []
tags: 
  - Cluster analysis
  - Hierarchical clustering
description: "I do perform two types of clustering on the Kaggle cereals dataset."
banner: "banners/cluster.png"
images: ["banners/cluster.png"]
draft: FALSE
header: 
  image: "banners/cluster.png"
  caption: ""
output:
  blogdown::html_page:
    toc: true
    number_sections: true
    toc_depth: 2
  fig_caption: true
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
x <-
  c("dplyr",
    "knitr",
    "here",
    "scales",
    "magrittr",
    "ggplot2",
    "cowplot",
    "tidyverse",
    "Metrics",
    "rsample",
    "broom",
    "sf",
    "rgdal",
    "summarytools",
    "tmap",
    "tmaptools",
    "lubridate",
    "RColorBrewer",
    "hrbrthemes",
    "ggrepel",
    "dummies",
    "stats",
    "ggrepel",
    "kableExtra",
    "dendextend",
    "cluster")


lapply(x, require, character.only = TRUE)


opts_chunk$set(echo = FALSE,
               warning = FALSE,
               error = FALSE,
               message = FALSE,
               collapse = TRUE,
               comment = NA,
               tidy = TRUE,
               include = FALSE,
               fig.height=10, 
               fig.width=14)

theme_set(theme_ipsum_rc())

options(
  digits=4,
  scipen = 999
)




cereals <- read_csv(here::here("static","data","cereals.csv"))
```

# Summary

*To see the all the code used in this post, visit my [GitHub repository for this site](https://github.com/acastillogill/acastillogill)* 

- Objectives: To find what what kind of clusters are there in the [cereals dataset](https://www.kaggle.com/crawford/80-cereals/home) available on Kaggle. 
- Challenge: To implement a clustering algorithm for the first time.
- Data points: `r length(cereals)*nrow(cereals)`
- Language: R 


# Question

**What types of clusters are there in the cereals dataset?**

# Dataset description 

The dataset is available on Kaggle. It contains nutrition data on 80 cereal products. I chose this dataset because it had plenty of numeric variables and I'm interested in using two types of clustering algorithms to explore the data. Another reason I chose this dataset is because the data is unlabelled so I can add labels with clustering. 

I wanted to focus on numeric variables using Euclidean distances. Not all variables in the dataset were numeric: the cereal name, the manufacturer, the type of cereal (hot or cold). There was also a numeric variable `Shelf` corresponding to the supemarket display shelf that doesn't make much sense as a numeric variable, so I removed it from the analysis. After leaving just numeric variables, I was left with 12 variables. 

I first scaled the data because I will calculate the Euclidean distance between each cereal but the variables are on different scales. When the data are scaled, the mean for each variable will be zero and a standard deviation of one.

```{r scaling-data}
#Remove categorical variables
num_cereals <- cereals[,4:16]
#Remove shelf
num_cereals$shelf <- NULL
#scale numeric variables
num_cereals <- scale(num_cereals)
#Calculate distances
dist_cereals <- dist(num_cereals, method = "euclidean")
```


# Hierarchical clustering


I first implemented hierarchical clustering with three different linkage methods.

- Complete: maximum distance between two sets.
- Single: minimum distance between two sets.
- Average: average distance between two sets.




```{r hierarchical-clustering-complete}

#Hierarchical clustering by complete linkage
hc_cereals_complete <- hclust(dist_cereals, method = "complete")

#Selecting two clusters
clusters_k2_complete <- cutree(hc_cereals_complete, k=2)

#Appending the clusters to all the dataset
cereals_k2_complete <- mutate(cereals, cluster=clusters_k2_complete)

complete_count <- count(cereals_k2_complete, cluster)
```

The complete linkage method will choose the closest observation based on the maximum distance bewteen two sets. I wanted balanced clusters so I ended up choosing the complete linkage method that classified observations more or less into two similar groups.  


```{r hierarchical-clustering-single}

#Hierarchical clustering by single linkage
hc_cereals_single <- hclust(dist_cereals, method = "single")

#Selecting two clusters
clusters_k2_single <- cutree(hc_cereals_single, k=2)

#Appending the clusters to all the dataset
cereals_k2_single <- mutate(cereals, cluster=clusters_k2_single)

single_count <- count(cereals_k2_single, cluster)
```

```{r hierarchical-clustering-average}

#Hierarchical clustering by average linkage
hc_cereals_average <- hclust(dist_cereals, method = "average")

#Selecting two clusters
clusters_k2_average <- cutree(hc_cereals_average, k=2)

#Appending the clusters to all the dataset
cereals_k2_average <- mutate(cereals, cluster=clusters_k2_average)


average_count <- count(cereals_k2_average, cluster)
```

```{r dendrograms, include=TRUE, fig.cap="This tree diagram shows the complete linkage process. It first linked the two observations that were closest together. Then it chose the closest observation based on the maximum distance between the considered observation and its distance to each element in the pair. The height in the plot shows the distance between two observation. Each horizontal line represents the number of clusters. After choosing two clusters, we can say that maximum distance between observations in the two clusters is equal two or less than 11."}

dend_cereals <- as.dendrogram(hc_cereals_complete)
dend_colored <- color_branches(dend_cereals, k=2)
plot(dend_colored)
```



```{r table-counting-sugar-shelf, include=TRUE}

shelf_cluster <- table(cereals_k2_complete$shelf,cereals_k2_complete$cluster)

colnames(shelf_cluster) <- c("Cluster 1","    Cluster 2")

shelf_cluster%>%
  kable(row.names = TRUE, caption="The top shelf (3), has an equal quantity of cereals belonging to either cluster. The bottom shelf has almost twice as doubl cluster 2 cereals. Finally, the middle shelf has almost as three times cluster 2 cereals.")%>%
  kable_styling()
```

```{r mean-for-each-cluster, include=TRUE}
cereals_k2_complete %>% 
  select(-shelf,-name,-mfr,-type)%>%
  group_by(cluster) %>% 
  summarise_all(funs(mean(.)))%>%
  cbind(complete_count$n)%>%
  kable(caption="")%>%
  kable_styling()
```


```{r plot-sugar-shelf, include=TRUE}

 ggplot(cereals_k2_complete, aes(x = sugars, y = shelf, color = factor(cluster))) +
  geom_point(alpha=0.5)+
  scale_color_manual(values=c("#1F78B4" , "#E31A1C"))+
  geom_text_repel(aes(label=name), force=2, size=3, segment.alpha = 0.5)+
    scale_y_continuous(breaks=c(1,2,3))+
  labs(title="Which shelf has the most sugary cereals?",
       x="Sugar content per serving",
       y="Supermarket shelf: from the floor (1) to the top (3)")
  

```

# K-means clustering

The first step of the K-means clustering is to decide how many clusters do we want. This is because the first step of the algorithm is to initialise k-points at random positions, these are called cluster centroids. In a second step, the Euclidean distance between each observation and the centroid is calculated. Observations are assigned to the closest centroid. The third step is to move the centroids to the central points of the resulting clusters. Once this is done, the second and third step are repeated until the centroids don't move any further and the observations are no longer reassigned. 


```{r kmeans-clustering}

#K-means clustering with two clusters
km_cereals <- kmeans(dist_cereals, centers=7)

#Appending the clusters to all the dataset
cereals_km_clusters <- mutate(cereals, cluster=km_cereals$cluster)


kmeans_count <- count(cereals_km_clusters , cluster)

cereals_km_clusters  %>% 
  select(-shelf,-name,-mfr,-type)%>%
  group_by(cluster) %>% 
  summarise_all(funs(mean(.)))%>%
  cbind(kmeans_count$n)
```

```{r plot-kmeans}
 ggplot(cereals_km_clusters, aes(x = sugars, y = vitamins, color = factor(cluster))) +
  geom_point()+
  geom_text_repel(aes(label=name), force=2, size=3, segment.alpha = 0.5)+
  labs(title="Cluster assignement using seven-means",
       x="Sugar content per serving",
       y="Vitamins per serving")
  

```

# Estimating K empirically with the elbow method

This method tries to estimate the number of k's that we should have. It calculates the sum of the distances between each observation and the cluster corresponding to the observation to which each observation is assigned. 

```{r elbow-plot}
tot_withinss <- map_dbl(1:10, function(k){
  
  model <- kmeans(x=dist_cereals, centers=k)
  model$tot.withinss
  
})


elbow_df <- data.frame(
  
  k=1:10,
  tot_withinss=tot_withinss
  
)

ggplot(elbow_df, aes(x=k, y=tot_withinss)) +
  geom_line()+
  geom_point()+
  scale_x_continuous(breaks = 1:10)
```


# Estimating k empirically with silhouette analysis

The silhouette width is a measurement composed by the Within Cluster Distance (each observation to every other observation) and the Closest Neighbor Distance (the closest average distance from that observation to the points of the closest neighboring cluster).

```{r calculating-silhouettes}
sil_width <- map_dbl(2:10, function(k){
  
  model <- pam(x=dist_cereals, k=k)
  model$silinfo$avg.width
  
})

sil_df <- data.frame(
  
  k=2:10,
  
  sil_width=sil_width
  
)

ggplot(sil_df, aes(x=k, y=sil_width)) +
  geom_line()+
  geom_point()+
  scale_x_continuous(breaks = 1:10)
```



# Results

# Conclusion


