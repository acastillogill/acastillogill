---
title: "Why did error go up?"
subtitle: "I chose a better tree and the error went up, why? :confused:"
author: √Ångela Castillo-Gill
date: '2018-11-21'
slug: why-did-error-go-up
categories:
  - Journal
tags: 
  - Learning
draft: FALSE
summary: 
output:
 blogdown::html_page:
  fig_caption: true
editor_options: 
  chunk_output_type: console
---

# Goal's for today's session are:

1. Figure out why the `RMSE` and `MSE` went up when I chose an "improved" tree. 

## Goal 1: Figure out why the `RMSE` and `MSE` went up when I chose an "improved" tree. 

```{r read data, echo=FALSE, message=FALSE}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(knitr)
options(digits=4)
options(scipen = 999)
opts_chunk$set(echo = FALSE,
               warning = FALSE,
               error = FALSE,
               message = FALSE,
               collapse= TRUE,
               comment = NA,
               tidy = TRUE)
theme_set(theme_light())
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
homes <- homes%>%
  select(-Id)
```



```{r }
#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15% 
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
                     prob = c(0.7,0.15,0.15),
                     replace = TRUE)

# Create a train, validation and tests from the original data frame 
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
```

```{r basic-model, fig.cap="A first model suggests 9 splits with 10 nodes.", include=FALSE}
# Train the model
homes_model <- rpart(formula = SalePrice ~ ., 
                     data = homes, 
                     method = "anova")

# Plot the basic model
rpart.plot(x = homes_model, type = 5, extra = 1)
```


```{r tuning-hyperparameters, include=FALSE}
# Plot the "CP Table"
plotcp(homes_model)

# Print the "CP Table"
print(homes_model$cptable)
```

```{r}
#Computing predicted values on the test set (NOT the training test)
pred_base <- predict(object=homes_model,
                newdata = homes_test)

library(Metrics)
rmse_base <- rmse(actual=homes_test$SalePrice, #Actual values
     predicted = pred_base )
mae_base <- mae(actual=homes_test$SalePrice, #Actual values
     predicted = pred_base )
```

The `rmse` for the base model `homes_model` is `r  rmse_base` and the `mae` is `r mae_base`. 

```{r improved-model, fig.cap="Improved model has 7 splits and 8 nodes.", include=FALSE}
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]

# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model, 
                         cp = cp_opt)
                          
# Plot the optimized model
 rpart.plot(x = homes_model_opt, type = 5, extra = 1)
```

```{r}
#Computing predicted values 
pred_opt <- predict(object=homes_model_opt,
                newdata = homes_test)

#Compute RMSE
rmse_opt <- rmse(actual=homes_test$SalePrice, #Actual values
     predicted = pred_opt) #Predicted values

#Compute MAE
mae_opt <- mae(actual=homes_test$SalePrice, #Actual values
    predicted = pred_opt) #Predicted values
```
 
The `rmse` for the improved model `homes_model_opt` is `r  round(rmse_opt, 2)` and the `mae` is `r round(mae_opt,2)`.

This shouldn't be the case, or should it?

Let's think what an increase in these two types of performance metrics mean. In the `rmse` case, yesterday we saw that this is a measure of how much the fit deviates from real values. 
An increase of `r  rmse_opt-rmse_base` means that the pruned model is a worse fit. WHY? Shouldn't a smaller tree prevent overfitting?

I've just posted the question on Stack Overflow because right now I don't have any ideas... maybe there is nothing wrong with performance metrics going up? But that doesn't make sense. Wasn't the whole process of choosing the best `cp_index` to choose a tree with improved performance?

I am very confused. It doesn't make sense. I am going to look at the documentation again to see if I can find anything on performance metrics. 
