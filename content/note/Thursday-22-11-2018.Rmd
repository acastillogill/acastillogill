---
title: "The perfect balance"
subtitle: "Striking a balance between overfitting and underfitting"
author: √Ångela Castillo-Gill
date: '2018-11-22'
slug: the-perfect-balance
categories:
  - Journal
tags: 
  - Learning
draft: FALSE
summary: "Striking a balance between overfitting and underfitting."
output:
 blogdown::html_page:
  fig_caption: true
editor_options: 
  chunk_output_type: console
---

# Daily goals:

1. Discuss the proposed answer for yesterday's question: why did the performance metrics go up?
2. Talk about next steps for the property values post. 

## Goal 1: Discuss the proposed answer for yesterday's question: why did the performance metrics go up?

```{r read data, echo=FALSE, message=FALSE}
library(tidyverse)
library(rpart)
library(rpart.plot)
library(knitr)
options(digits=4)
options(scipen = 999)
opts_chunk$set(echo = FALSE,
               warning = FALSE,
               error = FALSE,
               message = FALSE,
               collapse= TRUE,
               comment = NA,
               tidy = TRUE)
theme_set(theme_light())
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
homes <- homes%>%
  select(-Id)
```

```{r splitting data}
#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15% 
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
                     prob = c(0.7,0.15,0.15),
                     replace = TRUE)

# Create a train, validation and tests from the original data frame 
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
```

```{r basic-model, fig.cap="A first model suggests 9 splits with 10 nodes.", include=FALSE}
# Train the model
homes_model <- rpart(formula = SalePrice ~ ., 
                     data = homes, 
                     method = "anova")

# Plot the basic model
rpart.plot(x = homes_model, type = 5, extra = 1)
```

```{r tuning-hyperparameters, include=FALSE}
# Plot the "CP Table"
plotcp(homes_model)

# Print the "CP Table"
print(homes_model$cptable)
```

```{r performance_metrics_base}
#Computing predicted values on the test set (NOT the training test)
pred_base <- predict(object=homes_model,
                newdata = homes_test)

library(Metrics)
rmse_base <- rmse(actual=homes_test$SalePrice, #Actual values
     predicted = pred_base )
mae_base <- mae(actual=homes_test$SalePrice, #Actual values
     predicted = pred_base )
```

The `rmse` for the base model `homes_model` is `r  rmse_base` and the `mae` is `r mae_base`. 

```{r improved-model, fig.cap="Improved model has 7 splits and 8 nodes.", include=FALSE}
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]

# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model, 
                         cp = cp_opt)
                          
# Plot the optimized model
 rpart.plot(x = homes_model_opt, type = 5, extra = 1)
```

```{r performance_metrics_opt}
#Computing predicted values 
pred_opt <- predict(object=homes_model_opt,
                newdata = homes_test)

#Compute RMSE
rmse_opt <- rmse(actual=homes_test$SalePrice, #Actual values
     predicted = pred_opt) #Predicted values

#Compute MAE
mae_opt <- mae(actual=homes_test$SalePrice, #Actual values
    predicted = pred_opt) #Predicted values
```
 
The `rmse` for the improved model `homes_model_opt` is `r  round(rmse_opt, 2)` and the `mae` is `r round(mae_opt,2)`.

Yesterday I was pretty annoyed because I didn't understand why the smallest tree with lowest cross validated error had worse performance metrics than a large tree. Like, mmm, why?

So I kind of felt I was hitting a dead end and decided to take to trusty old Stack Overflow to ASK FOR HELP. I asked and I received. 

I received two helpful in less than twelve hours (Stack Overflow never change). The first suggested I consult the book Introduction to Statistical Learning from Springer. I have obtained it and it will be my reference book right after the one I'm currently doing called Regression Models. I have looked at the Table of Contents and it has a chapter dedicated to trees so that will come very much in handy.

The second answer was this: 

> There is always a balance between a tree big enough to represent the variation in the data and not so big that it overfits. The reason that bigger trees sometimes produce better results is that they more finely partition the data and so represent nuances. The reason that smaller trees sometimes produce better results is that there is less of a problem with overfitting. But if the smallest tree was always the best, the why not just use one node? Just using the root node would estimate the value using the average - not likely to be really accurate. The two conflicting forces must be balanced to get the best result.

My initial thoughts yesterday were that because a smaller tree, was ummm, well, smaller, it would have better performance on unseen data than a larger tree. 

That line they use: 

> But if the smallest tree was always the best, the why not just use one node?

Really captures my inital assumption. Exactly, if a smaller tree is always better, why not just use the smallest tree possible, the root node. 

And so the mystery is partially solved in that actually the bigger tree did perform better. 

I will consult the book chapter now to see if I can see any immediately useful tips. Wow! That book is super helfpul and explains well how the `RPart` algorithm works. I look forward to moving on to it soon. 

## Goal 2: Talk about next steps for the property values post. 

So now that we've decided that the larger tree is better, there is also an option where we use the data split we haven't used to tune the model's hyperparamters that we talked about in previous posts. 

I would think the next steps are to do that with the tree that performs better and see if we can get any improvements.

## Grid search for best hyperparameters [^1]

[^1]: Code from here until the end of the post was adapted from DataCamp's course on Machine Learning with Trees. 

```{r set up the grid, results=FALSE, echo=TRUE}

#Establish a list of possible values for minsplit and maxdepth
#From 1 to 30 by 5
min_split <- seq(1,10,1)
#From 1 to 30 by 1
max_depth <- seq(1, 30, 1)

#Create a dataframe containing all combinations
hyper_grid <- expand.grid(min_split=min_split,
                          max_depth=max_depth)

#Check dimensions
dim(hyper_grid)
```

```{r create models, results=FALSE, echo=TRUE}
#Create an empty list to store the models

models <- list()

#Execute the grid search

for (i in 1:nrow(hyper_grid)){
  
  #Get min_split, max_depth values at row i
  
  minsplit <- hyper_grid$min_split[i]
  max_depth <- hyper_grid$max_depth[i]
  
  #Train the model and store in the list
  
  models[[i]] <- rpart(formula = SalePrice~.,
                       data = homes_train,
                       method = "anova",
                       minsplit=min_split,
                       maxdepth=max_depth)
  
}
```

```{r evaluating models in grid, results=FALSE, echo=TRUE}
#Create an empty vector to store RMSE values
rmse_values <- c()
#Create an empty vector to store MAE values
mae_values <- c()

#Compute validation RMSE 
for (i in 1:length(models)){
  
  #Retrieve the ith model from the list
  
  model <- models[[i]]
  
  #Generate predictions on homes_valid validation set
  
  pred_grid <- predict(object=model,
                       newdata=homes_valid)
  
  #Compute validation RMSE and add to the 
  
  rmse_values[i] <- rmse(actual = homes_valid$SalePrice,
                         predicted = pred_grid)
  
    mae_values[i] <- mae(actual = homes_valid$SalePrice,
                         predicted = pred_grid)
  
  
}
```

```{r picking best model, results=FALSE, echo=TRUE}
#Identifying the model with smallest validation set RMSE
best_model_RMSE <- models[[which.min(rmse_values)]]
#Print the model parameters of the best_model_RMSE
best_model_RMSE$control
#Compute test set RMSE on best_model_RMSE
pred <- predict(object=best_model_RMSE,
                newdata=homes_test)

rmse(actual = homes_test$SalePrice,
     predicted = pred)

#Identifying the model with smallest validation set MAE
best_model_MAE <- models[[which.min(mae_values)]]
#Print the model parameters of the best_model_MAE
best_model_MAE$control
#Compute test set MAE on best_model_MAE
pred <- predict(object=best_model_MAE,
                newdata=homes_test)

mae(actual = homes_test$SalePrice,
     predicted = pred)
```

```{r}
rpart.plot(best_model_MAE)
rpart.plot(best_model_RMSE)
```

There is something wrong I think because the best tree after grid search has an `RMSE` higher than the base tree. And then if we look at the RMSE values resulting from the grid search they are all the same so it seems like the tree doesn't attempt any splits. WHY?
Another why? 

I think instead of doing this grid search which needs a proper study, I'm going to write up the full post and everything I've done with the Kaggle dataset. Better to talk about the interpretability and the next bits too. 
