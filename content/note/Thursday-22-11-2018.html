---
title: "The perfect balance"
subtitle: "Striking a balance between overfitting and underfitting"
author: Ángela Castillo-Gill
date: '2018-11-22'
slug: the-perfect-balance
categories:
  - Journal
tags: 
  - Learning
draft: FALSE
summary: "Striking a balance between overfitting and underfitting."
output:
 blogdown::html_page:
  fig_caption: true
editor_options: 
  chunk_output_type: console
---



<div id="daily-goals" class="section level1">
<h1>Daily goals:</h1>
<ol style="list-style-type: decimal">
<li>Discuss the proposed answer for yesterday’s question: why did the performance metrics go up?</li>
<li>Talk about next steps for the property values post.</li>
</ol>
<div id="goal-1-discuss-the-proposed-answer-for-yesterdays-question-why-did-the-performance-metrics-go-up" class="section level2">
<h2>Goal 1: Discuss the proposed answer for yesterday’s question: why did the performance metrics go up?</h2>
<p>The <code>rmse</code> for the base model <code>homes_model</code> is 36595.3198 and the <code>mae</code> is 25784.2432.</p>
<p>The <code>rmse</code> for the improved model <code>homes_model_opt</code> is 40848.44 and the <code>mae</code> is 28394.1.</p>
<p>Yesterday I was pretty annoyed because I didn’t understand why the smallest tree with lowest cross validated error had worse performance metrics than a large tree. Like, mmm, why?</p>
<p>So I kind of felt I was hitting a dead end and decided to take to trusty old Stack Overflow to ASK FOR HELP. I asked and I received.</p>
<p>I received two helpful in less than twelve hours (Stack Overflow never change). The first suggested I consult the book Introduction to Statistical Learning from Springer. I have obtained it and it will be my reference book right after the one I’m currently doing called Regression Models. I have looked at the Table of Contents and it has a chapter dedicated to trees so that will come very much in handy.</p>
<p>The second answer was this:</p>
<blockquote>
<p>There is always a balance between a tree big enough to represent the variation in the data and not so big that it overfits. The reason that bigger trees sometimes produce better results is that they more finely partition the data and so represent nuances. The reason that smaller trees sometimes produce better results is that there is less of a problem with overfitting. But if the smallest tree was always the best, the why not just use one node? Just using the root node would estimate the value using the average - not likely to be really accurate. The two conflicting forces must be balanced to get the best result.</p>
</blockquote>
<p>My initial thoughts yesterday were that because a smaller tree, was ummm, well, smaller, it would have better performance on unseen data than a larger tree.</p>
<p>That line they use:</p>
<blockquote>
<p>But if the smallest tree was always the best, the why not just use one node?</p>
</blockquote>
<p>Really captures my inital assumption. Exactly, if a smaller tree is always better, why not just use the smallest tree possible, the root node.</p>
<p>And so the mystery is partially solved in that actually the bigger tree did perform better.</p>
<p>I will consult the book chapter now to see if I can see any immediately useful tips. Wow! That book is super helfpul and explains well how the <code>RPart</code> algorithm works. I look forward to moving on to it soon.</p>
</div>
<div id="goal-2-talk-about-next-steps-for-the-property-values-post." class="section level2">
<h2>Goal 2: Talk about next steps for the property values post.</h2>
<p>So now that we’ve decided that the larger tree is better, there is also an option where we use the data split we haven’t used to tune the model’s hyperparamters that we talked about in previous posts.</p>
<p>I would think the next steps are to do that with the tree that performs better and see if we can get any improvements.</p>
</div>
<div id="grid-search-for-best-hyperparameters-1" class="section level2">
<h2>Grid search for best hyperparameters <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></h2>
<pre class="r"><code>
# Establish a list of possible values for minsplit and maxdepth From 1 to 30
# by 5
min_split &lt;- seq(1, 10, 1)
# From 1 to 30 by 1
max_depth &lt;- seq(1, 30, 1)

# Create a dataframe containing all combinations
hyper_grid &lt;- expand.grid(min_split = min_split, max_depth = max_depth)

# Check dimensions
dim(hyper_grid)</code></pre>
<pre class="r"><code># Create an empty list to store the models

models &lt;- list()

# Execute the grid search

for (i in 1:nrow(hyper_grid)) {
    
    # Get min_split, max_depth values at row i
    
    minsplit &lt;- hyper_grid$min_split[i]
    max_depth &lt;- hyper_grid$max_depth[i]
    
    # Train the model and store in the list
    
    models[[i]] &lt;- rpart(formula = SalePrice ~ ., data = homes_train, method = &quot;anova&quot;, 
        minsplit = min_split, maxdepth = max_depth)
    
}</code></pre>
<pre class="r"><code># Create an empty vector to store RMSE values
rmse_values &lt;- c()
# Create an empty vector to store MAE values
mae_values &lt;- c()

# Compute validation RMSE
for (i in 1:length(models)) {
    
    # Retrieve the ith model from the list
    
    model &lt;- models[[i]]
    
    # Generate predictions on homes_valid validation set
    
    pred_grid &lt;- predict(object = model, newdata = homes_valid)
    
    # Compute validation RMSE and add to the
    
    rmse_values[i] &lt;- rmse(actual = homes_valid$SalePrice, predicted = pred_grid)
    
    mae_values[i] &lt;- mae(actual = homes_valid$SalePrice, predicted = pred_grid)
    
    
}</code></pre>
<pre class="r"><code># Identifying the model with smallest validation set RMSE
best_model_RMSE &lt;- models[[which.min(rmse_values)]]
# Print the model parameters of the best_model_RMSE
best_model_RMSE$control
# Compute test set RMSE on best_model_RMSE
pred &lt;- predict(object = best_model_RMSE, newdata = homes_test)

rmse(actual = homes_test$SalePrice, predicted = pred)

# Identifying the model with smallest validation set MAE
best_model_MAE &lt;- models[[which.min(mae_values)]]
# Print the model parameters of the best_model_MAE
best_model_MAE$control
# Compute test set MAE on best_model_MAE
pred &lt;- predict(object = best_model_MAE, newdata = homes_test)

mae(actual = homes_test$SalePrice, predicted = pred)</code></pre>
<p><img src="/note/Thursday-22-11-2018_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>There is something wrong I think because the best tree after grid search has an <code>RMSE</code> higher than the base tree. And then if we look at the RMSE values resulting from the grid search they are all the same so it seems like the tree doesn’t attempt any splits. WHY?
Another why?</p>
<p>I think instead of doing this grid search which needs a proper study, I’m going to write up the full post and everything I’ve done with the Kaggle dataset. Better to talk about the interpretability and the next bits too.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Code from here until the end of the post was adapted from DataCamp’s course on Machine Learning with Trees.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
