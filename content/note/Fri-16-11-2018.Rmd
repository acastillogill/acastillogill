---
title: "Did the tree improve?"
subtitle: "Seems like it :smile:"
author: √Ångela Castillo-Gill
date: '2018-11-16'
slug: did-the-tree-improve
categories:
  - Journal
tags: 
  - Learning
draft: FALSE
summary: "Seems like it :smile:"
output:
 blogdown::html_page:
  fig_caption: true
editor_options: 
  chunk_output_type: console
---

# Goal's for today's session are:

1. Show if the variable change made the tree more interpretable. 

```{r read data, echo=FALSE, message=FALSE}
library(tidyverse)
library(rpart)
library(rpart.plot)
homes <- readRDS(file=here::here("static","data","homes.rds"))
```

```{r}
#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15% 
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
                     prob = c(0.7,0.15,0.15),
                     replace = TRUE)

# Create a train, validation and tests from the original data frame 
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
```

```{r basic-model, fig.cap="A first model suggests 9 splits with 10 nodes."}
# Train the model
homes_model <- rpart(formula = SalePrice ~ ., 
                     data = homes_train, 
                     method = "anova")

# Plot the basic model
rpart.plot(x = homes_model, type = 5, extra = 1)
```

Now we are going to look at the CP table. 

```{r tuning-hyperparameters}
# Plot the "CP Table"
plotcp(homes_model)

# Print the "CP Table"
print(homes_model$cptable)
```

According the `cptable`, the lowest `xerror` is in row 10, to that we add the `xstd` error and obtain: $0.39529$. We see that the smallest error with `xerror` below that is the tree with 7 splits and 8 nodes. I will use that in the improved plot.  

```{r improved-model, fig.cap="Improved model has 7 splits and 8 nodes."}
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]

# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model, 
                         cp = cp_opt)
                          
# Plot the optimized model
 rpart.plot(x = homes_model_opt, type = 5, extra = 1)
```

Tomorrow, I will look to see if there are any objective measures of error, clue, there are! And apply them to compare both models. 
