---
title: "Applying concepts learned to property prices"
subtitle: "and also, rpart plots and the problem with so many factors"
author: √Ångela Castillo-Gill
date: '2018-11-13'
slug: applying-concepts-learned-to-property-prices
categories:
  - Journal
tags: 
  - Learning
draft: FALSE
summary: 
output:
 blogdown::html_page
---

# Goal's for today's session are:

1. Interpret the `cptable` for the generic `homes_model`.

2. Interpret the `rpart.plot` associated to `homes_model`.

3. Bin the `neighborhood` variable and explain why I'm doing that.

## Goal 1: Interpret the `cptable` for the generic `homes_model`.

With the `homes` data as is (just removing missing values), I fited a model to predict `SalesPrice` using `rpart()`:


```{r, echo=TRUE, eval=FALSE}
homes_model <- rpart(formula = SalePrice ~ ., 
                     data = homes_train, 
                     method = "anova")
```

Looking at the `cptable` with `homes_model$cptable`, I'll apply the $1-SE$ rule I explained yesterday: 

![](/note/Tues-13-11-2018_files/cptable_rawhomes.png)

First, we can see that the smallest cross-validation `xerror` appears on row 11. Now let's add 1 standard error `xstd`: $0.3074050+0.03541012=0.3428151$.

The smallest tree with `xerror` less than $0.3428151$ is the tree appearing on row 8 with 8 splits and 9 nodes. 

Now, I look at the `plotcp(homes_model)`, I'm confused to whether size of the tree refers to nodes or splits:

![](/note/Tues-13-11-2018_files/cpplot_rawhomes.png)

I think it refers to nodes because the `xerror` for the tree with 9 nodes appears on the dotted line. A dotted line that intersects the 9-tree node must mean it's the chosen one, right? :laughing:

Now, let's run the model again, but specifying the best model, with 8 splits.

```{r, echo=TRUE, eval=FALSE}
# Hard code best tree based on above explained analysis
opt_index <- 8
#Retrieve cp_opt
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model, 
                         cp = cp_opt)
```

I suppose hard coding the number 8 instead of writing the code to pick the best tree isn't too elegant, but bear with me. Writing clean code is not the priority right now. :hankey:

## Goal 2: Interpret the `rpart.plot` associated to `homes_model`.

```{r, echo=TRUE, eval=FALSE}
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
```

That code produces this plot: 

![](/note/Tues-13-11-2018_files/rpartplot_rawhomes.png)

It has 8 splits and 9 nodes and we asked it to. Inside each terminal node is the number of observations which add up to 1025. Each split specifies the criteria for the observation being classified either into the left or right son. 

If we go back and look at the numeric variables that had the highest correlation with `SalesPrice`, `OverallQual` was leading followed by `GrLivArea`. In the tree we can see that these variables appear further below `X1stFlrSF` and `GarageArea` appear later down. 

The plot shows the `neighborhood` variable twice and in the second level. So it's an important variable. 

## Goal 3: Bin the `neighborhood` variable and explain why I'm doing that.

So today I don't think I'll be able to the binning since my alloted time for this is almost up. But I will explain the why since it appears clearly on the graph.

One of the advantages of regression trees is that they area easy to understand and visualise. Because the `neighborhood` variable has so many levels, it is actually going against one of the reasons I picked it. The whole neighborhood variable is confusing. Explaining the key differences amongst neighborhoods is not straightforward using just the plot. So I'm going to explore what the relationship between those neighborhoods and price is tomorrow and see if I can create another variable that better reflects that relationship. Exciting! :sparkles: