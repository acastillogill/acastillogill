#Does mcnb$scientificname match by first word daisie$scientificname?
#Step 1: extract the first word mus
#Create vector first_word to museo nacional
mcnb$word<- word(mcnb$species, 1,2)
# Step 2: match mcnb$word to the daisie$scientificname
after_1950 %>%
fuzzy_inner_join(mcnb,
by=c("species"="word"),
match_fun = str_detect)%>%
filter(!is.na(locality)|!=="")%>%
dplyr::count(scientificname,locality,year,sort=TRUE)
names(after_1950)
#Does mcnb$scientificname match by first word daisie$scientificname?
#Step 1: extract the first word mus
#Create vector first_word to museo nacional
mcnb$word<- word(mcnb$species, 1,2)
# Step 2: match mcnb$word to the daisie$scientificname
after_1950 %>%
fuzzy_inner_join(mcnb,
by=c("species"="word"),
match_fun = str_detect)%>%
filter(!is.na(locality)|!=="")%>%
dplyr::count(scientificname,locality,year,sort=TRUE)
#Does mcnb$scientificname match by first word daisie$scientificname?
#Step 1: extract the first word mus
#Create vector first_word to museo nacional
mcnb$word<- word(mcnb$species, 1,2)
# Step 2: match mcnb$word to the daisie$scientificname
after_1950 %>%
fuzzy_inner_join(mcnb,
by=c("species"="word"),
match_fun = str_detect)%>%
filter(!is.na(locality)|locality!="")
#Does mcnb$scientificname match by first word daisie$scientificname?
#Step 1: extract the first word mus
#Create vector first_word to museo nacional
mcnb$word<- word(mcnb$species, 1,2)
# Step 2: match mcnb$word to the daisie$scientificname
after_1950 %>%
fuzzy_inner_join(mcnb,
by=c("species"="word"),
match_fun = str_detect)%>%
filter(!is.na(locality)|locality!="")%>%
dplyr::count(scientificname,locality,year,sort=TRUE)
#Does mcnb$scientificname match by first word daisie$scientificname?
#Step 1: extract the first word mus
#Create vector first_word to museo nacional
mcnb$word<- word(mcnb$species, 1,2)
# Step 2: match mcnb$word to the daisie$scientificname
after_1950 %>%
fuzzy_inner_join(mcnb,
by=c("species"="word"),
match_fun = str_detect)%>%
filter(!is.na(locality)&&locality!="")%>%
dplyr::count(scientificname,
locality,
year,
sort=TRUE)
#Does mcnb$scientificname match by first word daisie$scientificname?
#Step 1: extract the first word mus
#Create vector first_word to museo nacional
mcnb$word<- word(mcnb$species, 1,2)
# Step 2: match mcnb$word to the daisie$scientificname
after_1950 %>%
fuzzy_inner_join(mcnb,
by=c("species"="word"),
match_fun = str_detect)%>%
filter(!is.na(locality)&locality!="")%>%
dplyr::count(scientificname,
locality,
year,
sort=TRUE)
#Does uni_lisboa$scientificname match by first word daisie$scientificname?
#Step 1: extract the first word mus
#Create vector first_word to museo nacional
uni_lisboa$word<- word(uni_lisboa$scientificname, 1,2)
# Step 2: match uni_lisboa$word to the daisie$scientificname
after_1950 %>%
fuzzy_inner_join(uni_lisboa,
by=c("species"="word"),
match_fun = str_detect)%>%
filter(!is.na(locality)&locality!="")%>%
dplyr::count(scientificname,
locality,
year,
sort=TRUE)
#Does oviedo$scientificname match by first word daisie$scientificname?
#Step 1: extract the first word mus
#Create vector first_word to museo nacional
oviedo$word<- word(oviedo$scientificname, 1,2)
# Step 2: match oviedo$word to the daisie$scientificname
after_1950 %>%
fuzzy_inner_join(oviedo,
by=c("species"="word"),
match_fun = str_detect)%>%
filter(!is.na(locality)&locality!="")%>%
dplyr::count(scientificname,
locality,
year,
sort=TRUE)
#Does museo_nacional$scientificname match by first word daisie$scientificname?
#Step 1: extract the first word mus
#Create vector first_word to museo nacional
museo_nacional$word<- word(museo_nacional$scientificname, 1,2)
# Step 2: match museo_nacional$word to the daisie$scientificname
after_1950 %>%
fuzzy_inner_join(museo_nacional,
by=c("species"="word"),
match_fun = str_detect)%>%
filter(!is.na(locality)&locality!="")%>%
dplyr::count(scientificname,
locality,
year,
sort=TRUE)
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
# Chunk 1: setup
x <-
c("tidyverse",
"knitr",
"formatR",
"stringr",
"quantmode",
"skimr",
"lubridate",
"tidyr",
"formattable",
"grid",
"gridExtra",
"kableExtra",
"corrplot")
lapply(x, require, character.only = TRUE)
opts_chunk$set(echo = FALSE,
warning = FALSE,
error = FALSE,
message = FALSE,
collapse= TRUE,
comment = NA,
tidy = TRUE)
# Chunk 2
path <- "/Volumes/TOSHIBAEXT/RStudio/blog/content/post/rossman_sales/"
#Read `trained_filled_gap.csv`
rossman <- read_csv(paste0(path,"train.csv"))
# Chunk 3
#See dataset structure
str(rossman, give.attr = FALSE)
# Chunk 4
# Visualization of correlations
rossman %>% select_if(is.numeric) %>%
dplyr::select(-Store,-DayOfWeek) %>%
cor()%>%
corrplot()
# Chunk 5
# Frequent days of week
ggplot(rossman) +
geom_boxplot(aes(x = as.character(DayOfWeek), y = Sales))
# Chunk 6
# Frequent days of week
ggplot(rossman) +
geom_boxplot(aes(x = as.character(Promo), y = Sales))
# Chunk 7
blogdown::serve_site()
blogdown::serve_site()
#See dataset structure
str(rossman, give.attr = FALSE)
length(rossman$Date)
year(DAte)
year(rossman$Date)
unique(year(rossman$Date))
# Plot a histogram
ggplot(rossman) +
geom_histogram(aes(x = Customers,
fill = factor(Open))) +
facet_grid( ~ Open) + # Separate plots for Open = 1 vs. 0
theme(legend.position = "none") # Don't show legend
# Plot a histogram
ggplot(rossman) +
geom_histogram(aes(x = Customers,
fill = factor(Promo))) +
facet_grid( ~ Promo) + # Separate plots for Open = 1 vs. 0
theme(legend.position = "none") # Don't show legend
# Plot a histogram
ggplot(rossman) +
geom_histogram(aes(x = Customers,
fill = factor(StateHoliday))) +
facet_grid( ~ StateHoliday) + # Separate plots for StateHoliday = 1 vs. 0
theme(legend.position = "none") # Don't show legend
# Plot a histogram
ggplot(rossman) +
geom_histogram(aes(x = Customers,
fill = factor(SchoolHoliday))) +
facet_grid( ~ SchoolHoliday) + # Separate plots for SchoolHoliday = 1 vs. 0
theme(legend.position = "none") # Don't show legend
rossman%>%
group_by(year(Date))
rossman%>%
group_by(year(Date))%>%
mutate(anual_sales=sum(Sales))
rossman%>%
aggregate(Sales ~ year(Date), sum)
rossman%>%
aggregate(Sales ~ year(Date) + Store, sum)
args(aggregate)
?aggregate
aggregate (. ~ Date, data = rossman, sum)
#Separate Date into Year Month Day
cbind(rossman, year=year(rossman$Date),
month=month(rossman$Date),
day=day(rossman$Date))
#Separate Date into Year Month Day
rossman_date <- cbind(rossman, year=year(rossman$Date),
month=month(rossman$Date),
day=day(rossman$Date))
aggregate (. ~ Year, data = rossman_date, sum)
aggregate (. ~ year, data = rossman_date, sum)
aggregate (. ~ year + ID + month, data = rossman_date, sum)
aggregate (. ~ year + Store + month, data = rossman_date, sum)
aggregate (. ~ year + Store , data = rossman_date, sum)
#Calculating annual sales and customers per per store
aggregate (. ~ year + Store , data = rossman_date, sum)%>%
select(year, Store, Sales, Customers)
?spread
#Calculating annual sales and customers per per store
aggregate (. ~ year + Store , data = rossman_date, sum)%>%
#Selecting columns that make sense to aggregate (non-boolean)
select(year, Store, Sales, Customers)%>%
spread(key = Store,
value = year)
#Calculating annual sales and customers per per store
aggregate (. ~ year + Store , data = rossman_date, sum)%>%
#Selecting columns that make sense to aggregate (non-boolean)
select(year, Store, Sales, Customers)%>%
spread(key = year,
value = c(Sales,Customers))
#Calculating annual sales and customers per per store
aggregate (. ~ year + Store , data = rossman_date, sum)%>%
#Selecting columns that make sense to aggregate (non-boolean)
select(year, Store, Sales, Customers)%>%
spread(key = year)
#Calculating annual sales and customers per per store
aggregate (. ~ year + Store , data = rossman_date, sum)%>%
#Selecting columns that make sense to aggregate (non-boolean)
select(year, Store, Sales, Customers)%>%
spread(year, Store, Sales, Customers)
#Calculating annual sales and customers per per store
aggregate (. ~ year + Store , data = rossman_date, sum)%>%
#Selecting columns that make sense to aggregate (non-boolean)
select(year, Store, Sales, Customers)
#Calculating annual sales and customers per per store
aggregate (Sales ~ year + Store , data = rossman_date, sum)%>%
#Calculating annual sales and customers per per store
aggregate (Sales ~ year + Store , data = rossman_date, sum)
#Calculating annual sales and customers per per store
aggregate (Sales ~ year + Store , data = rossman_date, sum)%>%
#Selecting columns that make sense to aggregate (non-boolean)
select(year, Store, Sales, Customers)%>%
spread(key=year)
#Calculating annual sales and customers per per store
aggregate (Sales ~ year + Store , data = rossman_date, sum)%>%
#Selecting columns that make sense to aggregate (non-boolean)
select(year, Store, Sales, Customers)%>%
spread(key=year, value = c(Store,Sales,Customers))
#Calculating annual sales and customers per per store
aggregate (Sales ~ year + Store , data = rossman_date, sum)%>%
#Selecting columns that make sense to aggregate (non-boolean)
select(year, Store, Sales, Customers)%>%
spread(key=year, value = Sales)
#Calculating annual sales and customers per per store
aggregate (Sales ~ year + Store , data = rossman_date, sum)%>%
#Selecting columns that make sense to aggregate (non-boolean)
dplyr::select(year, Store, Sales, Customers)%>%
spread(key=year, value = Sales)
#Calculating annual sales and customers per per store
aggregate (Sales ~ year + Store , data = rossman_date, sum)
#Calculating annual sales and customers per per store
aggregate (. ~ year + Store , data = rossman_date, sum)%>%
#Selecting columns that make sense to aggregate (non-boolean)
dplyr::select(year, Store, Sales, Customers)%>%
spread(key=year, value = Sales)
#Calculating annual sales and customers per per store
aggregate (. ~ year + Store , data = rossman_date, sum)%>%
#Selecting columns that make sense to aggregate (non-boolean)
dplyr::select(year, Store, Sales, Customers)%>%
spread(key=year, value = Store)
#Calculating annual sales and customers per per store
aggregate (. ~ year + Store , data = rossman_date, sum)%>%
#Selecting columns that make sense to aggregate (non-boolean)
dplyr::select(year, Store, Sales, Customers)%>%
spread(key=year, value = Sales)
#Calculating annual sales and customers per per store
aggregate (Sales ~ year + Store , data = rossman_date, sum)%>%
spread(key=year, value = Sales)
#Calculating annual sales and customers per per store
annual_sales <-  aggregate (Sales ~ year + Store,
data = rossman_date,
sum)%>%
spread(key=year,
value = Sales)
ggplot(annual_sales, aes(x=2013,y=2014))
geom_point()
ggplot(annual_sales, aes(x=2013,y=2014))+
geom_point()
str(annual_sales)
colnames(annual_sales) <- c(Store,
sales_2013,
sales_2014,
sales_2015)
#Calculating annual sales and customers per per store
annual_sales <-  aggregate (Sales ~ year + Store,
data = rossman_date,
sum)%>%
spread(key=year,
value = Sales)
#Changing names
colnames(annual_sales) <- c("Store",
"sales_2013",
"sales_2014",
"sales_2015")
str(annual_sales)
ggplot(annual_sales, aes(x=sales_2013,y=sales_2014))+
geom_point()
ggplot(annual_sales, aes(x=sales_2014,y=sales_2015))+
geom_point()
#Checking linear relationship
ggplot(annual_sales, aes(x=sales_2014,
y=sales_2015)+
geom_point()
salesSimpleModel <- lm(sales_2014 ~ sales_2013,
data = annual_sales)
#Checking linear relationship
ggplot(annual_sales, aes(x=sales_2013,
y=sales_2014))+geom_point()
#Checking linear relationship
ggplot(annual_sales, aes(x=sales_2013,
y=sales_2014))+geom_point()
#Checking linear relationship
ggplot(annual_sales, aes(x=sales_2013,
y=sales_2014))+geom_point()
#Checking linear relationship
ggplot(annual_sales, aes(x=sales_2013,
y=sales_2014))+geom_point()
#Checking linear relationship
ggplot(annual_sales, aes(x=sales_2013,
y=sales_2014))+geom_point()
salesSimpleModel <- lm(sales_2014 ~ sales_2013,
data = annual_sales)
salesSimpleModel <- lm(sales_2014 ~ sales_2013,
data = annual_sales)
summary(salesSimpleModel)
ggplot(annual_sales,aes(x=sales_2013))+
geom_density()
ggplot(annual_sales,aes(x=sales_2014))+
geom_density()
ggplot(annual_sales,aes(x=sales_2015))+
geom_density()
path <- "/Volumes/TOSHIBAEXT/RStudio/blog/content/post/rossman_sales/"
#Read `trained_filled_gap.csv`
rossman <- read_csv(paste0(path,"train.csv"))
#Read `store.csv`
rossman <- read_csv(paste0(path,"store.csv"))
rossman <- read_csv(paste0(path,"store.csv"))
path <- "/Volumes/TOSHIBAEXT/RStudio/blog/content/post/rossman_sales/"
#Read `trained_filled_gap.csv`
rossman <- read_csv(paste0(path,"train.csv"))
#Read `store.csv`
store <- read_csv(paste0(path,"store.csv"))
#See dataset structure
str(rossman, give.attr = FALSE)
str(store, give.attr = FALSE)
#Joining store characteristics with sales data
left_join(annual_sales,store,by=Store)
names(annual_sales)
#Joining store characteristics with sales data
left_join(annual_sales,store,by="Store")
#Joining store characteristics with sales data
full_store <- left_join(annual_sales,store,by="Store")
# Visualization of correlations
full_store %>% select_if(is.numeric) %>%
dplyr::select(-Store,-DayOfWeek) %>%
cor()%>%
corrplot()
# Visualization of correlations
full_store %>% select_if(is.numeric) %>%
dplyr::select(-Store) %>%
cor()%>%
corrplot()
which(is.na(full_store$CompetitionDistance))
which(is.na(full_store$CompetitionOpenSinceYear))
# Visualization of correlations
full_store %>% select_if(is.numeric) %>%
filter(!is.na(CompetitionDistance))%>%
dplyr::select(-Store) %>%
cor()%>%
corrplot()
# Visualization of correlations
full_store %>% select_if(is.numeric) %>%
filter(is.na(CompetitionDistance))%>%
dplyr::select(-Store) %>%
cor()%>%
corrplot()
# Visualization of correlations
full_store %>% select_if(is.numeric) %>%
filter(is.na(CompetitionDistance))
# Visualization of correlations
full_store %>% select_if(is.numeric) %>%
filter(!is.na(CompetitionDistance))
# Visualization of correlations
full_store %>% select_if(is.numeric) %>%
filter(!is.na(CompetitionDistance))%>%
dplyr::select(-Store) %>%
cor()%>%
corrplot()
# Visualization of correlations
full_store %>% select_if(is.numeric) %>%
filter(!is.na(CompetitionDistance))%>%
dplyr::select(-Store,
Promo2SinceWeek,
Promo2SinceYear,
CompètitionOpenSinceYear) %>%
cor()%>%
corrplot()
# Visualization of correlations
full_store %>% select_if(is.numeric) %>%
filter(!is.na(CompetitionDistance))%>%
dplyr::select(-Store,
Promo2SinceWeek,
Promo2SinceYear,
CompetitionOpenSinceYear) %>%
cor()%>%
corrplot()
# Visualization of correlations
full_store %>% select_if(is.numeric) %>%
filter(!is.na(CompetitionDistance))%>%
dplyr::select(-Store,
-Promo2SinceWeek,
-Promo2SinceYear,
-CompetitionOpenSinceYear)%>%
cor()%>%
corrplot()
#Looking at the distribution of each variable
ggplot(full_store,aes(x=CompetitionDistance))+
geom_density()
#Looking at the distribution of each variable
ggplot(full_store,aes(x=CompetitionDistance,y=sales_2013))+
geom_point()
#Looking at the distribution of each variable
ggplot(full_store,aes(x=CompetitionDistance,y=sales_2014))+
geom_point()
#Looking at the distribution of each variable
ggplot(full_store,aes(x=CompetitionDistance,y=sales_2015))+
geom_point()
# Visualization of correlations
full_store %>% select_if(is.numeric) %>%
filter(!is.na(CompetitionDistance))%>%
dplyr::select(-Store,
-Promo2SinceWeek,
-Promo2SinceYear,
-CompetitionOpenSinceYear)%>%
cor()%>%
corrplot()
#Making big dataset
full_store <- left_join(rossman,store,by="Store")
#See dataset structure
str(rossman, give.attr = FALSE)
#See dataset structure
str(full_store, give.attr = FALSE)
summary(full_store)
#Removing NA's
full_store <- na.omit(full_store)
summary(full_store)
#See dataset structure
str(full_store, give.attr = FALSE)
# Visualization of correlations
full_store %>% select_if(is.numeric) %>%
dplyr::select(-Store)%>%
cor()%>%
corrplot()
install.packages("rms")
library(rms)
#Checking for multicollinearity
simple_model <- lm(sales  ~ -Store - StateHoliday - Promo 2,
data = annual_sales)
#Checking for multicollinearity
simple_model <- lm(Sales  ~ - Store - StateHoliday - Promo 2,
data = annual_sales)
#Checking for multicollinearity
simple_model <- lm(Sales  ~ - Store - StateHoliday - Promo2,
data = annual_sales)
#Checking for multicollinearity
simple_model <- lm(Sales  ~ - Store - StateHoliday - Promo2,
data = annual_sales)
#Checking for multicollinearity
simple_model <- lm(Sales  ~ - Store - StateHoliday - Promo2,
data = full_store)
summary(simple_model)
#Checking for multicollinearity
simple_model <- lm(Sales  ~ . - Store - StateHoliday - Promo2,
data = full_store)
summary(simple_model)
#Checking for multicollinearity
vif(simple_model)
#Making model
simple_model <- lm(Sales  ~ . - Store - StateHoliday - Promo2 - Date,
data = full_store)
#Checking for multicollinearity
vif(simple_model)
#Making model
simple_model <- lm(Sales  ~ . - Store - StateHoliday - Promo2 - Date,
data = full_store)
summary(simple_model)
