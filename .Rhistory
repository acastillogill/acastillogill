plot(round(dfbetas(fit)[,2],3))
plot(round(cooks.distance(fit),3))
data(swiss);
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
a <- summary(fit1)$cov.unscaled[2,2]
fit2 <- update(fit, Fertility ~ Agriculture + Examination)
str(swiss)
library(car)
data(swiss);
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
a <- summary(fit1)$cov.unscaled[2,2]
fit2 <- update(fit, Fertility ~ Agriculture + Examination)
fit2 <- update(fit, Fertility ~ Agriculture + Examination)
fit2 <- update(fit, Fertility ~ Agriculture + Examination)
swiss$Fertility
library(car)
data(swiss);
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
a <- summary(fit1)$cov.unscaled[2,2]
fit2 <- update(fit, Fertility ~ Agriculture + Examination)
require(datasets); data(swiss)
require(GGally); require(tidyverse)
g <- ggpairs(swiss, lower=list(continuous="smooth"))
g
summary(lm(Fertility~., data=swiss))$coefficients
summary(lm(Fertility~ Agriculture, data=swiss))$coefficients
n = 100;
x2 <- 1 : n;
x1 <-  .01 * x2 + runif(n, -.1, .1); #money
y <-  -x1 + x2 + rnorm(n, sd = .01) #more money more problems
summary(lm(y ~ x1))$coef
summary(lm(y ~ x1 + x2))$coef
z <- swiss$Agriculture+swiss$Education
lm(Fertility~.+z, data=swiss)
require(datasets)
data("InsectSprays")
require(stats)
g <-  ggplot(data=InsectSprays, aes(y=count, x=spray, fill=spray))
g <- g+geom_violin(colour="black", size=2)
g <- g+xlab("Type of spray")+ylab("Insect count")
g
summary(lm(count~spray-1, data=InsectSprays))$coef
hist(swiss$Catholic)
swiss <- mutate(swiss, CatholicBin=1*(Catholic>50))
hist(swiss$CatholicBin)
g = ggplot(swiss, aes(x = Agriculture, y = Fertility, colour = factor(CatholicBin)))
g = g + geom_point(size = 6, colour = "black") + geom_point(size = 4)
g = g + xlab("% in Agriculture") + ylab("Fertility")
g
summary(lm(Fertility~Agriculture*factor(CatholicBin), data=swiss))$coef
seatbelts <- as.data.frame(datasets::Seatbelts)
summary(lm(I(log(DriversKilled))~kms+PetrolPrice, data=seatbelts))$coef
summary(lm(DriversKilled~kms+PetrolPrice+factor(law)-1,
data=seatbelts))$coef
seatbelts$law <- as.factor(seatbelts$law)
lawFactor <- relevel(seatbelts$law, "1")
summary(lm(DriversKilled~kms+PetrolPrice+lawFactor,
data=seatbelts))$coef
seatbelts <- seatbelts%>%
mutate(pp=(PetrolPrice-mean(PetrolPrice))/sd(PetrolPrice),
mm=kms/1000,
mmc=mm-mean(mm),
ppfactor=as.factor((pp<=-1.5)+(pp<=0)+(pp<=1.5)+(pp<Inf)))
summary(lm(DriversKilled~ppfactor+mmc+law, data=seatbelts))$coef
summary(lm(DriversKilled~pp+mmc, data=seatbelts))$coef
summary(lm(DriversKilled~mmc, data=seatbelts))$coef
data(swiss)
par(mfrow=c(2,2))
fit <- lm(Fertility~.,data=swiss)
plot(fit)
seatbelts <- as.data.frame(datasets::Seatbelts)
seatbelts <- seatbelts%>%
mutate(pp=(PetrolPrice-mean(PetrolPrice))/sd(PetrolPrice),
mm=kms/1000,
mmc=mm-mean(mm))
fit <- lm(DriversKilled~mmc+pp+law,
data=seatbelts)
sum(resid(fit)^2)/(nrow(seatbelts)-4)
summary(fit)$sigma^2
plot(fit)
resid(fit)
round(hatvalues(fit),3)
plot(round(dffits(fit),3))
plot(round(dfbetas(fit)[,2],3))
plot(round(cooks.distance(fit),3))
library(car)
data(swiss);
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
a <- summary(fit1)$cov.unscaled[2,2]
fit2 <- update(fit, Fertility ~ Agriculture + Examination)
library(car)
data(swiss);
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
a <- summary(fit1)$cov.unscaled[2,2]
fit2 <-lm(Fertility ~ Agriculture + Examination, data = swiss)
fit3 <- lm(Fertility ~ Agriculture + Examination+Education, data = swiss)
c(summary(fit2)$cov.unscaled[2,2],
summary(fit3)$cov.unscaled[2,2]) / a
a
fit <- lm(Fertility~.,data=swiss)
vif(fit)
fit <- lm(Fertility~.,data=swiss)
vif(fit)
sqrt(vif(fit))
fit <- lm(Fertility~.,data=swiss)
vif(fit)
sqrt(vif(fit))
blogdown::serve_site()
?expand.grid()
# Chunk 1: read data
library(tidyverse)
library(rpart)
library(rpart.plot)
library(knitr)
options(digits=4)
options(scipen = 999)
opts_chunk$set(echo = FALSE,
warning = FALSE,
error = FALSE,
message = FALSE,
collapse= TRUE,
comment = NA,
tidy = TRUE)
theme_set(theme_light())
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
homes <- homes%>%
select(-Id)
# Chunk 2: splitting data
#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
# Chunk 3: basic-model
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes,
method = "anova")
# Plot the basic model
rpart.plot(x = homes_model, type = 5, extra = 1)
# Chunk 4: tuning-hyperparameters
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Chunk 5: performance_metrics_base
#Computing predicted values on the test set (NOT the training test)
pred_base <- predict(object=homes_model,
newdata = homes_test)
library(Metrics)
rmse_base <- rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
mae_base <- mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
# Chunk 6: improved-model
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
# Chunk 7: performance_metrics_opt
#Computing predicted values
pred_opt <- predict(object=homes_model_opt,
newdata = homes_test)
#Compute RMSE
rmse_opt <- rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
#Compute MAE
mae_opt <- mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
# Chunk 8: set up the grid
#Establish a list of possible values for minsplit and maxdepth
#From 1 to 30 by 5
min_split <- seq(10,20,2)
#From 5 to 30 by 10
max_depth <- seq(2, 30, 2)
#Create a dataframe containing all combinations
hyper_grid <- expand.grid(min_split=min_split,
max_depth=max_depth)
#Check dimensions
dim(hyper_grid)
# Chunk 9: create models
#Create an empty list to store the models
models <- list()
#Execute the grid search
for (i in 1:nrow(hyper_grid)){
#Get min_split, max_depth values at row i
minsplit <- hyper_grid$min_split[i]
max_depth <- hyper_grid$max_depth[i]
#Train the model and store in the list
models[[i]] <- rpart(formula = SalePrice~.,
data = homes_train,
method = "anova",
minsplit=min_split,
maxdepth=max_depth)
}
# Chunk 10: evaluating models in grid
#Create an empty vector to store RMSE values
rmse_values <- c()
#Create an empty vector to store MAE values
mae_values <- c()
#Compute validation RMSE
for (i in 1:length(models)){
#Retrieve the ith model from the list
model <- models[[i]]
#Generate predictions on homes_valid validation set
pred_grid <- predict(object=model,
newdata=homes_valid)
#Compute validation RMSE and add to the
rmse_values[i] <- rmse(actual = homes_valid$SalePrice,
predicted = pred_grid)
mae_values[i] <- mae(actual = homes_valid$SalePrice,
predicted = pred_grid)
}
# Chunk 11: picking best model
#Identifying the model with smallest validation set RMSE
best_model_RMSE <- models[[which.min(rmse_values)]]
#Print the model parameters of the best_model_RMSE
best_model_RMSE$control
#Compute test set RMSE on best_model_RMSE
pred <- predict(object=best_model_RMSE,
newdata=homes_test)
rmse(actual = homes_test$SalePrice,
predicted = pred)
#Identifying the model with smallest validation set MAE
best_model_MAE <- models[[which.min(mae_values)]]
#Print the model parameters of the best_model_MAE
best_model_MAE$control
#Compute test set MAE on best_model_MAE
pred <- predict(object=best_model_MAE,
newdata=homes_test)
mae(actual = homes_test$SalePrice,
predicted = pred)
# Chunk 12
rpart.plot(best_model_MAE)
rpart.plot(best_model_RMSE)
models[[which.min(mae_values)]]
?rpart.control
rpart.plot(best_model_MAE)
rpart.plot(best_model_MAE)
rpart.plot(best_model_RMSE)
# Chunk 1: read data
library(tidyverse)
library(rpart)
library(rpart.plot)
library(knitr)
options(digits=4)
options(scipen = 999)
opts_chunk$set(echo = FALSE,
warning = FALSE,
error = FALSE,
message = FALSE,
collapse= TRUE,
comment = NA,
tidy = TRUE)
theme_set(theme_light())
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
homes <- homes%>%
select(-Id)
# Chunk 2: splitting data
#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
# Chunk 3: basic-model
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes,
method = "anova")
# Plot the basic model
rpart.plot(x = homes_model, type = 5, extra = 1)
# Chunk 4: tuning-hyperparameters
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Chunk 5: performance_metrics_base
#Computing predicted values on the test set (NOT the training test)
pred_base <- predict(object=homes_model,
newdata = homes_test)
library(Metrics)
rmse_base <- rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
mae_base <- mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
# Chunk 6: improved-model
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
# Chunk 7: performance_metrics_opt
#Computing predicted values
pred_opt <- predict(object=homes_model_opt,
newdata = homes_test)
#Compute RMSE
rmse_opt <- rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
#Compute MAE
mae_opt <- mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
# Chunk 8: set up the grid
#Establish a list of possible values for minsplit and maxdepth
#From 1 to 30 by 5
min_split <- seq(1,10,1)
#From 1 to 30 by 1
max_depth <- seq(1,30,1)
#Create a dataframe containing all combinations
hyper_grid <- expand.grid(min_split=min_split,
max_depth=max_depth)
#Check dimensions
dim(hyper_grid)
# Chunk 9: create models
#Create an empty list to store the models
models <- list()
#Execute the grid search
for (i in 1:nrow(hyper_grid)){
#Get min_split, max_depth values at row i
minsplit <- hyper_grid$min_split[i]
max_depth <- hyper_grid$max_depth[i]
#Train the model and store in the list
models[[i]] <- rpart(formula = SalePrice~.,
data = homes_train,
method = "anova",
minsplit=min_split,
maxdepth=max_depth)
}
# Chunk 10: evaluating models in grid
#Create an empty vector to store RMSE values
rmse_values <- c()
#Create an empty vector to store MAE values
mae_values <- c()
#Compute validation RMSE
for (i in 1:length(models)){
#Retrieve the ith model from the list
model <- models[[i]]
#Generate predictions on homes_valid validation set
pred_grid <- predict(object=model,
newdata=homes_valid)
#Compute validation RMSE and add to the
rmse_values[i] <- rmse(actual = homes_valid$SalePrice,
predicted = pred_grid)
mae_values[i] <- mae(actual = homes_valid$SalePrice,
predicted = pred_grid)
}
# Chunk 11: picking best model
#Identifying the model with smallest validation set RMSE
best_model_RMSE <- models[[which.min(rmse_values)]]
#Print the model parameters of the best_model_RMSE
best_model_RMSE$control
#Compute test set RMSE on best_model_RMSE
pred <- predict(object=best_model_RMSE,
newdata=homes_test)
rmse(actual = homes_test$SalePrice,
predicted = pred)
#Identifying the model with smallest validation set MAE
best_model_MAE <- models[[which.min(mae_values)]]
#Print the model parameters of the best_model_MAE
best_model_MAE$control
#Compute test set MAE on best_model_MAE
pred <- predict(object=best_model_MAE,
newdata=homes_test)
mae(actual = homes_test$SalePrice,
predicted = pred)
# Chunk 12
rpart.plot(best_model_MAE)
rpart.plot(best_model_RMSE)
mae_values
#Establish a list of possible values for minsplit and maxdepth
#From 1 to 30 by 5
min_split <- seq(1,10,1)
#From 1 to 30 by 1
max_depth <- seq(1,30,1)
#Create a dataframe containing all combinations
hyper_grid <- expand.grid(min_split=min_split,
max_depth=max_depth)
#Check dimensions
dim(hyper_grid)
#Create an empty list to store the models
models <- list()
#Execute the grid search
for (i in 1:nrow(hyper_grid)){
#Get min_split, max_depth values at row i
minsplit <- hyper_grid$min_split[i]
max_depth <- hyper_grid$max_depth[i]
#Train the model and store in the list
models[[i]] <- rpart(formula = SalePrice~.,
data = homes_train,
method = "anova",
minsplit=min_split,
maxdepth=max_depth)
}
#Create an empty list to store the models
models <- list()
#Execute the grid search
for (i in 1:nrow(hyper_grid)){
#Get min_split, max_depth values at row i
minsplit <- hyper_grid$min_split[i]
max_depth <- hyper_grid$max_depth[i]
#Train the model and store in the list
models[[i]] <- rpart(formula = SalePrice~.,
data = homes,
method = "anova",
minsplit=min_split,
maxdepth=max_depth)
}
#Create an empty vector to store RMSE values
rmse_values <- c()
#Create an empty vector to store MAE values
mae_values <- c()
#Compute validation RMSE
for (i in 1:length(models)){
#Retrieve the ith model from the list
model <- models[[i]]
#Generate predictions on homes_valid validation set
pred_grid <- predict(object=model,
newdata=homes)
#Compute validation RMSE and add to the
rmse_values[i] <- rmse(actual = homes$SalePrice,
predicted = pred_grid)
mae_values[i] <- mae(actual = homes$SalePrice,
predicted = pred_grid)
}
#Identifying the model with smallest validation set RMSE
best_model_RMSE <- models[[which.min(rmse_values)]]
#Print the model parameters of the best_model_RMSE
best_model_RMSE$control
#Compute test set RMSE on best_model_RMSE
pred <- predict(object=best_model_RMSE,
newdata=homes)
rmse(actual = homes$SalePrice,
predicted = pred)
#Identifying the model with smallest validation set MAE
best_model_MAE <- models[[which.min(mae_values)]]
#Print the model parameters of the best_model_MAE
best_model_MAE$control
#Compute test set MAE on best_model_MAE
pred <- predict(object=best_model_MAE,
newdata=homes)
mae(actual = homes$SalePrice,
predicted = pred)
rpart.plot(best_model_MAE)
rpart.plot(best_model_RMSE)
rpart.plot(best_model_MAE)
rpart.plot(best_model_RMSE)
mae_values
rpart.plot(best_model_MAE)
rpart.plot(best_model_RMSE)
rmse_base
mae_base
rmse(actual = homes_test$SalePrice,
predicted = pred)
rmse(actual = homes_test$SalePrice,
predicted = pred)
best_model_RMSE <- models[[which.min(rmse_values)]]
best_model_RMSE
#Create an empty list to store the models
models <- list()
#Execute the grid search
for (i in 1:nrow(hyper_grid)){
#Get min_split, max_depth values at row i
minsplit <- hyper_grid$min_split[i]
max_depth <- hyper_grid$max_depth[i]
#Train the model and store in the list
models[[i]] <- rpart(formula = SalePrice~.,
data = homes_train,
method = "anova",
minsplit=min_split,
maxdepth=max_depth)
}
#Create an empty vector to store RMSE values
rmse_values <- c()
#Create an empty vector to store MAE values
mae_values <- c()
#Compute validation RMSE
for (i in 1:length(models)){
#Retrieve the ith model from the list
model <- models[[i]]
#Generate predictions on homes_valid validation set
pred_grid <- predict(object=model,
newdata=homes_valid)
#Compute validation RMSE and add to the
rmse_values[i] <- rmse(actual = homes_valid$SalePrice,
predicted = pred_grid)
mae_values[i] <- mae(actual = homes_valid$SalePrice,
predicted = pred_grid)
}
rmse_values
rmse(actual = homes_test$SalePrice,
predicted = pred)
pred
