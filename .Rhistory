#Arrange correlations with SalePrice in descending order
high_cor_names <- cor_homes%>%
arrange(desc(SalePrice))%>%
filter(abs(SalePrice)>0.5)%>%#Filter variables that have relationship >0.5
pull(.rownames)#Pull names of variables
#Filter entire correlation matrix for the variables
high_cor <- cor(homes_num)[high_cor_names,high_cor_names]
#Create correlation matrix
corrplot.mixed(high_cor, tl.col="black", tl.pos = "lt",cl.ratio=0.1,number.cex=.6, cl.cex = .6, tl.cex = 0.6)
# Chunk 13: split-data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
# Chunk 14: train the model
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Look at the model output
print(homes_model)
# Chunk 15: variable-importance-table
#Look at variable importance
var_importance <- tidy(homes_model$variable.importance)
total_importance <- sum(var_importance$x)
var_importance%>%
mutate(x=x/total_importance*100)%>%
kable(caption="Top 30 variables in order of importance",
#Add column names
col.names = c("Variable","Percentage")) %>%
#Style table
kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
# Chunk 16: initial-prediction
#Computing predicted values
pred_base <- predict(object=homes_model,
newdata = homes_test)
#Compute RMSE
rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_base ) #Predicted values
#Compute MAE
mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_base ) #Predicted values
# Chunk 17: tuning-hyperparameters
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
# Chunk 18: prediction
#Computing predicted values
pred_opt <- predict(object=homes_model_opt,
newdata = homes_test)
#Compute RMSE
rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
#Compute MAE
mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
# Chunk 19: set up the grid
#Establish a list of possible values for minsplit and maxdepth
#From 1 to 30 by 5
min_split <- seq(10,20,2)
#From 5 to 30 by 10
max_depth <- seq(2, 30, 2)
#Create a dataframe containing all combinations
hyper_grid <- expand.grid(min_split=min_split,
max_depth=max_depth)
#Check dimensions
dim(hyper_grid)
# Chunk 20: create models
#Create an empty list to store the models
models <- list()
#Execute the grid search
for (i in 1:nrow(hyper_grid)){
#Get min_split, max_depth values at row i
minsplit <- hyper_grid$min_split[i]
max_depth <- hyper_grid$max_depth[i]
#Train the model and store in the list
models[[i]] <- rpart(formula = SalePrice~.,
data = homes_train,
method = "anova",
minsplit=min_split,
maxdepth=max_depth)
}
# Chunk 21: evaluating models in grid
#Create an empty vector to store RMSE values
rmse_values <- c()
#Create an empty vector to store MAE values
mae_values <- c()
#Compute validation RMSE
for (i in 1:length(models)){
#Retrieve the ith model from the list
model <- models[[i]]
#Generate predictions on homes_valid validation set
pred_grid <- predict(object=model,
newdata=homes_valid)
#Compute validation RMSE and add to the
rmse_values[i] <- rmse(actual = homes_valid$SalePrice,
predicted = pred_grid)
mae_values[i] <- mae(actual = homes_valid$SalePrice,
predicted = pred_grid)
}
# Chunk 22: picking best model
#Identifying the model with smallest validation set RMSE
best_model_RMSE <- models[[which.min(rmse_values)]]
#Print the model parameters of the best_model_RMSE
best_model_RMSE$control
#Compute test set RMSE on best_model_RMSE
pred <- predict(object=best_model_RMSE,
newdata=homes_test)
rmse(actual = homes_test$SalePrice,
predicted = pred)
#Identifying the model with smallest validation set MAE
best_model_MAE <- models[[which.min(mae_values)]]
#Print the model parameters of the best_model_MAE
best_model_MAE$control
#Compute test set MAE on best_model_MAE
pred <- predict(object=best_model_MAE,
newdata=homes_test)
mae(actual = homes_test$SalePrice,
predicted = pred)
# Chunk 23
rpart.plot(best_model_MAE)
rpart.plot(best_model_RMSE)
blogdown::serve_site()
#Save RDS
saveRDS(homes)
#Save RDS
saveRDS(homes, file="homes.rds")
list.files()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
#Save RDS
saveRDS(homes, file="../homes.rds")
here()
getwd()
"../homes.rds"
#Save RDS
saveRDS(homes, file="../homes.rds")
blogdown::serve_site()
warnings()
here
rm(list = ls())
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
library(tidyverse)
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
library(tidyverse)
library(rpart)
library(rpart.plot)
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Look at the model output
print(homes_model)
```{r tuning-hyperparameters}
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
```{r tuning-hyperparameters}
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
rm(list = ls())
unlink('content/note/Fri-16-11-2018_cache', recursive = TRUE)
library(tidyverse)
library(rpart)
library(rpart.plot)
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Plot the basic model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Plot the basic model
rpart.plot(x = homes_model, type = 5, extra = 1)
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
print(homes_model$cptable)
blogdown::serve_site()
blogdown::serve_site()
# Chunk 1: read data
library(tidyverse)
library(rpart)
library(rpart.plot)
opts_chunk$set(echo = FALSE,
warning = FALSE,
error = FALSE,
message = FALSE,
collapse= TRUE,
comment = NA,
tidy = TRUE)
theme_set(theme_light())
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
# Chunk 2
#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
# Chunk 3: basic-model
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Plot the basic model
rpart.plot(x = homes_model, type = 5, extra = 1)
# Chunk 4: tuning-hyperparameters
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Chunk 5: improved-model
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
blogdown::serve_site()
blogdown::serve_site()
(1.68+1.9+1.4+0.97+5.4)/5
(1.68+1.9+1.4+0.97+5.4)/5
grocery_bill <- 1.68+1.9+1.4+0.97+5.4
(grocery_bill)/5
grocery_bill <- 1.68+1.9+1.4+0.97+5.4
(grocery_bill)/5
mean(grocery_bill)
grocery_bill
grocery_bill <- 1.68+1.9+1.4+0.97+5.4
grocery_bill
grocery_bill <- 1.68+1.9+1.4+0.97+5.4
(grocery_bill)/5
mean(grocery_bill)
mean(grocery_bill)
grocery_bill<- c(1.68,1.9,1.4,0.97,5.4)
(grocery_bill)/5
mean(grocery_bill)
sum(grocery_bill)/5
median(grocery_bill)
grocery_bill<- c(1.68,1.9,1.4,0.97,5.4)
sum(grocery_bill)/nrow(grocery_bill)
grocery_bill<- c(1.68,1.9,1.4,0.97,5.4)
sum(grocery_bill)/length(grocery_bill)
var(grocery_bill)
grocery_bill-mean(grocery_bill)
sum(grocery_bill-mean(grocery_bill))
sum((grocery_bill-mean(grocery_bill))^2)
(sum((grocery_bill-mean(grocery_bill))^2))/5
(sum((grocery_bill-mean(grocery_bill))^2))/5
var(grocery_bill)
(sum((grocery_bill-mean(grocery_bill))^2))/4
var(grocery_bill)
(sum((grocery_bill-mean(grocery_bill))^2))/4
var(grocery_bill)
sd(grocery_bill)
sqrt((sum((grocery_bill-mean(grocery_bill))^2))/4)
sd(grocery_bill)
plot(grocery_bill)
?median
blogdown::serve_site()
# Chunk 1: read data
library(tidyverse)
library(rpart)
library(rpart.plot)
library(knitr)
opts_chunk$set(echo = FALSE,
warning = FALSE,
error = FALSE,
message = FALSE,
collapse= TRUE,
comment = NA,
tidy = TRUE)
theme_set(theme_light())
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
# Chunk 2
#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
# Chunk 3: basic-model
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Plot the basic model
rpart.plot(x = homes_model, type = 5, extra = 1)
# Chunk 4: tuning-hyperparameters
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Chunk 5: improved-model
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
RMSE(homes_model)
MAE(homes_model)
library(caret)
RMSE(homes_model)
MAE(homes_model)
library(caret)
rmse(homes_model)
mae(homes_model)
#Computing predicted values on the test set (NOT the training test)
pred_base <- predict(object=homes_model,
newdata = homes_test)
library(caret)
rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
library(caret)
#Computing predicted values on the test set (NOT the training test)
pred_base <- predict(object=homes_model,
newdata = homes_test)
library(caret)
RMSE(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
MAE(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
#Computing predicted values on the test set (NOT the training test)
pred_base <- predict(object=homes_model,
newdata = homes_test)
library(caret)
RMSE(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
MAE(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
#Computing predicted values on the test set (NOT the training test)
pred_base <- predict(object=homes_model,
newdata = homes_test)
library(Metrics)
rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
#Computing predicted values
pred_opt <- predict(object=homes_model_opt,
newdata = homes_test)
#Compute RMSE
rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
#Compute MAE
mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
# Chunk 1: read data
library(tidyverse)
library(rpart)
library(rpart.plot)
library(knitr)
opts_chunk$set(echo = FALSE,
warning = FALSE,
error = FALSE,
message = FALSE,
collapse= TRUE,
comment = NA,
tidy = TRUE)
theme_set(theme_light())
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
# Chunk 2
#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
# Chunk 3: basic-model
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Plot the basic model
rpart.plot(x = homes_model, type = 5, extra = 1)
# Chunk 4: tuning-hyperparameters
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Chunk 5
#Computing predicted values on the test set (NOT the training test)
pred_base <- predict(object=homes_model,
newdata = homes_test)
library(Metrics)
rmse_base <- rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
mae_base <- mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_base )
# Chunk 6: improved-model
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
# Chunk 7
#Computing predicted values
pred_opt <- predict(object=homes_model_opt,
newdata = homes_test)
#Compute RMSE
rmse_opt <- rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
#Compute MAE
mae_opt <- mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
blogdown::serve_site()
blogdown::serve_site()
rmse_base
rmse_opt
