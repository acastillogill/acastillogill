#Mutate to create `YearsSinceBuilt`, `YearsSinceGarageBuilt`, and `YearsSinceRemod`
#It will be the difference of the present year - YearBuilt
mutate(YearsSinceBuilt = year(Sys.Date())-YearBuilt,
#Same for YearsSinceRemod
YearsSinceRemod = year(Sys.Date())-YearRemodAdd,
#Same for GarageYrBlt
YearsSinceGarageBuilt = year(Sys.Date())-GarageYrBlt)%>%
#Remove old variables
dplyr::select(-GarageYrBlt,
-YearRemodAdd,
-YearBuilt)
# Chunk 7: covariance-example
cov(scale(homes$SalePrice), scale(homes$OverallQual))
# Chunk 8: single-regression-normalised-data
norm_fit <- lm(scale(SalePrice) ~ scale(KitchenQual), data = homes)
round(coefficients(norm_fit), digits = 2)
# Chunk 9: correlation_matrix
homes_num <- homes%>%
select_if(is.numeric)
#Create tidy correlation matrix
cor_homes <- tidy(cor(homes_num))
#Arrange correlations with SalePrice in descending order
high_cor_names <- cor_homes%>%
arrange(desc(SalePrice))%>%
filter(abs(SalePrice)>0.5)%>%#Filter variables that have relationship >0.5
pull(.rownames)#Pull names of variables
#Filter entire correlation matrix for the variables
high_cor <- cor(homes_num)[high_cor_names,high_cor_names]
#Create correlation matrix
corrplot.mixed(high_cor, tl.col="black", tl.pos = "lt",cl.ratio=0.1,number.cex=.8, cl.cex = .6, tl.cex = 0.8)
# Chunk 10: removing correlated variables
homes <- homes%>%
dplyr::select(-GarageCars,
-TotalBsmtSF,
-TotRmsAbvGrd,
-YearsSinceGarageBuilt)
# Chunk 11: low var
homes <- homes[,-nearZeroVar(homes)]
# Chunk 12: split-data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(combined),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- combined[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- combined[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- combined[assignment == 3, ]   # subset the homes data frame to test indices only
# Chunk 13: train the model
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Look at the model output
print(homes_model)
# Chunk 14: variable-importance-table
#Look at variable importance
var_importance <- tidy(homes_model$variable.importance)
total_importance <- sum(var_importance$x)
var_importance%>%
mutate(x=x/total_importance*100)%>%
kable(caption="Top 30 variables in order of importance",
#Add column names
col.names = c("Variable","Percentage")) %>%
#Style table
kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
# Chunk 15: prediction
#Computing predicted values
pred_base <- predict(object=homes_model,
newdata = homes_test)
#Compute RMSE
rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_base ) #Predicted values
#Compute MAE
mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_base ) #Predicted values
# Chunk 16: tuning-hyperparameters
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Retrieve optimal cp value based on cross-validated error
opt_index <- which.min(homes_model$cptable[, "xerror"])
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, yesno = 2, type = 0, extra = 0)
# Chunk 17: prediction
#Computing predicted values
pred_opt <- predict(object=homes_model_opt,
newdata = homes_test)
#Compute RMSE
rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
#Compute MAE
mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
# Chunk 18: set up the grid
#Establish a list of possible values for minsplit and maxdepth
#From 1 to 30 by 5
min_split <- seq(10,20,2)
#From 5 to 30 by 10
max_depth <- seq(2, 30, 2)
#Create a dataframe containing all combinations
hyper_grid <- expand.grid(min_split=min_split,
max_depth=max_depth)
#Check dimensions
dim(hyper_grid)
# Chunk 19: create models
#Create an empty list to store the models
models <- list()
#Execute the grid search
for (i in 1:nrow(hyper_grid)){
#Get min_split, max_depth values at row i
minsplit <- hyper_grid$min_split[i]
max_depth <- hyper_grid$max_depth[i]
#Train the model and store in the list
models[[i]] <- rpart(formula = SalePrice~.,
data = homes_train,
method = "anova",
minsplit=min_split,
maxdepth=max_depth)
}
# Chunk 20: evaluating models in grid
#Create an empty vector to store RMSE values
rmse_values <- c()
#Create an empty vector to store MAE values
mae_values <- c()
#Compute validation RMSE
for (i in 1:length(models)){
#Retrieve the ith model from the list
model <- models[[i]]
#Generate predictions on homes_valid validation set
pred_grid <- predict(object=model,
newdata=homes_valid)
#Compute validation RMSE and add to the
rmse_values[i] <- rmse(actual = homes_valid$SalePrice,
predicted = pred_grid)
mae_values[i] <- mae(actual = homes_valid$SalePrice,
predicted = pred_grid)
}
# Chunk 21: picking best model
#Identifying the model with smallest validation set RMSE
best_model_RMSE <- models[[which.min(rmse_values)]]
#Print the model parameters of the best_model_RMSE
best_model_RMSE$control
#Compute test set RMSE on best_model_RMSE
pred <- predict(object=best_model_RMSE,
newdata=homes_test)
rmse(actual = homes_test$SalePrice,
predicted = pred)
#Identifying the model with smallest validation set MAE
best_model_MAE <- models[[which.min(mae_values)]]
#Print the model parameters of the best_model_MAE
best_model_MAE$control
#Compute test set MAE on best_model_MAE
pred <- predict(object=best_model_MAE,
newdata=homes_test)
mae(actual = homes_test$SalePrice,
predicted = pred)
# Chunk 22
rpart.plot(best_model_MAE)
rpart.plot(best_model_RMSE)
rpart.plot(homes_model)
rpart.plot(homes_model_opt)
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Look at the model output
print(homes_model)
#Look at variable importance
var_importance <- tidy(homes_model$variable.importance)
total_importance <- sum(var_importance$x)
var_importance%>%
mutate(x=x/total_importance*100)%>%
kable(caption="Top 30 variables in order of importance",
#Add column names
col.names = c("Variable","Percentage")) %>%
#Style table
kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
#Computing predicted values
pred_base <- predict(object=homes_model,
newdata = homes_test)
#Compute RMSE
rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_base ) #Predicted values
#Compute MAE
mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_base ) #Predicted values
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Retrieve optimal cp value based on cross-validated error
opt_index <- which.min(homes_model$cptable[, "xerror"])
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, yesno = 2, type = 0, extra = 0)
#Computing predicted values
pred_opt <- predict(object=homes_model_opt,
newdata = homes_test)
#Compute RMSE
rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
#Compute MAE
mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
#Establish a list of possible values for minsplit and maxdepth
#From 1 to 30 by 5
min_split <- seq(10,20,2)
#From 5 to 30 by 10
max_depth <- seq(2, 30, 2)
#Create a dataframe containing all combinations
hyper_grid <- expand.grid(min_split=min_split,
max_depth=max_depth)
#Check dimensions
dim(hyper_grid)
#Create an empty list to store the models
models <- list()
#Execute the grid search
for (i in 1:nrow(hyper_grid)){
#Get min_split, max_depth values at row i
minsplit <- hyper_grid$min_split[i]
max_depth <- hyper_grid$max_depth[i]
#Train the model and store in the list
models[[i]] <- rpart(formula = SalePrice~.,
data = homes_train,
method = "anova",
minsplit=min_split,
maxdepth=max_depth)
}
#Create an empty vector to store RMSE values
rmse_values <- c()
#Create an empty vector to store MAE values
mae_values <- c()
#Compute validation RMSE
for (i in 1:length(models)){
#Retrieve the ith model from the list
model <- models[[i]]
#Generate predictions on homes_valid validation set
pred_grid <- predict(object=model,
newdata=homes_valid)
#Compute validation RMSE and add to the
rmse_values[i] <- rmse(actual = homes_valid$SalePrice,
predicted = pred_grid)
mae_values[i] <- mae(actual = homes_valid$SalePrice,
predicted = pred_grid)
}
rmse_values
mae_values
rpart.plot(best_model_MAE)
rpart.plot(best_model_RMSE)
rpart.plot(homes_model)
rpart.plot(homes_model_opt)
#Identifying the model with smallest validation set RMSE
best_model_RMSE <- models[[which.min(rmse_values)]]
#Print the model parameters of the best_model_RMSE
best_model_RMSE$control
#Compute test set RMSE on best_model_RMSE
pred <- predict(object=best_model_RMSE,
newdata=homes_test)
rmse(actual = homes_test$SalePrice,
predicted = pred)
#Identifying the model with smallest validation set MAE
best_model_MAE <- models[[which.min(mae_values)]]
#Print the model parameters of the best_model_MAE
best_model_MAE$control
#Compute test set MAE on best_model_MAE
pred <- predict(object=best_model_MAE,
newdata=homes_test)
mae(actual = homes_test$SalePrice,
predicted = pred)
rpart.plot(best_model_MAE)
rpart.plot(best_model_RMSE)
rpart.plot(homes_model)
rpart.plot(homes_model_opt)
DataExplorer::create_report(homes)
rm(list = ls())
x <-
c("tidyverse",
"officer",
"bookdown",
"knitr",
"kableExtra",
"kable",
"formatR",
"gridExtra",
"jsonlite",
"httr",
"base64enc",
"rlist",
"RefManageR",
"openxlsx",
"BMA",
"BAS",
"statsr",
"MASS",
"deSolve",
"FME",
"RUnit",
"plyr",
"gdata",
"citr",
"FinancialMath",
"quantmod",
"lubridate",
"stringr",
"openxlsx)",
"bookdown",
"knitr",
"kableExtra",
"formatR",
"plyr",
"biomod2",
"ggmap",
"lubridate",
"stringr",
"fuzzyjoin",
"data.table",
"gdata",
"reshape",
"xts",
"zoo",
"ggstance")
lapply(x, require, character.only = TRUE)
opts_chunk$set(echo = FALSE,
message=FALSE,
warning=FALSE,
include=FALSE)
options(knitr.table.format = "latex")
knit_hooks$set(inline = function(x) {
prettyNum(round(x,2), big.mark=",")
})
#Read clean_data.csv
inv_insects <- readxl::read_xlsx("/Volumes/TOSHIBAEXT/google_drive/PhD/Database/clean_data.xlsx", sheet = 1, n_max = 143)
rm(list = ls())
x <-
c("tidyverse",
"officer",
"bookdown",
"knitr",
"kableExtra",
"kable",
"formatR",
"gridExtra",
"jsonlite",
"httr",
"base64enc",
"rlist",
"RefManageR",
"openxlsx",
"BMA",
"BAS",
"statsr",
"MASS",
"deSolve",
"FME",
"RUnit",
"plyr",
"gdata",
"citr",
"FinancialMath",
"quantmod",
"lubridate",
"stringr",
"openxlsx)",
"bookdown",
"knitr",
"kableExtra",
"formatR",
"plyr",
"biomod2",
"ggmap",
"lubridate",
"stringr",
"fuzzyjoin",
"data.table",
"gdata",
"reshape",
"xts",
"zoo",
"ggstance")
lapply(x, require, character.only = TRUE)
opts_chunk$set(echo = FALSE,
message=FALSE,
warning=FALSE,
include=FALSE)
options(knitr.table.format = "latex")
knit_hooks$set(inline = function(x) {
prettyNum(round(x,2), big.mark=",")
})
#Read clean_data.csv
inv_insects <- readxl::read_xlsx("/Volumes/TOSHIBAEXT/google_drive/PhD/Database/clean_data.xlsx", sheet = 1, n_max = 143)
#Create factor columns
cols <-c("order",
"family",
"native_ranges",
"month",
"year",
"first_country_iberia",
"potential_actual_damage")
#Make factors
inv_insects[cols] <- lapply(inv_insects[cols], factor)
#Make year a data
inv_insects$year <- as.Date(paste(inv_insects$year,
12, 31, sep = "-"))
inv_insects <- inv_insects%>%
dplyr::select(species_name,
order,
family,
native_ranges,
month,
year,
first_country_iberia,
potential_actual_damage)
#Glimpse data
str(inv_insects, give.attr = FALSE)
#Count how many arrivals per year
inv_insects%>%
filter(!is.na(year))%>% #Remove observations with no year
group_by(year)%>% #Group by year
tally()-> insect_tally #Tally and save
#Save the total number of species
total_tally <- sum(insect_tally$n)
#Add the cumulative count to the tally
insect_tally$cumulative <- cumsum(insect_tally$n)
#Create plot of cumulative arrivals
ggplot(insect_tally,aes(x=year,y=cumulative))+
geom_line(color="#CD4F39")+
labs(x="Year",
y="Cumulative arrivals",
title=paste("Cumulative sum of invasive insects to Iberia, n =",total_tally))+
theme_minimal()
setwd("/Volumes/TOSHIBAEXT/google_drive/PhD/timeseries-article")
inv_insects
head(inv_insects)
#Canary palms model
options(scipen = 999)
kable(head(inv_insects),
booktabs = T,
caption = "Glimpse of invasive insect arrivals",
escape = FALSE) %>%
kable_styling(latex_options = c("hold_position"),
full_width = F)
#Canary palms model
options(scipen = 999)
kable(inv_insects[1:3,1:3],
booktabs = T,
caption = "Glimpse of invasive insect arrivals",
escape = FALSE) %>%
kable_styling(latex_options = c("hold_position"),
full_width = F)
inv_insects[1,1]
#Count how many arrivals per year
annual_arrivals <- inv_insects%>%
filter(!is.na(year))%>% #Remove observations with no year
group_by(year)%>% #Group by year
tally()%>% #Count how many observations per year
#Create annual arrival plot
ggplot(aes(x=lubridate::year(year),y=n))+
geom_line(color="#CD4F39")+
labs(x="Year",
y="Annual arrivals",
title=paste("Annual arrivals of invasive insects to Iberia, n =",total_tally))+
theme_minimal()
ggsave("annual_arrivals.png",annual_arrivals,width = 14,height=8,limitsize = FALSE,bg="transparent")
rm(list = ls())
require(datasets); data(swiss)
require(GGally); require(tidyverse)
g <- ggpairs(swiss, lower=list(continuous="smooth"))
g
summary(lm(Fertility~., data=swiss))$coefficients
summary(lm(Fertility~ Agriculture, data=swiss))$coefficients
#Minute 11:05 https://youtu.be/z8--IymvW4s?list=PLpl-gQkQivXjqHAJd2t-J_One_fYE55tC
n = 100;
x2 <- 1 : n;
x1 <-  .01 * x2 + runif(n, -.1, .1); #money
y <-  -x1 + x2 + rnorm(n, sd = .01) #more money more problems
summary(lm(y ~ x1))$coef
summary(lm(y ~ x1 + x2))$coef
z <- swiss$Agriculture+swiss$Education
lm(Fertility~.+z, data=swiss)
require(datasets)
data("InsectSprays")
require(stats)
g <-  ggplot(data=InsectSprays, aes(y=count, x=spray, fill=spray))
g <- g+geom_violin(colour="black", size=2)
g <- g+xlab("Type of spray")+ylab("Insect count")
g
glimpse(InsectSprays)
head(InsectSprays)
summary(lm(count~spray, data=InsectSprays))$coef
summary(lm(count~spray-1, data=InsectSprays))$coef
