group_by(BsmtExposure) %>%
dplyr::summarise(median = median(SalePrice), counts=n())
#Missingness BsmtExposure
homes$BsmtExposure[is.na(homes$BsmtExposure)] <- "None"
#It is ordinal, create vector substitute vector
exposure_ordinal <- c("Gd"= 4,"Av"= 3,"Mn"=2,"No"=1, "None"=0)
#Recode BsmtExposure
homes$BsmtExposure<-as.integer(plyr::revalue(homes$BsmtExposure, exposure_ordinal))
table(homes$BsmtExposure)
#Checking if BsmtFinType2 is ordinal
homes%>%
filter(!is.na(SalePrice))%>%
group_by(BsmtFinType2) %>%
dplyr::summarise(median = median(SalePrice), counts=n())
#Missingness BsmtFinType2
homes$BsmtFinType2[is.na(homes$BsmtFinType2)] <- "No"
#It is ordinal, create vector substitute vector
FinType_ordinal <- c('No'=0, 'Unf'=1, 'LwQ'=2, 'Rec'=3, 'BLQ'=4, 'ALQ'=5, 'GLQ'=6)
#Recode BsmtFinType2
homes$BsmtFinType2<-as.integer(plyr::revalue(homes$BsmtFinType2, FinType_ordinal))
table(homes$BsmtFinType2)
#Missingness BsmtQual
homes$BsmtQual[is.na(homes$BsmtQual)] <- "No"
#Recode BsmtQual
homes$BsmtQual<-as.integer(plyr::revalue(homes$BsmtQual, ordinal_scale))
#Missingness BsmtCond
homes$BsmtCond[is.na(homes$BsmtCond)] <- "No"
#Recode BsmtCond
homes$BsmtCond<-as.integer(plyr::revalue(homes$BsmtCond, ordinal_scale))
#Missingness BsmtFinType1
homes$BsmtFinType1[is.na(homes$BsmtFinType1)] <- "No"
#Recode BsmtFinType1
homes$BsmtFinType1<-as.integer(plyr::revalue(homes$BsmtFinType1, FinType_ordinal))
table(homes$BsmtFinType1)
#Checking if MasVnrType is ordinal
homes%>%
filter(!is.na(SalePrice))%>%
group_by(MasVnrType) %>%
dplyr::summarise(median = median(SalePrice), counts=n())
#Missingness MasVnrType
homes$MasVnrType[is.na(homes$MasVnrType)] <- "None"
#Create ordinality vector
mas_ordinality <- c('None'=0, 'BrkCmn'=0, 'BrkFace'=1, 'Stone'=2)
#Recode MasVnrType
homes$MasVnrType<-as.integer(plyr::revalue(homes$MasVnrType, mas_ordinality))
table(homes$MasVnrType)
#Missingness MasVnrArea
homes$MasVnrArea[is.na(homes$MasVnrArea)] <- 0
#Missingness Electrical
homes%>%
group_by(Electrical)%>%
dplyr::count()%>%
arrange(desc(n))%>%
dplyr::select(Electrical)%>%
head(n=1)->replacement_Electrical
homes$Electrical[is.na(homes$Electrical)] <- unlist(replacement_Electrical)
#Recode Electrical
homes$Electrical <- as.factor(homes$Electrical)
#Recode MSZoning
homes$MSZoning  <- as.factor(homes$MSZoning)
#Recode MSSubClass
homes$MSSubClass  <- as.factor(homes$MSSubClass)
#Recode Street
homes$Street  <- as.factor(homes$Street)
#Recode LotShape
homes$LotShape   <- as.factor(homes$LotShape)
#Recode LandContour
homes$LandContour   <- as.factor(homes$LandContour)
#Recode Utilities
homes$Utilities    <- as.factor(homes$Utilities)
#Looking at factors
table(homes$Utilities)
#When looking at levels, all observations except 1, fall into one level. Therefore this variable is not too helpful.
homes$Utilities <- NULL
#Recode LotConfig
homes$LotConfig     <- as.factor(homes$LotConfig)
#Recode LandSlope
homes$LandSlope     <- as.factor(homes$LandSlope)
#Recode Neighborhood
homes$Neighborhood     <- as.factor(homes$Neighborhood)
#Recode Condition1
homes$Condition1     <- as.factor(homes$Condition1)
#Recode Condition2
homes$Condition2     <- as.factor(homes$Condition2)
#Recode BldgType
homes$BldgType      <- as.factor(homes$BldgType)
#Recode HouseStyle
homes$HouseStyle       <- as.factor(homes$HouseStyle)
#Recode RoofStyle
homes$RoofStyle       <- as.factor(homes$RoofStyle)
#Recode RoofMatl
homes$RoofMatl      <- as.factor(homes$RoofMatl)
#Recode Exterior1st
homes$Exterior1st       <- as.factor(homes$Exterior1st)
#Recode Exterior2nd
homes$Exterior2nd       <- as.factor(homes$Exterior2nd)
#Recode ExterQual
homes$ExterQual<-as.integer(plyr::revalue(homes$ExterQual, ordinal_scale))
#Recode ExterCond
homes$ExterCond<-as.integer(plyr::revalue(homes$ExterCond, ordinal_scale))
#Recode Foundation
homes$Foundation      <- as.factor(homes$Foundation)
#Recode Heating
homes$Heating       <- as.factor(homes$Heating)
#Recode HeatingQC
homes$HeatingQC<-as.integer(plyr::revalue(homes$HeatingQC, ordinal_scale))
#Recode CentralAir
boolean <- c("Y"=1,"N"=0)
homes$CentralAir<-as.integer(plyr::revalue(homes$CentralAir, boolean))
table(homes$CentralAir)
#Recode KitchenQual
homes$KitchenQual<-as.integer(plyr::revalue(homes$KitchenQual, ordinal_scale))
#Recode Functional
homes$Functional      <- as.factor(homes$Functional)
#Recode PavedDrive
homes$PavedDrive       <- as.factor(homes$PavedDrive)
#Recode SaleType
homes$SaleType        <- as.factor(homes$SaleType)
#Recode SaleCondition
homes$SaleCondition     <- as.factor(homes$SaleCondition)
# Chunk 6: create time variables
homes<-homes%>%
#Mutate to create `YearsSinceBuilt`, `YearsSinceGarageBuilt`, and `YearsSinceRemod`
#It will be the difference of the present year - YearBuilt
mutate(YearsSinceBuilt = year(Sys.Date())-YearBuilt,
#Same for YearsSinceRemod
YearsSinceRemod = year(Sys.Date())-YearRemodAdd,
#Same for GarageYrBlt
YearsSinceGarageBuilt = year(Sys.Date())-GarageYrBlt)%>%
#Remove old variables
dplyr::select(-GarageYrBlt,
-YearRemodAdd,
-YearBuilt)
# Chunk 7: understanding-neighborhood-var
homes%>%
group_by(Neighborhood)%>%
summarise(mean=mean(SalePrice))%>%
arrange(desc(mean))%>%
ggplot(aes(x=reorder(Neighborhood, mean), y=mean))+
geom_col()+
theme_minimal() +
labs(title="Neighborhoods vs. Average SalePrice",
x="Neighborhood")+
scale_y_continuous(labels = scales::dollar_format(suffix = "", prefix = "$"))+
coord_flip()
# Chunk 8: binning_neighboorhood
homes <- homes%>%
group_by(Neighborhood)%>%
mutate(Mean_neigh_price=mean(SalePrice))%>%
ungroup()%>%
mutate(Neighborhood_type=ifelse(Mean_neigh_price>200000,"Fancy",
ifelse(Mean_neigh_price>150000&Mean_neigh_price<200000,"Somewhat_fancy",
ifelse(Mean_neigh_price>100000&Mean_neigh_price<150000,"Not_fancy","OK"))))
homes <- homes%>%
dplyr::select(-Neighborhood,
-Mean_neigh_price)
# Chunk 9: export the data set for posts
#Save RDS
saveRDS(homes, file="static/data/homes.rds")
# Chunk 10: covariance-example
cov(scale(homes$SalePrice), scale(homes$OverallQual))
# Chunk 11: single-regression-normalised-data
norm_fit <- lm(scale(SalePrice) ~ scale(KitchenQual), data = homes)
round(coefficients(norm_fit), digits = 2)
# Chunk 12: correlation-matrix
homes_num <- homes%>%
select_if(is.numeric)
#Create tidy correlation matrix
cor_homes <- tidy(cor(homes_num))
#Arrange correlations with SalePrice in descending order
high_cor_names <- cor_homes%>%
arrange(desc(SalePrice))%>%
filter(abs(SalePrice)>0.5)%>%#Filter variables that have relationship >0.5
pull(.rownames)#Pull names of variables
#Filter entire correlation matrix for the variables
high_cor <- cor(homes_num)[high_cor_names,high_cor_names]
#Create correlation matrix
corrplot.mixed(high_cor, tl.col="black", tl.pos = "lt",cl.ratio=0.1,number.cex=.6, cl.cex = .6, tl.cex = 0.6)
# Chunk 13: split-data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
# Chunk 14: train the model
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Look at the model output
print(homes_model)
# Chunk 15: variable-importance-table
#Look at variable importance
var_importance <- tidy(homes_model$variable.importance)
total_importance <- sum(var_importance$x)
var_importance%>%
mutate(x=x/total_importance*100)%>%
kable(caption="Top 30 variables in order of importance",
#Add column names
col.names = c("Variable","Percentage")) %>%
#Style table
kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
# Chunk 16: initial-prediction
#Computing predicted values
pred_base <- predict(object=homes_model,
newdata = homes_test)
#Compute RMSE
rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_base ) #Predicted values
#Compute MAE
mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_base ) #Predicted values
# Chunk 17: tuning-hyperparameters
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
# Chunk 18: prediction
#Computing predicted values
pred_opt <- predict(object=homes_model_opt,
newdata = homes_test)
#Compute RMSE
rmse(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
#Compute MAE
mae(actual=homes_test$SalePrice, #Actual values
predicted = pred_opt) #Predicted values
# Chunk 19: set up the grid
#Establish a list of possible values for minsplit and maxdepth
#From 1 to 30 by 5
min_split <- seq(10,20,2)
#From 5 to 30 by 10
max_depth <- seq(2, 30, 2)
#Create a dataframe containing all combinations
hyper_grid <- expand.grid(min_split=min_split,
max_depth=max_depth)
#Check dimensions
dim(hyper_grid)
# Chunk 20: create models
#Create an empty list to store the models
models <- list()
#Execute the grid search
for (i in 1:nrow(hyper_grid)){
#Get min_split, max_depth values at row i
minsplit <- hyper_grid$min_split[i]
max_depth <- hyper_grid$max_depth[i]
#Train the model and store in the list
models[[i]] <- rpart(formula = SalePrice~.,
data = homes_train,
method = "anova",
minsplit=min_split,
maxdepth=max_depth)
}
# Chunk 21: evaluating models in grid
#Create an empty vector to store RMSE values
rmse_values <- c()
#Create an empty vector to store MAE values
mae_values <- c()
#Compute validation RMSE
for (i in 1:length(models)){
#Retrieve the ith model from the list
model <- models[[i]]
#Generate predictions on homes_valid validation set
pred_grid <- predict(object=model,
newdata=homes_valid)
#Compute validation RMSE and add to the
rmse_values[i] <- rmse(actual = homes_valid$SalePrice,
predicted = pred_grid)
mae_values[i] <- mae(actual = homes_valid$SalePrice,
predicted = pred_grid)
}
# Chunk 22: picking best model
#Identifying the model with smallest validation set RMSE
best_model_RMSE <- models[[which.min(rmse_values)]]
#Print the model parameters of the best_model_RMSE
best_model_RMSE$control
#Compute test set RMSE on best_model_RMSE
pred <- predict(object=best_model_RMSE,
newdata=homes_test)
rmse(actual = homes_test$SalePrice,
predicted = pred)
#Identifying the model with smallest validation set MAE
best_model_MAE <- models[[which.min(mae_values)]]
#Print the model parameters of the best_model_MAE
best_model_MAE$control
#Compute test set MAE on best_model_MAE
pred <- predict(object=best_model_MAE,
newdata=homes_test)
mae(actual = homes_test$SalePrice,
predicted = pred)
# Chunk 23
rpart.plot(best_model_MAE)
rpart.plot(best_model_RMSE)
blogdown::serve_site()
#Save RDS
saveRDS(homes)
#Save RDS
saveRDS(homes, file="homes.rds")
list.files()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
#Save RDS
saveRDS(homes, file="../homes.rds")
here()
getwd()
"../homes.rds"
#Save RDS
saveRDS(homes, file="../homes.rds")
blogdown::serve_site()
warnings()
here
rm(list = ls())
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
library(tidyverse)
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
library(tidyverse)
library(rpart)
library(rpart.plot)
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Look at the model output
print(homes_model)
```{r tuning-hyperparameters}
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
```{r tuning-hyperparameters}
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
rm(list = ls())
unlink('content/note/Fri-16-11-2018_cache', recursive = TRUE)
library(tidyverse)
library(rpart)
library(rpart.plot)
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Plot the basic model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Plot the basic model
rpart.plot(x = homes_model, type = 5, extra = 1)
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
print(homes_model$cptable)
blogdown::serve_site()
blogdown::serve_site()
# Chunk 1: read data
library(tidyverse)
library(rpart)
library(rpart.plot)
opts_chunk$set(echo = FALSE,
warning = FALSE,
error = FALSE,
message = FALSE,
collapse= TRUE,
comment = NA,
tidy = TRUE)
theme_set(theme_light())
homes <- readRDS(file="/Volumes/TOSHIBAEXT/google_drive/acastillogill/content/homes.rds")
# Chunk 2
#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15%
set.seed(1)
assignment <- sample(1:3, size = nrow(homes),
prob = c(0.7,0.15,0.15),
replace = TRUE)
# Create a train, validation and tests from the original data frame
homes_train <- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid <- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test <- homes[assignment == 3, ]   # subset the homes data frame to test indices only
# Chunk 3: basic-model
# Train the model
homes_model <- rpart(formula = SalePrice ~ .,
data = homes_train,
method = "anova")
# Plot the basic model
rpart.plot(x = homes_model, type = 5, extra = 1)
# Chunk 4: tuning-hyperparameters
# Plot the "CP Table"
plotcp(homes_model)
# Print the "CP Table"
print(homes_model$cptable)
# Chunk 5: improved-model
# Retrieve optimal cp value based on cross-validated error
opt_index <- 7
cp_opt <- homes_model$cptable[opt_index, "CP"]
# Prune the model (to optimized cp value)
homes_model_opt <- prune(tree = homes_model,
cp = cp_opt)
# Plot the optimized model
rpart.plot(x = homes_model_opt, type = 5, extra = 1)
blogdown::serve_site()
blogdown::serve_site()
(1.68+1.9+1.4+0.97+5.4)/5
(1.68+1.9+1.4+0.97+5.4)/5
grocery_bill <- 1.68+1.9+1.4+0.97+5.4
(grocery_bill)/5
grocery_bill <- 1.68+1.9+1.4+0.97+5.4
(grocery_bill)/5
mean(grocery_bill)
grocery_bill
grocery_bill <- 1.68+1.9+1.4+0.97+5.4
grocery_bill
grocery_bill <- 1.68+1.9+1.4+0.97+5.4
(grocery_bill)/5
mean(grocery_bill)
mean(grocery_bill)
grocery_bill<- c(1.68,1.9,1.4,0.97,5.4)
(grocery_bill)/5
mean(grocery_bill)
sum(grocery_bill)/5
median(grocery_bill)
grocery_bill<- c(1.68,1.9,1.4,0.97,5.4)
sum(grocery_bill)/nrow(grocery_bill)
grocery_bill<- c(1.68,1.9,1.4,0.97,5.4)
sum(grocery_bill)/length(grocery_bill)
var(grocery_bill)
grocery_bill-mean(grocery_bill)
sum(grocery_bill-mean(grocery_bill))
sum((grocery_bill-mean(grocery_bill))^2)
(sum((grocery_bill-mean(grocery_bill))^2))/5
(sum((grocery_bill-mean(grocery_bill))^2))/5
var(grocery_bill)
(sum((grocery_bill-mean(grocery_bill))^2))/4
var(grocery_bill)
(sum((grocery_bill-mean(grocery_bill))^2))/4
var(grocery_bill)
sd(grocery_bill)
sqrt((sum((grocery_bill-mean(grocery_bill))^2))/4)
sd(grocery_bill)
plot(grocery_bill)
?median
blogdown::serve_site()
