globe <- readOGR("static","data","world_shapefile","world_shapefile")
?readOGR
here::here("static","data","world_shapefile","world_shapefile")
map_path <- here::here("static","data","world_shapefile")
globe <- readOGR(map_path,"world-shapefile")
dsn <- here::here("static","data","world_shapefile")
globe <- readOGR(dsn=dsn,layer="world-shapefile")
here::here("static","data","world_shapefile")
dir("static","data")
here::here()
dsn <- here::here("static","data","world_shapefile")
globe <- readOGR(dsn=dsn,layer="world_shapefile")
summary(water)
summary(globe)
plot(globe)
proj4string(globe)
?sp::merge
sp::merge(globe,full_maps,by.x="ISO3", by.y="iso_3")
merged_globe <- sp::merge(globe,full_maps,by.x="ISO3", by.y="iso_3")
summary(merged_globe)
tm_shape(merged_globe)+
tm_borders()+
tm_fill(col=n)
tm_shape(merged_globe)+
tm_borders()+
tm_fill(col="n")
tm_shape(merged_globe)+
tm_borders()+
tm_fill(col="n_capita")
tm_shape(merged_globe)+
tm_borders()+
tm_fill(col="n_capita")
str(merged_globe, hide.attr=TRUE)
names(merged_globe)
tm_shape(merged_globe)+
tm_borders()
class(merged_globe)
tm_shape(merged_globe)+
tm_borders()
tm_shape(merged_globe)+
tm_polygons()
devtools::install_version("tmap", version = "1.10")
library(tmap)
dsn <- here::here("static","data","world_shapefile")
globe <- readOGR(dsn=dsn,layer="world_shapefile")
merged_globe <- sp::merge(globe,full_maps,by.x="ISO3", by.y="iso_3")
tm_shape(merged_globe)+
tm_polygons()
library(sf)
blogdown::serve_site()
format(Sys.Date)
blogdown::serve_site()
# Chunk 1: setup
library(dplyr)
library(knitr)
library(here)
library(scales)
library(magrittr)
library(ggplot2)
library(cowplot)
library(tidyverse)
library(Metrics)
library(rsample)
library(broom)
library(tmap)
library(rgdal)
opts_chunk$set(echo = TRUE,
warning = FALSE,
error = FALSE,
message = FALSE,
collapse= TRUE,
comment = NA,
tidy = TRUE)
theme_set(theme_light())
options(
digits=3,
scipen = 999
)
scale_colour_discrete <- function(...) scale_colour_brewer(..., palette="Paired")
scale_fill_discrete <- function(...) scale_fill_brewer(... , palette="Paired")
r_downloads <- readRDS(file=here::here("static","data","r-downloads.rds"))
r_downloads <- na.omit(r_downloads)
# Chunk 2: downloads-by-country
r_downloads%>%
group_by(country)%>%
summarise(n=n())%>%
filter(!is.na(n))%>%
arrange(desc(n))->downloads_per_country
# Chunk 3: read-country-codes
iso_codes <- read_csv(here::here("static","data","iso_countries.csv"))
# Chunk 4: merge-country-codes
downloads_per_country%<>%
full_join(iso_codes, by=c("country"="iso"))
# Chunk 5: plot-top-10-countries
downloads_per_country%>%
arrange(desc(n))%>%
head(10)%>%
mutate(name=fct_reorder(name,n))%>%
ggplot(aes(name,n,fill=name))+
geom_col()+
expand_limits(y=0)+
coord_flip()+
scale_y_continuous(labels=comma_format())+
labs(title="Top 10 countries by R downloads",
x="",
y="Downloads")+
theme(legend.position = "none")
# Chunk 6: read-internet-users
internet_users <-read_csv(here::here("static","data","internet_users.csv"))
# Chunk 7: merge-internet-users
downloads_users_country <-downloads_per_country%>%
right_join(internet_users, by=c("name"="name"))%>%
filter(!is.na(name),
!is.na(users),
!is.na(n))%>%
arrange(desc(n))
# Chunk 8: plot-downloads-internet-users
library(plotly)
downloads_internet_plot<- downloads_users_country%>%
arrange(desc(n))%>%
ggplot(aes(users, n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas x axis
scale_x_continuous(labels=comma_format())+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Fix legend
theme(legend.position = "bottom", legend.title = element_blank())+
#Add title
labs(title="Do R-Language downloads increase with more internet users?",
y="Downloads per country",
x="Internet users in each country")
ggplotly(downloads_internet_plot)%>%
layout(legend = list(orientation = "h", x = 0.4, y = -0.2))
# Chunk 9: dataset-without-outliers
downloads_users_country%>%
filter(name!="United Kingdom",
name!="Germany",
name!="China",
name!="India",
name!="United States of America",
name!="Brazil",
name!="Japan",
name!="Canada",
name!="Australia",
name!="Namibia")->outliers_gone
# Chunk 10: plot-without-outliers
downloads_internet_plot_out<- outliers_gone%>%
arrange(desc(n))%>%
ggplot(aes(users, n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas x axis
scale_x_continuous(labels=comma_format())+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Fix legend
theme(legend.position = "bottom", legend.title = element_blank())+
#Add title
labs(title="Do R-Language downloads increase with more internet users?",
subtitle = "China, India, United Kingdom, and Germany removed.",
y="Downloads per country",
x="Internet users in each country")
ggplotly(downloads_internet_plot_out)%>%
layout(legend = list(orientation = "h", x = 0.4, y = -0.2))
# Chunk 11: read-hdi-index
human_dev_ind <-read_csv(here::here("static","data","HDI.csv"))
# Chunk 12: merge-hdi-users
downloads_hdi_country <-downloads_users_country%>%
right_join(human_dev_ind, by=c("name"="name"))%>%
filter(!is.na(name),
!is.na(users),
!is.na(n),
!is.na(HDI))%>%
arrange(desc(HDI))
# Chunk 13: hdi-downloads-plot
downloads_hdi_plot<- downloads_hdi_country%>%
ggplot(aes(x=HDI, y=n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas x axis
scale_x_continuous(labels=percent_format())+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Fix legend
theme(legend.position = "bottom", legend.title = element_blank())+
#Add title
labs(title="Human Development Index (HDI) and downloads.",
y="Downloads per country",
x="Human Development Index")
ggplotly(downloads_hdi_plot)
# Chunk 14: checking-correlation
cor(downloads_hdi_country$users,downloads_hdi_country$HDI)*100
# Chunk 15: fitting-hdi-users
internet_fit <- lm(n ~ users, data=downloads_hdi_country)
hdi_fit <- lm(n ~ users + HDI, data=downloads_hdi_country)
summary(hdi_fit)$coef
# Chunk 16: diagnostics-hdi-users
par(mfrow=c(2,2))
plot(hdi_fit)
# Chunk 17: adj-r-squared
summary(hdi_fit)$adj.r.squared*100
# Chunk 18: read-four-new-indicators
standard_liv <- read_csv(here::here("static","data","GNIPC.csv"))
standard_liv$GNI <- as.integer(standard_liv$GNI)
life_exp <- read_csv(here::here("static","data","life_exp.csv"))
edu_year <- read_csv(here::here("static","data","edu_year.csv"))
pop <-read_csv(here::here("static","data","country_populations.csv"))
edu_year <- edu_year%>%
filter(Indicator=="Mean years of schooling (ISCED 1 or higher), population 25+ years, both sexes",
year==2016)%>%
select(country,educ)
# Chunk 19: join-new-indicators
full_hdi <- downloads_hdi_country%>%
left_join(pop, by=c("name"="name"))%>%
left_join(standard_liv, by=c("iso_3"="iso"))%>%
left_join(life_exp, by=c("country_code"="country_code"))%>%
left_join(edu_year, by=c("iso_3"="country"))%>%
arrange(name)
# Chunk 20: plot-users-per-capita
full_hdi%>%
mutate(downloads_per_capita=n/pop_2018)%>%
arrange(desc(downloads_per_capita))%>%
head(10)%>%
mutate(name=fct_reorder(name,downloads_per_capita))%>%
ggplot(aes(name,downloads_per_capita,fill=name))+
geom_col()+
expand_limits(y=0)+
coord_flip()+
scale_y_continuous(labels=comma_format())+
labs(title="Top 10 countries by R downloads per capita",
x="",
y="Downloads per capita")+
theme(legend.position = "none")
# Chunk 21: plot-life-exp
life_exp_plot<- full_hdi%>%
ggplot(aes(x=life_exp, y=n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Remove legend
theme(legend.position = "none")+
#Add title
labs(title="Life expectancy at birth and R downloads.",
y="Downloads per country",
x="Life expectancy at birth (Years)")
# Chunk 22: educ-plot
educ_plot<- full_hdi%>%
ggplot(aes(x=educ, y=n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Fix legend
theme(legend.position = "bottom", legend.title = element_blank())+
#Add title
labs(title="Mean years of schooling (years) and R downloads.",
y="Downloads per country",
x="Mean years of schooling (years)")
# Chunk 23: GNI_plot
GNI_plot<- full_hdi%>%
ggplot(aes(x=GNI, y=n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas x axis
scale_x_continuous(labels=dollar_format())+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Remove legend
theme(legend.position = "none")+
#Add title
labs(title="Gross National Income and R downloads.",
y="Downloads per country",
x="Gross National Income")
# Chunk 24: plot-three-HDI
plot_grid(life_exp_plot, educ_plot, GNI_plot, rows = 3)
# Chunk 25: four-var-fit
four_fit <- lm(n ~ users + GNI + educ + life_exp, data=full_hdi)
summary(four_fit)$coef
# Chunk 26: population-fit
pop_fit <- lm(n ~ pop_2018 + HDI, data=full_hdi, na.action = na.omit)
summary(pop_fit)$coef
# Chunk 27: nesting-models
nested_hdi <- full_hdi%>%
group_by(region)%>%
nest()
# Chunk 28: plot-median-downloads-per-region
downloads_nested <- nested_hdi %>%
mutate(median_n = map_dbl(data, ~median(.x$n)))
# Extract the median_n value by using unnest
downloads_nested %>%
unnest(median_n)%>%
arrange(desc(median_n))%>%
mutate(region=fct_reorder(region,median_n))%>%
ggplot(aes(x=region, y=median_n, fill=region))+
geom_col()+
expand_limits(y=0)+
coord_flip()+
scale_y_continuous(labels=comma_format())+
labs(title="median R Downloads by country per region",
x="",
y="R Downloads")+
theme(legend.position = "none")
# Chunk 29: linear-model-for-each-region
regression_each_region <- nested_hdi %>%
mutate(model=map(data, ~lm(formula=n~users, data=.x)))
# Chunk 30: extracting-coefficients
# Extract the coefficient statistics of each model into nested dataframes
model_coef_nested <- regression_each_region %>%
mutate(coef = map(model, ~broom::tidy(.x)))
# Simplify the coef dataframes for each model
model_coef <- model_coef_nested %>%
unnest(coef)
# Chunk 31: table-significant-coefficient
model_coef%>%
#Filter significant coefficient
filter(p.value<0.05)%>%
arrange(p.value)%>%
kable(caption="The model describing R downloads as a function of internet users per country and the HDI has significant coefficients for Asia and Europe.")
# Chunk 32: assessing-model-fit
# Extract the fit statistics of each model into dataframes
model_performance <- regression_each_region%>%
mutate(fit = map(model, ~glance(.x)))%>%
unnest(fit)
# Chunk 33: table-model-fit
#Make a table with model fit in descending order #Filter significant coefficient
model_performance%>%
arrange(desc(adj.r.squared))%>%
select(region, adj.r.squared)%>%
kable(caption="The model describing R downloads as a function of internet users per country and the HDI has significant coefficients for Asia and Europe.")
# Chunk 34: augment-models
models_augmented <- regression_each_region%>%
mutate(augmented=map(model,~augment(.x)))%>%
unnest(augmented)
# Chunk 35: plot-model-fit
models_augmented%>%
ggplot(aes(x=users/1000, y=n))+
geom_point(alpha=0.5)+
geom_line(aes(y=.fitted), color="red", alpha=0.5)+
facet_wrap(~region, scales = "free")+
#Add nice commas x axis
scale_x_continuous(labels=comma_format())+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
labs(title="Top 10 countries by R downloads per 1,000 capita",
subtitle = "Red dots represents fitted value, black dots are real values.",
x="Divided by 1,000 users",
y="Downloads")
# Chunk 36: creating-train-test-data-europe
set.seed(123)
full_europe <- full_hdi%>%
filter(region=="Europe")
#Setting seed
downloads_split_europe <- initial_split(full_europe,prop=0.75)
#Store training_data
training_data_europe <-training(downloads_split_europe)
#Store testing_data
testing_data_europe <- testing(downloads_split_europe)
#Split the data into 3 folds
cv_split_europe <- vfold_cv(training_data_europe, v=3)
# Chunk 37: creating-train-and-validate-data-from-each-fold-europe
cv_data_europe <- cv_split_europe%>%
#Create cross-validated data
mutate(train=map(splits, ~training(.x)),
validate=map(splits, ~testing(.x)))
# Chunk 38: creating-models-with-cross-validated-data-europe
cv_models_lm_europe <- cv_data_europe %>%
#Create models
mutate(model=map(train,~lm(formula=n ~ users, data = .x)))
# Chunk 39: extract-actual-and-predicted-values-europe
cv_prep_lm_europe <- cv_models_lm_europe %>%
#Create actual values
mutate(validate_actual=map(validate, ~ .x$n),
#Store predicted values
validate_predicted=map2(model, validate,~predict(.x,.y)))
# Chunk 40: calculate-MAE-europe
cv_eval_lm_europe <- cv_prep_lm_europe%>%
#Compute MAE
mutate(validate_mae=map2_dbl(validate_actual,validate_predicted, ~mae(actual=.x,predicted = .y)))
# Chunk 41: creating-train-test-data-americas
full_americas <- full_hdi%>%
filter(region=="Americas")
#Setting seed
downloads_split_americas <- initial_split(full_americas,prop=0.75)
#Store training_data
training_data_americas <-training(downloads_split_americas)
#Store testing_data
testing_data_americas <- testing(downloads_split_americas)
#Split the data into 3 folds
cv_split_americas <- vfold_cv(training_data_americas, v=3)
# Chunk 42: creating-train-and-validate-data-from-each-fold-americas
cv_data_americas <- cv_split_americas%>%
#Create cross-validated data
mutate(train=map(splits, ~training(.x)),
validate=map(splits, ~testing(.x)))
# Chunk 43: creating-models-with-cross-validated-data-americas
cv_models_lm_americas <- cv_data_americas %>%
#Create models
mutate(model=map(train,~lm(formula=n ~ users, data = .x)))
# Chunk 44: extract-actual-and-predicted-values-americas
cv_prep_lm_americas <- cv_models_lm_americas %>%
#Create actual values
mutate(validate_actual=map(validate, ~ .x$n),
#Store predicted values
validate_predicted=map2(model, validate,~predict(.x,.y)))
# Chunk 45: calculate-MAE-americas
cv_eval_lm_americas <- cv_prep_lm_americas%>%
#Compute MAE
mutate(validate_mae=map2_dbl(validate_actual,validate_predicted, ~mae(actual=.x,predicted = .y)))
# Chunk 46: creating-train-test-data-asia
full_asia <- full_hdi%>%
filter(region=="Asia")
#Setting seed
downloads_split_asia <- initial_split(full_asia,prop=0.75)
#Store training_data
training_data_asia <-training(downloads_split_asia)
#Store testing_data
testing_data_asia <- testing(downloads_split_asia)
#Split the data into 3 folds
cv_split_asia <- vfold_cv(training_data_asia, v=3)
# Chunk 47: creating-train-and-validate-data-from-each-fold-asia
cv_data_asia <- cv_split_asia%>%
#Create cross-validated data
mutate(train=map(splits, ~training(.x)),
validate=map(splits, ~testing(.x)))
# Chunk 48: creating-models-with-cross-validated-data-asia
cv_models_lm_asia <- cv_data_asia %>%
#Create models
mutate(model=map(train,~lm(formula=n ~ users, data = .x)))
# Chunk 49: extract-actual-and-predicted-values-asia
cv_prep_lm_asia <- cv_models_lm_asia %>%
#Create actual values
mutate(validate_actual=map(validate, ~ .x$n),
#Store predicted values
validate_predicted=map2(model, validate,~predict(.x,.y)))
# Chunk 50: calculate-MAE-asia
cv_eval_lm_asia <- cv_prep_lm_asia%>%
#Compute MAE
mutate(validate_mae=map2_dbl(validate_actual,validate_predicted, ~mae(actual=.x,predicted = .y)))
# Chunk 51: testing-the-final-model-europe
#Store the best model with traning data
best_model_europe <- lm(n~users, data=training_data_europe)
#Store test data
test_actual_europe <- testing_data_europe$n
#Use the best model on the the test data
test_predict_europe <-predict(best_model_europe,testing_data_europe)
#Compute mae
mae(test_actual_europe,test_predict_europe)
# Chunk 52: testing-the-final-model-americas
#Store the best model with traning data
best_model_americas <- lm(n~users, data=training_data_americas)
#Store test data
test_actual_americas <- testing_data_americas$n
#Use the best model on the the test data
test_predict_americas <-predict(best_model_americas,testing_data_americas)
#Compute mae
mae(test_actual_americas,test_predict_americas)
# Chunk 53: testing-the-final-model-asia
#Store the best model with traning data
best_model_asia <- lm(n~users, data=training_data_asia)
#Store test data
test_actual_asia <- testing_data_asia$n
#Use the best model on the the test data
test_predict_asia <-predict(best_model_asia,testing_data_asia)
#Compute mae
mae(test_actual_asia,test_predict_asia)
# Chunk 54: create-tibble-with-pop-and-users-only
full_maps <- downloads_per_country%>%
left_join(pop, by=c("name"="name"))%>%
select(country, n, name, iso_3, region, sub_region, pop_2018)%>%
mutate(n_capita=n/pop_2018)%>%
filter(!is.na(n_capita),
pop_2018!=0)
view(dfSummary(downloads_per_country))
library(summarytools)
view(dfSummary(downloads_per_country))
view(dfSummary(full_maps))
library(sf)
?read_sf
map_path <- here::here("static","data","world_shapefile")
map_path
map_path <- here::here("static","data","world_shapefile")
sf::st_read(map_path)
(world_shapefile)
map_path <- here::here("static","data","world_shapefile","world_shapefile.shp")
sf::st_read(map_path)
(world_shapefile)
map_path <- here::here("static","data","world_shapefile","world_shapefile.shp")
sf::st_read(map_path)
map_path <- here::here("static","data","world_shapefile","world_shapefile.shp")
world <- sf::st_read(map_path)
plot(world)
st_crs(world)
head(world)
world%>%
left_join(full_maps, by=c("ISO3"="iso_3"))
str(world)
names(world)
