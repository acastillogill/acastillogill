life_exp_plot<- full_hdi%>%
ggplot(aes(x=life_exp, y=n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Remove legend
theme(legend.position = "none")+
#Add title
labs(title="Life expectancy at birth and R downloads.",
y="Downloads per country",
x="Life expectancy at birth (Years)")
# Chunk 22: educ-plot
educ_plot<- full_hdi%>%
ggplot(aes(x=educ, y=n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Fix legend
theme(legend.position = "bottom", legend.title = element_blank())+
#Add title
labs(title="Mean years of schooling (years) and R downloads.",
y="Downloads per country",
x="Mean years of schooling (years)")
# Chunk 23: GNI_plot
GNI_plot<- full_hdi%>%
ggplot(aes(x=GNI, y=n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas x axis
scale_x_continuous(labels=dollar_format())+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Remove legend
theme(legend.position = "none")+
#Add title
labs(title="Gross National Income and R downloads.",
y="Downloads per country",
x="Gross National Income")
# Chunk 24: plot-three-HDI
plot_grid(life_exp_plot, educ_plot, GNI_plot, rows = 3)
# Chunk 25: four-var-fit
four_fit <- lm(n ~ users + GNI + educ + life_exp, data=full_hdi)
summary(four_fit)$coef
# Chunk 26: population-fit
pop_fit <- lm(n ~ pop_2018 + HDI, data=full_hdi, na.action = na.omit)
summary(pop_fit)$coef
# Chunk 27: nesting-models
library(broom)
nested_hdi <- full_hdi%>%
group_by(region)%>%
nest()
# Chunk 28: plot-mean-downloads-per-region
downloads_nested <- nested_hdi %>%
mutate(median_n = map_dbl(data, ~median(.x$n)))
# Extract the mean_n value by using unnest
downloads_nested %>%
unnest(median_n)%>%
arrange(desc(median_n))%>%
mutate(region=fct_reorder(region,median_n))%>%
ggplot(aes(x=region, y=median_n, fill=region))+
geom_col()+
expand_limits(y=0)+
coord_flip()+
scale_y_continuous(labels=comma_format())+
labs(title="Median R Downloads by country per region",
x="",
y="R Downloads")+
theme(legend.position = "none")
# Chunk 29: linear-model-for-each-region
regression_each_region <- nested_hdi %>%
mutate(model=map(data, ~lm(formula=n~users+Area, data=.x)))
# Chunk 30: extracting-coefficients
# Extract the coefficient statistics of each model into nested dataframes
model_coef_nested <- regression_each_region %>%
mutate(coef = map(model, ~tidy(.x)))
# Simplify the coef dataframes for each model
model_coef <- model_coef_nested %>%
unnest(coef)
# Chunk 31: table-significant-coefficient
model_coef%>%
#Filter significant coefficient
filter(p.value<0.05)%>%
arrange(p.value)%>%
kable(caption="The model describing R downloads as a function of internet users per country and the HDI has significant coefficients for Asia and Europe.")
# Chunk 32: assessing-model-fit
# Extract the fit statistics of each model into dataframes
model_performance <- regression_each_region %>%
mutate(fit = map(model, ~glance(.x)))%>%
unnest(fit)
# Chunk 33: table-model-fit
#Make a table with model fit in descending order #Filter significant coefficient
model_performance%>%
arrange(desc(adj.r.squared))%>%
select(region, adj.r.squared)%>%
kable(caption="The model describing R downloads as a function of internet users per country and the HDI has significant coefficients for Asia and Europe.")
# Chunk 34: augment-models
models_augmented <- regression_each_region%>%
mutate(augmented=map(model,~augment(.x)))%>%
unnest(augmented)
# Chunk 35: plot-model-fit
models_augmented%>%
ggplot(aes(x=users/1000, y=n))+
geom_point(alpha=0.5)+
geom_line(aes(y=.fitted), color="red", alpha=0.5)+
facet_wrap(~region, scales = "free")+
#Add nice commas x axis
scale_x_continuous(labels=comma_format())+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
labs(title="Top 10 countries by R downloads per 1,000 capita",
subtitle = "Red dots represents fitted value, black dots are real values.",
x="Divided by 1,000 users",
y="Downloads")
# Chunk 36: creating-train-test-data
set.seed(123)
library(rsample)
#Setting seed
downloads_split <- initial_split(full_hdi,prop=0.75)
#Store training_data
training_data <-training(downloads_split)
#Store testing_data
testing_data <- testing(downloads_split)
#Split the data into 3 folds
cv_split <- vfold_cv(training_data, v=3)
# Chunk 37: creating-train-and-validate-data-from-each-fold
cv_data <- cv_split%>%
#Create cross-validated data
mutate(train=map(splits, ~training(.x)),
validate=map(splits, ~testing(.x)))
# Chunk 38: creating-models-with-cross-validated-data
cv_models_lm <- cv_data %>%
#Create models
mutate(model=map(train,~lm(formula=n ~ users, data = .x)))
# Chunk 39: extract-actual-and-predicted-values
cv_prep_lm <- cv_models_lm %>%
#Create actual values
mutate(validate_actual=map(validate, ~ .x$n),
#Store predicted values
validate_predicted=map2(model, validate,~predict(.x,.y)))
# Chunk 40: calculate-MAE
library(Metrics)
cv_eval_lm <- cv_prep_lm%>%
#Compute MAE
mutate(validate_mae=map2_dbl(validate_actual,validate_predicted, ~mae(actual=.x,predicted = .y)))
model_coef%>%
#Filter significant coefficient
filter(p.value<0.05)%>%
arrange(p.value)%>%
kable(caption="The model describing R downloads as a function of internet users per country and the HDI has significant coefficients for Asia and Europe.")
# Chunk 1: setup
library(dplyr)
library(knitr)
library(here)
library(scales)
library(magrittr)
library(ggplot2)
library(cowplot)
library(tidyverse)
opts_chunk$set(echo = TRUE,
warning = FALSE,
error = FALSE,
message = FALSE,
collapse= TRUE,
comment = NA,
tidy = TRUE)
theme_set(theme_light())
options(
digits=3,
scipen = 999
)
scale_colour_discrete <- function(...) scale_colour_brewer(..., palette="Paired")
scale_fill_discrete <- function(...) scale_fill_brewer(... , palette="Paired")
r_downloads <- readRDS(file=here::here("static","data","r-downloads.rds"))
r_downloads <- na.omit(r_downloads)
# Chunk 2: downloads-by-country
r_downloads%>%
group_by(country)%>%
summarise(n=n())%>%
filter(!is.na(n))%>%
arrange(desc(n))->downloads_per_country
# Chunk 3: read-country-codes
iso_codes <- read_csv(here::here("static","data","iso_countries.csv"))
# Chunk 4: merge-country-codes
downloads_per_country%<>%
full_join(iso_codes, by=c("country"="iso"))
# Chunk 5: plot-top-10-countries
downloads_per_country%>%
arrange(desc(n))%>%
head(10)%>%
mutate(name=fct_reorder(name,n))%>%
ggplot(aes(name,n,fill=name))+
geom_col()+
expand_limits(y=0)+
coord_flip()+
scale_y_continuous(labels=comma_format())+
labs(title="Top 10 countries by R downloads",
x="",
y="Downloads")+
theme(legend.position = "none")
# Chunk 6: read-internet-users
internet_users <-read_csv(here::here("static","data","internet_users.csv"))
# Chunk 7: merge-internet-users
downloads_users_country <-downloads_per_country%>%
right_join(internet_users, by=c("name"="name"))%>%
filter(!is.na(name),
!is.na(users),
!is.na(n))%>%
arrange(desc(n))
# Chunk 8: plot-downloads-internet-users
library(plotly)
downloads_internet_plot<- downloads_users_country%>%
arrange(desc(n))%>%
ggplot(aes(users, n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas x axis
scale_x_continuous(labels=comma_format())+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Fix legend
theme(legend.position = "bottom", legend.title = element_blank())+
#Add title
labs(title="Do R-Language downloads increase with more internet users?",
y="Downloads per country",
x="Internet users in each country")
ggplotly(downloads_internet_plot)%>%
layout(legend = list(orientation = "h", x = 0.4, y = -0.2))
# Chunk 9: dataset-without-outliers
downloads_users_country%>%
filter(name!="United Kingdom",
name!="Germany",
name!="China",
name!="India",
name!="United States of America",
name!="Brazil",
name!="Japan",
name!="Canada",
name!="Australia",
name!="Namibia")->outliers_gone
# Chunk 10: plot-without-outliers
downloads_internet_plot_out<- outliers_gone%>%
arrange(desc(n))%>%
ggplot(aes(users, n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas x axis
scale_x_continuous(labels=comma_format())+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Fix legend
theme(legend.position = "bottom", legend.title = element_blank())+
#Add title
labs(title="Do R-Language downloads increase with more internet users?",
subtitle = "China, India, United Kingdom, and Germany removed.",
y="Downloads per country",
x="Internet users in each country")
ggplotly(downloads_internet_plot_out)%>%
layout(legend = list(orientation = "h", x = 0.4, y = -0.2))
# Chunk 11: read-hdi-index
human_dev_ind <-read_csv(here::here("static","data","HDI.csv"))
# Chunk 12: merge-hdi-users
downloads_hdi_country <-downloads_users_country%>%
right_join(human_dev_ind, by=c("name"="name"))%>%
filter(!is.na(name),
!is.na(users),
!is.na(n),
!is.na(HDI))%>%
arrange(desc(HDI))
# Chunk 13: hdi-downloads-plot
downloads_hdi_plot<- downloads_hdi_country%>%
ggplot(aes(x=HDI, y=n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas x axis
scale_x_continuous(labels=percent_format())+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Fix legend
theme(legend.position = "bottom", legend.title = element_blank())+
#Add title
labs(title="Human Development Index (HDI) and downloads.",
y="Downloads per country",
x="Human Development Index")
ggplotly(downloads_hdi_plot)
# Chunk 14: checking-correlation
cor(downloads_hdi_country$users,downloads_hdi_country$HDI)*100
# Chunk 15: fitting-hdi-users
internet_fit <- lm(n ~ users, data=downloads_hdi_country)
hdi_fit <- lm(n ~ users + HDI, data=downloads_hdi_country)
summary(hdi_fit)$coef
# Chunk 16: diagnostics-hdi-users
par(mfrow=c(2,2))
plot(hdi_fit)
# Chunk 17: adj-r-squared
summary(hdi_fit)$adj.r.squared*100
# Chunk 18: read-four-new-indicators
standard_liv <- read_csv(here::here("static","data","GNIPC.csv"))
standard_liv$GNI <- as.integer(standard_liv$GNI)
life_exp <- read_csv(here::here("static","data","life_exp.csv"))
edu_year <- read_csv(here::here("static","data","edu_year.csv"))
pop <-read_csv(here::here("static","data","country_populations.csv"))
edu_year <- edu_year%>%
filter(Indicator=="Mean years of schooling (ISCED 1 or higher), population 25+ years, both sexes",
year==2016)%>%
select(country,educ)
# Chunk 19: join-new-indicators
full_hdi <- downloads_hdi_country%>%
left_join(pop, by=c("name"="name"))%>%
left_join(standard_liv, by=c("iso_3"="iso"))%>%
left_join(life_exp, by=c("country_code"="country_code"))%>%
left_join(edu_year, by=c("iso_3"="country"))%>%
arrange(name)
# Chunk 20: plot-users-per-capita
full_hdi%>%
mutate(downloads_per_capita=n/pop_2018)%>%
arrange(desc(downloads_per_capita))%>%
head(10)%>%
mutate(name=fct_reorder(name,downloads_per_capita))%>%
ggplot(aes(name,downloads_per_capita,fill=name))+
geom_col()+
expand_limits(y=0)+
coord_flip()+
scale_y_continuous(labels=comma_format())+
labs(title="Top 10 countries by R downloads per capita",
x="",
y="Downloads per capita")+
theme(legend.position = "none")
# Chunk 21: plot-life-exp
life_exp_plot<- full_hdi%>%
ggplot(aes(x=life_exp, y=n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Remove legend
theme(legend.position = "none")+
#Add title
labs(title="Life expectancy at birth and R downloads.",
y="Downloads per country",
x="Life expectancy at birth (Years)")
# Chunk 22: educ-plot
educ_plot<- full_hdi%>%
ggplot(aes(x=educ, y=n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Fix legend
theme(legend.position = "bottom", legend.title = element_blank())+
#Add title
labs(title="Mean years of schooling (years) and R downloads.",
y="Downloads per country",
x="Mean years of schooling (years)")
# Chunk 23: GNI_plot
GNI_plot<- full_hdi%>%
ggplot(aes(x=GNI, y=n, label=name, color=region))+
#Add light opacity
geom_point(alpha=0.5)+
#Add nice commas x axis
scale_x_continuous(labels=dollar_format())+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
#Add a linear regression line for all regions
geom_smooth(aes(group=1),method="lm")+
#Remove legend
theme(legend.position = "none")+
#Add title
labs(title="Gross National Income and R downloads.",
y="Downloads per country",
x="Gross National Income")
# Chunk 24: plot-three-HDI
plot_grid(life_exp_plot, educ_plot, GNI_plot, rows = 3)
# Chunk 25: four-var-fit
four_fit <- lm(n ~ users + GNI + educ + life_exp, data=full_hdi)
summary(four_fit)$coef
# Chunk 26: population-fit
pop_fit <- lm(n ~ pop_2018 + HDI, data=full_hdi, na.action = na.omit)
summary(pop_fit)$coef
# Chunk 27: nesting-models
library(broom)
nested_hdi <- full_hdi%>%
group_by(region)%>%
nest()
# Chunk 28: plot-mean-downloads-per-region
downloads_nested <- nested_hdi %>%
mutate(median_n = map_dbl(data, ~median(.x$n)))
# Extract the mean_n value by using unnest
downloads_nested %>%
unnest(median_n)%>%
arrange(desc(median_n))%>%
mutate(region=fct_reorder(region,median_n))%>%
ggplot(aes(x=region, y=median_n, fill=region))+
geom_col()+
expand_limits(y=0)+
coord_flip()+
scale_y_continuous(labels=comma_format())+
labs(title="Median R Downloads by country per region",
x="",
y="R Downloads")+
theme(legend.position = "none")
# Chunk 29: linear-model-for-each-region
regression_each_region <- nested_hdi %>%
mutate(model=map(data, ~lm(formula=n~users+Area, data=.x)))
# Chunk 30: extracting-coefficients
# Extract the coefficient statistics of each model into nested dataframes
model_coef_nested <- regression_each_region %>%
mutate(coef = map(model, ~tidy(.x)))
# Simplify the coef dataframes for each model
model_coef <- model_coef_nested %>%
unnest(coef)
# Chunk 31: table-significant-coefficient
model_coef%>%
#Filter significant coefficient
filter(p.value<0.05)%>%
arrange(p.value)%>%
kable(caption="The model describing R downloads as a function of internet users per country and the HDI has significant coefficients for Asia and Europe.")
# Chunk 32: assessing-model-fit
# Extract the fit statistics of each model into dataframes
model_performance <- regression_each_region %>%
mutate(fit = map(model, ~glance(.x)))%>%
unnest(fit)
# Chunk 33: table-model-fit
#Make a table with model fit in descending order #Filter significant coefficient
model_performance%>%
arrange(desc(adj.r.squared))%>%
select(region, adj.r.squared)%>%
kable(caption="The model describing R downloads as a function of internet users per country and the HDI has significant coefficients for Asia and Europe.")
# Chunk 34: augment-models
models_augmented <- regression_each_region%>%
mutate(augmented=map(model,~augment(.x)))%>%
unnest(augmented)
# Chunk 35: plot-model-fit
models_augmented%>%
ggplot(aes(x=users/1000, y=n))+
geom_point(alpha=0.5)+
geom_line(aes(y=.fitted), color="red", alpha=0.5)+
facet_wrap(~region, scales = "free")+
#Add nice commas x axis
scale_x_continuous(labels=comma_format())+
#Add nice commas y axis
scale_y_continuous(labels=comma_format())+
labs(title="Top 10 countries by R downloads per 1,000 capita",
subtitle = "Red dots represents fitted value, black dots are real values.",
x="Divided by 1,000 users",
y="Downloads")
# Chunk 36: creating-train-test-data
set.seed(123)
library(rsample)
#Setting seed
downloads_split <- initial_split(full_hdi,prop=0.75)
#Store training_data
training_data <-training(downloads_split)
#Store testing_data
testing_data <- testing(downloads_split)
#Split the data into 3 folds
cv_split <- vfold_cv(training_data, v=3)
# Chunk 37: creating-train-and-validate-data-from-each-fold
cv_data <- cv_split%>%
#Create cross-validated data
mutate(train=map(splits, ~training(.x)),
validate=map(splits, ~testing(.x)))
# Chunk 38: creating-models-with-cross-validated-data
cv_models_lm <- cv_data %>%
#Create models
mutate(model=map(train,~lm(formula=n ~ users, data = .x)))
# Chunk 39: extract-actual-and-predicted-values
cv_prep_lm <- cv_models_lm %>%
#Create actual values
mutate(validate_actual=map(validate, ~ .x$n),
#Store predicted values
validate_predicted=map2(model, validate,~predict(.x,.y)))
# Extract the coefficient statistics of each model into nested dataframes
model_coef_nested <- regression_each_region %>%
mutate(coef = map(model, ~tidy(.x)))
# Simplify the coef dataframes for each model
model_coef <- model_coef_nested %>%
unnest(coef)
# Extract the coefficient statistics of each model into nested dataframes
model_coef_nested <- regression_each_region %>%
mutate(coef = map(model, ~broom::tidy(.x)))
# Simplify the coef dataframes for each model
model_coef <- model_coef_nested %>%
unnest(coef)
model_coef_nested <- regression_each_region %>%
mutate(coef = map(model, ~broom::tidy(.x)))
model_coef <- model_coef_nested %>%
unnest(coef)
model_coef
model_coef%>%
#Filter significant coefficient
filter(p.value<0.05)%>%
arrange(p.value)%>%
kable(caption="The model describing R downloads as a function of internet users per country and the HDI has significant coefficients for Asia and Europe.")
blogdown::serve_site()
