[{"authors":null,"categories":null,"content":"","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461103200,"objectID":"8a648aa279e347a7f7d13144c3d1dff5","permalink":"/posts/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/posts/","section":"","summary":"","tags":null,"title":"Recent Posts","type":"page"},{"authors":null,"categories":null,"content":"","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461103200,"objectID":"73c9b0342715f88a87dd788563688eac","permalink":"/journal/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/journal/","section":"","summary":"","tags":null,"title":"Recent journal entries","type":"page"},{"authors":null,"categories":["Journal"],"content":" I hope the authors of the Rpart documentation don’t take it personally but they mention surrogate variables starting on page 11 and they don’t properly define what surrogate variables mean until page 18. Maybe it’s just me.\nAnyway! So here is another response to a previous question: what is a surrogate variable?\nSo the Rpart algorithm deals with missing data. The way it does this is when it’s doing its thing and determining the next variable and split point and comes across an observation that does not have a that variable, it figures out which variables can act as a replacement for that variable. Surrogate variable mystery solved. N\nNow I’m having a bit of trouble figuring out if all the rules I’ve read for choosing the best tree are the same. In the DataCamp course we pick the tree that minimises xerror. Yesterday I posted this rule I found on Cross Validated:\n\\(rel_{error}\\) \\(+\\) \\(xstd\\) \\(\u0026lt;\\) \\(xerror\\)\nIn the documentation this is a rule:\n\\(1-SE\\)\nIn my next post I’m going to get to the bottom of this. I’m particularly confused because if we want to minimise the xerror, why does page 16 of the documentation show a cptable with 27 splits and says: we see that the best tree has 10 terminal nodes, 9 splits, based on cross-validation. It has xerror 0.3944.\nCP table on page 16 from the Rpart documentation.\n Doesn’t the tree with 11 splits and 12 nodes have the first smallest error with 0.36667?\n","date":1541635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541635200,"objectID":"89fb107b96c4711ac1db4329eab7342d","permalink":"/journal/surrogate-who/","publishdate":"2018-11-08T00:00:00Z","relpermalink":"/journal/surrogate-who/","section":"journal","summary":"More jargon clarified!","tags":["Rpart","Learning"],"title":"Surrogate who?","type":"journal"},{"authors":null,"categories":["Journal"],"content":" According to user Harold Ship in this post, we should pick the tree with that keeps this relationship but has the tallest (that’s what he means with the lowest level, since each row represents a tree with more splits):\n\\(rel_{error}\\) \\(+\\) \\(xstd\\) \\(\u0026lt;\\) \\(xerror\\)\n","date":1541548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541548800,"objectID":"7eeba2c4a3e25a2078b8eacd826cc779","permalink":"/journal/choosing-the-best-tree/","publishdate":"2018-11-07T00:00:00Z","relpermalink":"/journal/choosing-the-best-tree/","section":"journal","summary":"How to choose the best tree using the output tables.","tags":["Rpart","Learning"],"title":"Choosing the best tree","type":"journal"},{"authors":null,"categories":["Journal"],"content":" Yesterday I went to a talk by a woman that worked at Facebook helping startups scale. One of slides was about prioritisation at Facebook. It was something like this:\nMission \u0026amp; vission, objectives, key results, and tasks.\n So today we continue trying to understand all the rpart() function and algorithm.\nIn response to one of yesterday’s questions, the PRESS statistic is the predicted residual error sum of squares:\n\\(\\sum_{i=1}^{n} (y_i-\\hat{y}_{i,-i})^2\\)\nAccording to trusty Wikipedia, the PRESS statistic is a form of cross validation derived from comparing the model’s predictions vs. actual values on a dataset that was not used for fitting the model.\nA model that is overfitted would give small residuals using the observations with which the model was fitted and larger residuals with observations that were not used to fit the model. This makes sense.\nNow that I understand the PRESS statistic which is called xerror in the rpart output table. I still didn’t understand the xstd. I assumed it was the standard deviation of the xerror but after some manual attempts at calculating xstd based on xerror and having no luck. I took to Cross Validated to sort me out. According to user bearoak on this thread, it is not the standard deviation as one would think because of the std bit of xstd, but it is actually the standard error which is the standard deviation divided by the number of observations.\n","date":1541462400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541462400,"objectID":"cdb96a1d902aad6593a1c135f247d696","permalink":"/journal/objectives-key-results-tasks/","publishdate":"2018-11-06T00:00:00Z","relpermalink":"/journal/objectives-key-results-tasks/","section":"journal","summary":"Prioritising tips.","tags":["Prioritising","Learning"],"title":"Objectives, key results, tasks","type":"journal"},{"authors":null,"categories":["Journal"],"content":" I’ve been able to answer quite a few of the questions that I asked before. For example, I don’t have to remove outliers because the algorithm is robust to outliers. No centering and scaling needs to be done to the variables. I can leave factors as factors, but the issue with with a lot of levels is that the tree can get quite wide which reduces its interpretability, which is after all, one of the advantages of using binary trees. I’ve also answered what constitutes an improvemment in the branch-splitting algorithm, which is finding a variable that can produce the most homogenous subgroups after a split.\nThe rpart function has several hyperparameters I would like to really understand before I modify the variables. Today I set out to understand these hyperparameters but instead got caught up understanding the, “five additional ingredients” that need be specified to “generalise”extend\u0026quot; the algorithm. These are:\nSplitting criterion, in ANOVA, this is maximising the between-groups sum-of-squares in a simple analysis of variance.  \\(SS_T-(SS_L+SS_R)\\)\nWhere \\(SS_T\\) is the sum of squares for the node; \\(SS_L\\) is the sum of squares for the left son; and \\(SS_R\\) is the sum of squares for the right son.\nA summary statistic or vector to describe a node. The first element is considered to be the fitted value. For ANOVA or regression, this the mean of the node.\n Error of the node: Variance of \\(Y\\) for ANOVA.\n The prediction error for a new observation, assigned to the node (\\(y_{new}-\\bar{y}\\)).\n Any initalisation parameter.\n  I was also trying to understand the output of printcp() which shows the complexity parameter in descending order.\nIt shows the columns: cp, nsplit, rel error, Xerror, and Xstd.\n rel error: is defined as \\(1-R^2\\) Xerror: “related to the PRESS statistic”. What is the PRESS statistic? Xstd: related to the cross validation error.  Also, when reading the rpart documentation, the authors constantly refer to a surrogate split. Which I’m having a bit of trouble trying to understand.\n","date":1541376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541376000,"objectID":"ff49fb1f7edc0b433589c8b36b6e88cf","permalink":"/journal/understanding-output-tables/","publishdate":"2018-11-05T00:00:00Z","relpermalink":"/journal/understanding-output-tables/","section":"journal","summary":"Generalising the Rpart algorithm.","tags":["RPart","Learning"],"title":"Understanding output tables","type":"journal"},{"authors":null,"categories":["Journal"],"content":" Today I went to a poetry competition and heard a useful tip for memorising a poem, or in my case, speeches. Sometimes when we can’t memorise certain part of a text, after trying many times, maybe whatever is written down there, should go on the final version. Memorising can be a tool for editing the final text. When you read the text aloud, it’s own cadence and rhythm will point out that that word shouldn’t be there.\nI’ve joined the local chapter of Toastmasters and this recently happened to me with a speech. There were two lines in the third paragraph that I just couldn’t remember. After trying to memorise them over and over, I realised that actually the bit I was struggling with could have been said in another way. In a way that was more memorable for the audience and that made more sense within the text. Changed it and voila! Same idea expressed differently and it was easy to memorise and it made a bigger impact.\n","date":1541289600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541289600,"objectID":"f24dc08eaabbf0be1402c7934276fd00","permalink":"/journal/memorising-tip/","publishdate":"2018-11-04T00:00:00Z","relpermalink":"/journal/memorising-tip/","section":"journal","summary":"Let the text write itself...","tags":["Rest day","Learning"],"title":"Memorising tip","type":"journal"},{"authors":null,"categories":["Journal"],"content":" The goal of today’s session was to learn how to preprocess the homes dataset I’ve been working with. The missing values have been filled and now I have a few questions about whether I should perform or not, additional pre-processing steps before I can implement the CART algorithm.\n Should I remove highly correlated variables? I have three and I don’t know what to do with them? Should I remove levels within factor variables with few observations? Should I bin variables with lots of factors like neighboorhoods? Should I bin numeric variables that related to the same metric? (i.e. square feet) What about outliers?  So for now I’m reading the introduction to Recursive Partioning document that comes with the R Part package.\nI am also trying to figure out what constitutes improvements in the branch-splitting algorithm to know why it stops when it does.\n","date":1541203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541203200,"objectID":"3788d2b6e2594ca81c4c3c46c71f546e","permalink":"/journal/pre-processing-to-what-extent/","publishdate":"2018-11-03T00:00:00Z","relpermalink":"/journal/pre-processing-to-what-extent/","section":"journal","summary":"Trying to figure out if I should further pre-process the data or not.","tags":["Pre -processing-data","Learning","CART"],"title":"Pre-processing to what extent?","type":"journal"},{"authors":null,"categories":["R","RPART"],"content":"   1 Summary 2 Purpose of this post 3 The data 3.1 Missing values 3.2 Variable creation 3.3 Correlation 3.4 Removing correlated variables 3.5 Removing variables that have low variance  4 Classification and Regression Trees (CART) time 4.1 Splitting the data 4.2 Training the model 4.3 Revising variable importance 4.4 Evaluating the model 4.5 Tuning hyperparameters 4.6 Grid search for best hyperparameters  5 Techniques used 6 Questions from this analysis   1 Summary To see the code used in this post, visit my kernel on kaggle in R Markdown format.\n 2 Purpose of this post  3 The data The dataset House Prices: Advanced Regression Techniques was downloaded from Kaggle and put together by Dean De Cock. It has 79 explanatory variables describing 1,460 homes in Ames, Iowa. The codebook for all the variables can be found here. As I go along, I’ll explain the most relevant ones.\nFirst we will see how many numerical vs. categorical variables there are.\nOur dataset has 38 numeric and 43 character variables. Next, since we are interested in estimating sales prices SalePrice, we will recode character variables and see the most strongly correlated variables. There are 43 character variables available. I want to recode them where there is ordinality and where there isn’t dummify them.\nThere are two numerical variables that are actually date related: YearBuiltand YearRemodAdd (remodelled date). It makes more sense to make two new variables that relate the build and remodelled dates with the present. In other words, I will create the years since built and years since remodelled date variables, this will help interpret the results better.\n3.1 Missing values As it more common than not, the dataset contains missing values. Missing values need to be dealt with because regression (and other models) requires complete observations.\nDealing with missing data depends on why the data are missing. This article explains four reasons why data could be missing. When the data are missing at random (MAR) or completely at random (MCAR), observations with missing values can be removed without introducing bias into the model.\nSometimes, however, if the dataset is not too big and we don’t want to lose observations, or even if it is big, yet we still don’t want to remove observations, we can impute data. Imputing means replacing missing values by doing some educated guesses. This article summarises how to impute data depending on why it is missing.\nIf the data are not missing at random, then the imputation mechanism has to modelled.\nLet’s look at which variables are missing:\n Table 3.1: Variables with missing values in descending order    Variable  Number of NAs      PoolQC  1453    MiscFeature  1406    Alley  1369    Fence  1179    FireplaceQu  690    LotFrontage  259    GarageType  81    GarageYrBlt  81    GarageFinish  81    GarageQual  81    GarageCond  81    BsmtExposure  38    BsmtFinType2  38    BsmtQual  37    BsmtCond  37    BsmtFinType1  37    MasVnrType  8    MasVnrArea  8    Electrical  1     19 variables have missing values. Based on the codebook, the reason why so many houses have PoolQC missing is because NA, means there is no pool. Since this variable is ordinal, I can revalue it to make it numerical and 0 will mean the property has no pool. MiscFeature, Alley, Fence, and FireplaceQu are missing because of similar reasons. We don’t know why LotFrontage is missing but we will impute as the median for properties in the same neighborhood. Erik Bruin’s kernel on Kaggle with this dataset, was a great guideline for what to do in each missing value case.\n 3.2 Variable creation  3.3 Correlation Correlation, \\(Cor(X,Y)\\), measures the strength of the linear relationship between two variables \\(X\\) and \\(Y\\).\nThe correlation between SalePrice and another variable, let’s say, OverallQual, is the covariance of the separately normalised data between the two variables.\ncov(scale(homes$SalePrice), scale(homes$OverallQual)) [,1] [1,] 0.7909816 Since covariance units are OverallQual * SalePrice, calculating the correlation is instead more helpful since it is unit free.\nIf we created a model with only variable as the predictor of SalesPrice, let’s say, KitchenQual and normalised the data, the regression slope would be the correlation between the two variables.\nnorm_fit \u0026lt;- lm(scale(SalePrice) ~ scale(KitchenQual), data = homes) round(coefficients(norm_fit), digits = 2) (Intercept) scale(KitchenQual) 0.00 0.66  Here is the correlation matrix for variables that have a relationship stronger than 0.5 with SalePrice.\nThere are 17 variables that have a correlation stronger than 0.5. They are arranged in descending order. It is interesting to note the high correlation that exists amongst variables. The correlation plot highlights some obvious ones, GarageArea and GarageCars. Makes sense, a bigger garage can hold more cars. X1stFlrSF and TotalBsmtSF, the total area of the first floor and basement, this also seems reasonable since basements are underneath the same floor and would tend to have a similar area. TotRmsAbvGrd and GrLivArea, the total number of rooms and area above ground, again ok, more rooms would be linked to a bigger living area. Finally, YearsSinceBuilt and YearsSinceGarageBuilt since garages are usually built at the same time as the house.\nHere is the codebook for all the variables featured in the correlation matrix in case they come up later and we need to interpret what they mean.\nPositive correlation\n OverallQual: Rates the overall material and finish of the house GrLivArea: Above grade (ground) living area square feet. ExterQual: Exterior quality KitchenQual: Kitchen quality GarageCars: Size of garage in car capacity GarageArea: Size of garage in square feet TotalBsmtSF: Total square feet of basement area X1stFlrSF: First floor square feet BsmtQual: Height of basement FullBath: Full bathrooms above grade GarageFinish: Interior finish of the garage TotRmsAbvGrd: Total rooms above grade (does not include bathrooms) FireplaceQu: Fireplace quality  Negative correlation\n YearsSinceRemod: Years since remodel date (same as construction date if no remodelling or additions) YearsSinceGarageBuilt: Years since the garage was built YearsSinceBuilt: Years since construction date   3.4 Removing correlated variables  3.5 Removing variables that have low variance   4 Classification and Regression Trees (CART) time In this post I am using the recursing and partitioning (RPART) algorithm also known as classification and regression trees (CART). It will be implemented with the rpart package in R.\nRpart, builds a model in two stages:\nFirst stage:\nA single variable is identified which can best split the data into two groups. The data is then separated intwo two groups and the whole process is repeated recursively or indefinitely until the sub groups reach a minimum size, or until no further improvements can be made.\nTo be defined later - What constitutes as the variable that BEST splits the data?\nWhen the split is made, similarity amongst the observations can more or less homogenous. This homogeneity is also called purity and it can be measured. The impurity measure of a node specifies how mixed the resulting subset is.\n What are the further improvements that the algorithm can make?  These are further splits that reduce the homogeneity of the subgroups.\nSecond stage:\nThe tree is trimmed back or prunned using cross-validation.\n4.1 Splitting the data  4.2 Training the model  4.3 Revising variable importance  Table 4.1: Top 30 variables in order of importance    Variable  Percentage      OverallQual  23.0515224    Neighborhood  16.3419785    BsmtQual  9.2732165    GarageArea  8.2673885    YearsSinceBuilt  7.2442216    KitchenQual  5.3352759    X1stFlrSF  5.1291660    GrLivArea  4.9662571    FullBath  2.8642603    ExterQual  2.7602501    Foundation  2.6651332    BsmtFinSF1  2.0997208    X2ndFlrSF  1.8566046    MSSubClass  1.3087047    LotArea  0.9037093    HouseStyle  0.6407663    BsmtUnfSF  0.6362427    Fireplaces  0.6103881    BedroomAbvGr  0.5957507    LotFrontage  0.5332390    RoofMatl  0.5106289    YearsSinceRemod  0.5106289    Exterior1st  0.4069254    Condition1  0.3404193    LandContour  0.3404193    HalfBath  0.3054079    MasVnrArea  0.1988047    MSZoning  0.1088826    Fence  0.0762178    BsmtCond  0.0589345    BsmtExposure  0.0589345      4.4 Evaluating the model [1] 46379.54 [1] 31179.09  4.5 Tuning hyperparameters  CP nsplit rel error xerror xstd 1 0.44576120 0 1.0000000 1.0034376 0.09648934 2 0.11980464 1 0.5542388 0.5578944 0.05448931 3 0.07227853 2 0.4344342 0.4383458 0.05343974 4 0.03113653 3 0.3621556 0.3953313 0.04125842 5 0.02840259 4 0.3310191 0.4006663 0.04426050 6 0.01998604 6 0.2742139 0.3773829 0.04402623 7 0.01893938 7 0.2542279 0.3683706 0.04056180 8 0.01351892 8 0.2352885 0.3434948 0.03750211 9 0.01198871 9 0.2217696 0.3330784 0.03737704 10 0.01182315 10 0.2097809 0.3230293 0.03735644 11 0.01000000 11 0.1979577 0.3074050 0.03541012 ## Checking the error on the optimised model\n[1] 46379.54 [1] 31179.09  4.6 Grid search for best hyperparameters [1] 90 2 $minsplit [1] 10 12 14 16 18 20 $minbucket [1] 3 4 5 5 6 7 $cp [1] 0.01 $maxcompete [1] 4 $maxsurrogate [1] 5 $usesurrogate [1] 2 $surrogatestyle [1] 0 $maxdepth [1] 2 $xval [1] 10 [1] 81900.42 $minsplit [1] 10 12 14 16 18 20 $minbucket [1] 3 4 5 5 6 7 $cp [1] 0.01 $maxcompete [1] 4 $maxsurrogate [1] 5 $usesurrogate [1] 2 $surrogatestyle [1] 0 $maxdepth [1] 2 $xval [1] 10 [1] 58664.27   5 Techniques used  I   6 Questions from this analysis     ","date":1533772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533772800,"objectID":"58b1c36adbb58e860f9c6ec655a1a5a5","permalink":"/post/predicting-property-prices/","publishdate":"2018-08-09T00:00:00Z","relpermalink":"/post/predicting-property-prices/","section":"post","summary":"What factors most affect sales prices for homes?","tags":["Kaggle","Home sales prices"],"title":"Predicting property prices","type":"post"},{"authors":null,"categories":["R","End-to-end","Exploratory data analysis"],"content":"   1 Summary 2 Purpose of this post 3 The data 4 Data cleaning 5 Exploratory data analysis 6 Techniques used 7 Questions from this analysis   1 Summary To see the code used in this post, visit my kernel on kaggle in R Markdown format.\nIn this post, I analyse 671,205 observations. Each represents a Kiva loan. I find that the gender that requests the most loans is female with single females being the most frequent borrowers. The main uses for these loans are agriculture, retail, and food with some variations amongst continents. Half of the loans are 4.22 USD or less and are funded by 12 or less contributers. The median time between posting a loan online and disbursing it to the borrower is 16.89 days. I mainly use the tidyverse, stringr, and quantmode packages.\n 2 Purpose of this post Muhammad Yunus and the Grameen Bank won the Nobel Peace Prize in 2006 for “their efforts through microcredit to create economic and social development from below.”\nBack in 1976, Yunus, at the time a professor at the University of Chittagong (Bangladesh), noticed that small amounts of money could make a substantial difference to people living in poverty. He started to loan money to people that didn’t meet the requirements listed by the mainstream banking system. It was reported that these type of loans were effective to “emerge” from poverty using default rates lower than those of commercial banks, reported at 2%. Eventually, in October 1983, Muhammad Yunus founded Grameen Bank, considered to be the first microfinance institution.\nFounded in 2005, Kiva, has the same mission as Grameen Bank except that anyone can become a Kiva banker. This online platform enables microcredit lending to help low-income entrepreneurs around the world with a couple of clicks. Pretty neat, huh? In this post, I unpack a large dataset published by Kiva on the Kaggle platform and explore these microloans.\n 3 The data The dataset was published during the first months of 2018 on the Kaggle platform.\nThe complete dataset was a zip file with size 232.7 MB containing four files: kiva_loans.csv, kiva_mpi_region_locations.csv,loan_theme_ids.csv, and loan_themes_by_region.csv. After I looked at the contents, I chose to work with the first one kiva_loans.csv.\nThe dataset had 671,205 observations and 20 variables.\nSome of the codebook came with the dataset and some I researched to make assumptions:\n funded_amount: “The amount disbursed by Kiva to the field agent(USD)” loan_amount: “The amount disbursed by the field agent to the borrower(USD)” activity: “More granular category” sector: “High level category” use: “Exact usage of loan amount” country_code: “ISO country code of country in which loan was disbursed” country: “Full country name of country in which loan was disbursed” region: “Full region name within the country” currency: “The currency in which the loan was disbursed” partner_id: “ID of partner organization”  This variable has a lot of missing values and the Kiva explanation on Kaggle doesn’t go much further. For now, I will exclude partner_id.\n posted_time: “The time at which the loan is posted on Kiva by the field agent” disbursed_time: “The time at which the loan is disbursed by the field agent to the borrower” funded_time: “The time at which the loan posted to Kiva gets funded by lenders completely” term_in_months: “The duration for which the loan was disbursed in months” lender_count: “The total number of lenders that contributed to this loan” borrower_genders: “Comma separated M,F letters, where each instance represents a single male/female in the group” repayment_interval: Not specified so we’ll assume that it means the standard definition of when the loan is repaid back to the lender.   4 Data cleaning From at the structure of our data, we can soon see there are a few bits that don’t make sense and need to be fixed.\na. Missing values\nWe will leave missing values in for now.\nb.borrower_genders\nFrom the variable descriptions, we expected borrower_gendersto have only two levels, maleor female. Here we see many more levels, 11,298 to be precise. This isn’t very clear so we’ll fix that first by creating five levels:\n[1] \u0026quot;mixed_genders\u0026quot; \u0026quot;mult_females\u0026quot; \u0026quot;mult_males\u0026quot; \u0026quot;single_female\u0026quot; [5] \u0026quot;single_male\u0026quot;  c.loan_amounts\nNow, since we’re trying to make sense of all loans, it’s better if loan_amounts is in the same currency. Let’s translate it to USD.\nd.country_codes\nFinally, with 86 countries, we have 86 levels. Perhaps it would be interesting to create another category called continent to produce less levels and have a better understanding of the overall function of regional distributions.\nOur country codes are in the ISO3166 format, so we will use the associated continent code found here. And make five continents. Africa, Asia, Europe, Oceania, and South America.\n[1] \u0026quot;AF\u0026quot; \u0026quot;AS\u0026quot; \u0026quot;EU\u0026quot; \u0026quot;OC\u0026quot; \u0026quot;SA\u0026quot; e. Dates\nLet’s calculate two lengths of time that I think are interesting. First, how much time passes from the moment the loan is posted to the moment it’s disbursed (total_time). Second, how long does a loan take to get funded (giving_time)?\n 5 Exploratory data analysis Now, let’s describe the data starting with some plots and tables to understand it.\n Figure 5.1: Four continents in descending order from the continent that requests the most loans (Asia). Notice Europe and North America do not appear. mult_females or mult_males means more than one female or male.   Table 5.1: A single female is the most common type of borrower gender with over half of all Kiva loans requested.    Gender  Percentage      single_female  65.82    single_male  17.43    mult_females  9.58    mixed_genders  6.58    mult_males  0.59     In all continents, single females request the most loans followed by single males. In Asia and South America, the third category is multiple females while in Africa it’s mixed genders.\n Figure 5.2: In Asia and Oceania, the most popular repayment interval is irregular while in Africa and South America it’s monthly.   Table 5.2: The most popular type of repayment interval is monthly.    Repayment interval  Percentage      monthly  47    irregular  43    bullet  10      Figure 5.3: In every continent, the three most popular sectors are agriculture, retail, and food.   Table 5.3: Generally, the most frequent use of loans is agriculture, followed by food and retail.    Sector  Percentage      Agriculture  27.5    Food  20.8    Retail  19.5    Services  6.9    Personal Use  5.9    Education  5.1    Clothing  4.7    Housing  4.1    Transportation  2.6    Arts  1.7    Health  1.2      Table 5.4: Half of all Kiva loans are requested in Asia followed by Africa, South America, Oceania and finally the European Union.    Continents  Percentage      AS  55.4    AF  29.8    SA  13.5    OC  1.4     Notice most loans are requested by single females, the least amount of loans are given in the EU, weekly repayment is an unpopular form of paying loans back and entertainment, wholesale, manufacturing, and construction amount less than 2.2% of sectors.\n Figure 5.4: Philippines requests the mosts loans with over double as the second country, Kenya KE. Other top countries in this ranking are Cambodia KH, Pakistan PK, Peru PE, Colombia CO, Uganda UG, Tajikistan TJ, Ecuador EC, and Paraguay PY.  Now I’m curious to look at the top 10 countries requesting Kiva loans but per capita and per number of internet users.\n Figure 5.5: Interesting to see how the ranking changed! The island of Samoa WS is leading, followed by Armenia AM. Countries that remain in the top 10 are Cambodia KH, Philippines PH, Kenya KE, and Tajikstan TJ. Other new countries are Timor-Leste TL, Paraguay PY, Palestine PS, and Lebanon LB.  Let’s see how the top 10 changes when it comes to internet users. I used this ranking, which is based on numbers published by the International Telecommunications Union.\n Figure 5.6: Interesting to see how the ranking changed! Leading the charts is the island of Samoa, followed by Armenia. Countries that remain in the top 10 are Cambodia KH, Philippines PH, Kenya KE, and Tajikstan TJ. Other new countries are Timor-Leste TL, Paraguay PY, Palestine PS, and Lebanon LB.  Now let’s look at the distributions of our numeric variables.\n Figure 5.7: Half of Kiva loans are 4.22 USD or less. Although the maximum loan is 100,000 USD, 75% of loans are equal to or less than 89.79 USD.   Figure 5.8: The loan term in months is a bimodal distribution with its first peak around 8 months and its second around 12, the median.   Figure 5.9: The lender count has a text book right-skewed distribution with a median of 12 lenders.   Figure 5.10: Lender count ranges between 5 and 40 for the top 16 countries that request the most Kiva loans.   Figure 5.11: The median time between posting the loan and the loan being fully funded is about 9.2 days. It is a bimodal distribution with its first peak around 6 days and its second peak around 30 days.   Figure 5.12: The total time between posting the loan and it being disbursed has a funky looking multimodal distribution. It seems to be cut off at 30 days with peaks at 7, 14, 21 and 30 days. Median time is 17.2 days.   6 Techniques used  I used the quantmode package to convert all loans into a unique currency (US dollars) for comparison. There were two currencies that were unavailable.\n Taking all the different levels that came in the borrower_genders variable and creating five neat categories to better understand who are the borrowers was good practice with lists and the stringr package.\n   7 Questions from this analysis  Why is Retail and not Food (as in other regions) the second most common use for loans in Asia.\n Who are the givers? Where are they? Does proximity of the lender to the borrower have anything to do with funding times?\n Does the Kiva website have anything to do with funding times? For example, giving_time, the time between posting the loan and the loan being fully funded, has two peaks, at around one week and one month. Is this due to the platform and the promotion of loans that have been posted for a certain amount of time?\n   ","date":1530921600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530921600,"objectID":"41429b3fa65e9e188171ee8bdc0f02eb","permalink":"/post/exploring-kiva-loans/","publishdate":"2018-07-07T00:00:00Z","relpermalink":"/post/exploring-kiva-loans/","section":"post","summary":"What do Kiva loans around the world look like?","tags":["Kaggle","Kiva"],"title":"Exploring Kiva loans","type":"post"},{"authors":null,"categories":null,"content":"\u0026hellip;\n","date":1530136800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530136800,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2018-06-28T00:00:00+02:00","relpermalink":"/privacy/","section":"","summary":"\u0026hellip;","tags":null,"title":"Privacy Policy","type":"page"}]