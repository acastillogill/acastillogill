<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ángela Castillo-Gill on Ángela Castillo-Gill</title>
    <link>/</link>
    <description>Recent content in Ángela Castillo-Gill on Ángela Castillo-Gill</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Ángela Castillo-Gill</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Learning Friday</title>
      <link>/note/learning-friday/</link>
      <pubDate>Fri, 30 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/learning-friday/</guid>
      <description>


&lt;p&gt;There are a bunch of packages that I’ve accumulated in the “to learn/later” basket but I never get around to them. Today, the last day of November 2018, I’m going to give myself two hours for some calm documentation reading because this time investment will bring much rewards in the future.&lt;/p&gt;
&lt;p&gt;Let’s begin with a laundry list of the packages I want to learn:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jennybc/here_here&#34;&gt;here&lt;/a&gt;: brought to my attention by Jenny Brian.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ropenscilabs.github.io/drake-manual/index.html#the-drake-r-package&#34;&gt;drake&lt;/a&gt;: Recommended by Sina Rueger, fellow member of R-Ladies Remote.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/krlmlr/testthat&#34;&gt;testthat&lt;/a&gt;: mentioned in this week’s DataFramed podcast.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html&#34;&gt;roxygen2&lt;/a&gt;: used by of R-Ladies Remote.&lt;/li&gt;
&lt;li&gt;Creating a &lt;code&gt;makefile&lt;/code&gt; with .Rmd. Heather Turner, also from R-Ladies Remote suggest: &lt;code&gt;rmarkdown::render&lt;/code&gt; instead of &lt;code&gt;source()&lt;/code&gt; when working with &lt;code&gt;.Rmd()&lt;/code&gt; files.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;OK. Great. Now that I’ve gotten myself all nice and overwhelmed. I need to plan how to read the documentation and slowly integrate those packages into my workflow.&lt;/p&gt;
&lt;p&gt;The one that I simply must try now is &lt;code&gt;here&lt;/code&gt;. I keep using pathways I only have in both the PhD work and on this site. Awful. Jenny Brian would come and set my laptop on fire. Okidokes.&lt;/p&gt;
&lt;p&gt;Let’s fix that.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What next?</title>
      <link>/note/what-next/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/what-next/</guid>
      <description>


&lt;div id=&#34;daily-goals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Daily goals:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Reflect on what post I would like do next, what tool will I use, what dataset will I use and why?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The last posts I haven’t planned properly in terms of what tool I’m going to use, why is it well-suited, and what dataset would I like to use. Don’t get me wrong, there are some positive aspects to this lacking of planning. I get to see lots of raw datasets where I can see the challenges of dealing with raw data.&lt;/p&gt;
&lt;p&gt;Despite linear regression being such a foundational thing, I haven’t used it in any post so far. I did attempt to do it with the Kiva loans one, but the dataset was so large that it got a little bit out of hand. Performing linear regression on 2,000 observations is very much different than doing so on a dataset with 500,000. I rememeber the distributions being bimodal and that tripping me up and also the number of outliers at a time.&lt;/p&gt;
&lt;p&gt;I asked for help at some point and someone on CrossValidated, I think quite appropriately so, said: stop and take baby steps. And I didn’t want to but now I kind of see why. The frustration of taking very large steps can exhaust and feel daunting. It’s much better to take small steps and come across issues slowly that seem figure-out-able.&lt;/p&gt;
&lt;p&gt;So for this next post I would like to use a linear regression. Why? Because I would like to properly test my understanding of the assumptions it carries, what are the steps I should take, how will I deal with outliers, and how I will I choose the best model and why.&lt;/p&gt;
&lt;p&gt;Getting linear regression is critical/crucial/essential in this whole data science thing and I intend to get it right.&lt;/p&gt;
&lt;p&gt;Tuesday and Wednesday were this month’s R-Ladies reading group and it was bloody EXCELLENT. I learned so much about the importance of being organised when doing computing and how to do so. I’m having a ball reading Hadley Wickam’s reading list and learning lots. I’ve also started the book Introduction to Statistical Learning which is the foundation of these blogpost.&lt;/p&gt;
&lt;p&gt;Ok, moving on.&lt;/p&gt;
&lt;p&gt;I want to apply linear regression because it is critical I get right, but what will be the dataset and how do I come up with a question? Ah. Yes. A bit more tricky. The reason it’s a bit more tricky is because receiving a simple dataset and cleaning it, getting to know it, etc, is just a pre step to figuring out what is the question we want to ask.&lt;/p&gt;
&lt;p&gt;Oh, just had an idea, why don’t I just use the same one? I could use the property price and get on with it. It could help look at the effects of adjusting for another variable and also, how to deal with dummy variables. Hmmmm.&lt;/p&gt;
&lt;p&gt;Could be a good idea, since it’s a dataset I know well, I’m sort of tempted. Also, it could be a good start to compare the results with property values. I definitely need to improve my interpretation of results. It’s a dataset I understand well so why not. Ok. Same dataset. I wanted to go through the Tidy Tuesday one but maybe in another post. I can’t move on until I get linear regression right. I mean I can but I don’t want to.&lt;/p&gt;
&lt;p&gt;So, what is the first condition for a linear regression:&lt;/p&gt;
&lt;p&gt;-The input (&lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;) and the output (&lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt;) do not have to be normally distributed.
-The &lt;span class=&#34;math inline&#34;&gt;\(Y|X\)&lt;/span&gt; do have to be linearly related.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Finished!</title>
      <link>/note/finished-property-prices/</link>
      <pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/finished-property-prices/</guid>
      <description>


&lt;div id=&#34;daily-goals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Daily goals:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Daily goals were to finish the blogpost and I think I have. Tomorrow I will start with k-means clustering!&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Do it again.</title>
      <link>/note/do-it-again/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/do-it-again/</guid>
      <description>


&lt;div id=&#34;daily-goals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Daily goals:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bin the neighbhorhood variable properly.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;goal-1-bin-the-neighborhood-variable-properly.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 1: Bin the neighborhood variable properly.&lt;/h2&gt;
&lt;p&gt;So job done than on that part. Now all I have to do it seems is write up the summary and then proofread. Let me see the whole thing again and decide.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Editing property prices - part 2</title>
      <link>/note/editing-property-prices/</link>
      <pubDate>Mon, 26 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/editing-property-prices/</guid>
      <description>


&lt;div id=&#34;daily-goals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Daily goals:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bin the neighbhorhood variable properly.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;goal-1-bin-the-neighborhood-variable-properly.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 1: Bin the neighborhood variable properly.&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Editing property prices</title>
      <link>/note/editing-property-prices/</link>
      <pubDate>Sat, 24 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/editing-property-prices/</guid>
      <description>


&lt;div id=&#34;daily-goals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Daily goals:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Edit the final blogpost.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;goal-1-edit-the-final-blogpost.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 1: Edit the final blogpost.&lt;/h2&gt;
&lt;p&gt;So today I’ve had a session of ruthlessly cutting out a lot of the things I had written and I guess, a lot of the work I had done. In the end the most important thing is to make a much more readable blogpost that goes to the point. When I started writing that blogpost, I hadn’t really started this daily journal to think out loud about I was doing. Today I made a lot of changes.&lt;/p&gt;
&lt;p&gt;I still have to write the interpretation, the summary, final proofing, and then upload to Kaggle.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Structure</title>
      <link>/note/structure/</link>
      <pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/structure/</guid>
      <description>


&lt;div id=&#34;daily-goals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Daily goals:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Plan the structure of the final blogpost.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;goal-1-plan-the-structure-of-the-final-blogpost.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 1: Plan the structure of the final blogpost.&lt;/h2&gt;
&lt;p&gt;This is the second blogpost I’m going to be publishing. I want to set out a plan and know well what I’m going to include and what I’m not going to include.&lt;/p&gt;
&lt;p&gt;The first step is to remind myself why I’m publishing this blog post. The reason is…&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;drumroll&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I want to show how I applied the regression tree algorithm to a Kaggle dataset.&lt;/p&gt;
&lt;p&gt;If that is my goal, then let’s answer the other four W’s and one H in function of that.&lt;/p&gt;
&lt;p&gt;Where: On this very site and also as a kernel on Kaggle.
When: Sometime next week.
Who’s the reader: Interested people in the regression tree algorithm, Kaggle users and learners
What: I want to do a short but informative blogpost. Where I explain a problem and the solution I propose.&lt;/p&gt;
&lt;p&gt;How: By being very concise about what is the final goal of the post and what is the roadmap to get there.&lt;/p&gt;
&lt;p&gt;So that’s like my meta intention for the blogpost, now in detail I think it would go something like:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Business question: Let’s say that you have a house in Ames, Iowa (that’s where the dataset is from). Let’s say you are about to buy a house or even buy some terrain and you want to understand what makes a house more valuable than an other one.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A quite cool business question that I don’t know if I’ll be able to answer but we’ll see is: are there any features that can be added to the house that can significantly increase it’s value. A good idea would be to buy a house for a low value with features that are modifiable (i.e. kitchen, exterior, etc.), modify those features and resell that house.&lt;/p&gt;
&lt;p&gt;It’s a naive idea since probably the potential to do that is already reflected in the prices.&lt;/p&gt;
&lt;p&gt;So I would state to be the goal, then I would say, here is how I’m going to solve that problem, I’m going to understand the dataset, how it was collected, what are the key features, and then, I’m going to motivate the use of one algorithm over another. Like what’s so special about regression trees that I’m going to use it.&lt;/p&gt;
&lt;p&gt;Then I’m going to use it and explain the outcome. I want to give a lot of attention to the interpretability since this is a particular advantage of regression trees and prime reason for selecting it.&lt;/p&gt;
&lt;p&gt;There was some important work in terms of data cleaning. I want to give this some attention but not so much that the post becomes about data cleaning.&lt;/p&gt;
&lt;p&gt;OK. So I think that’s a solid plan:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Business question&lt;/li&gt;
&lt;li&gt;Dataset description&lt;/li&gt;
&lt;li&gt;Motivation for tool&lt;/li&gt;
&lt;li&gt;Steps&lt;/li&gt;
&lt;li&gt;Result&lt;/li&gt;
&lt;li&gt;Interpretation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Oh! Another think I forgot, I do have a nagging question about whether I should partition my data or not with rpart. Somewhere I think I read that cross-validation was built into rpart so I shouldn’t split my data. But then the DataCamp course I did said I should… One for CrossValidated I think.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The perfect balance</title>
      <link>/note/the-perfect-balance/</link>
      <pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/the-perfect-balance/</guid>
      <description>


&lt;div id=&#34;daily-goals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Daily goals:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Discuss the proposed answer for yesterday’s question: why did the performance metrics go up?&lt;/li&gt;
&lt;li&gt;Talk about next steps for the property values post.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;goal-1-discuss-the-proposed-answer-for-yesterdays-question-why-did-the-performance-metrics-go-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 1: Discuss the proposed answer for yesterday’s question: why did the performance metrics go up?&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;rmse&lt;/code&gt; for the base model &lt;code&gt;homes_model&lt;/code&gt; is 36595.3198 and the &lt;code&gt;mae&lt;/code&gt; is 25784.2432.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;rmse&lt;/code&gt; for the improved model &lt;code&gt;homes_model_opt&lt;/code&gt; is 40848.44 and the &lt;code&gt;mae&lt;/code&gt; is 28394.1.&lt;/p&gt;
&lt;p&gt;Yesterday I was pretty annoyed because I didn’t understand why the smallest tree with lowest cross validated error had worse performance metrics than a large tree. Like, mmm, why?&lt;/p&gt;
&lt;p&gt;So I kind of felt I was hitting a dead end and decided to take to trusty old Stack Overflow to ASK FOR HELP. I asked and I received.&lt;/p&gt;
&lt;p&gt;I received two helpful in less than twelve hours (Stack Overflow never change). The first suggested I consult the book Introduction to Statistical Learning from Springer. I have obtained it and it will be my reference book right after the one I’m currently doing called Regression Models. I have looked at the Table of Contents and it has a chapter dedicated to trees so that will come very much in handy.&lt;/p&gt;
&lt;p&gt;The second answer was this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There is always a balance between a tree big enough to represent the variation in the data and not so big that it overfits. The reason that bigger trees sometimes produce better results is that they more finely partition the data and so represent nuances. The reason that smaller trees sometimes produce better results is that there is less of a problem with overfitting. But if the smallest tree was always the best, the why not just use one node? Just using the root node would estimate the value using the average - not likely to be really accurate. The two conflicting forces must be balanced to get the best result.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My initial thoughts yesterday were that because a smaller tree, was ummm, well, smaller, it would have better performance on unseen data than a larger tree.&lt;/p&gt;
&lt;p&gt;That line they use:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;But if the smallest tree was always the best, the why not just use one node?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Really captures my inital assumption. Exactly, if a smaller tree is always better, why not just use the smallest tree possible, the root node.&lt;/p&gt;
&lt;p&gt;And so the mystery is partially solved in that actually the bigger tree did perform better.&lt;/p&gt;
&lt;p&gt;I will consult the book chapter now to see if I can see any immediately useful tips. Wow! That book is super helfpul and explains well how the &lt;code&gt;RPart&lt;/code&gt; algorithm works. I look forward to moving on to it soon.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goal-2-talk-about-next-steps-for-the-property-values-post.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 2: Talk about next steps for the property values post.&lt;/h2&gt;
&lt;p&gt;So now that we’ve decided that the larger tree is better, there is also an option where we use the data split we haven’t used to tune the model’s hyperparamters that we talked about in previous posts.&lt;/p&gt;
&lt;p&gt;I would think the next steps are to do that with the tree that performs better and see if we can get any improvements.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grid-search-for-best-hyperparameters-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grid search for best hyperparameters &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
# Establish a list of possible values for minsplit and maxdepth From 1 to 30
# by 5
min_split &amp;lt;- seq(1, 10, 1)
# From 1 to 30 by 1
max_depth &amp;lt;- seq(1, 30, 1)

# Create a dataframe containing all combinations
hyper_grid &amp;lt;- expand.grid(min_split = min_split, max_depth = max_depth)

# Check dimensions
dim(hyper_grid)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create an empty list to store the models

models &amp;lt;- list()

# Execute the grid search

for (i in 1:nrow(hyper_grid)) {
    
    # Get min_split, max_depth values at row i
    
    minsplit &amp;lt;- hyper_grid$min_split[i]
    max_depth &amp;lt;- hyper_grid$max_depth[i]
    
    # Train the model and store in the list
    
    models[[i]] &amp;lt;- rpart(formula = SalePrice ~ ., data = homes_train, method = &amp;quot;anova&amp;quot;, 
        minsplit = min_split, maxdepth = max_depth)
    
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create an empty vector to store RMSE values
rmse_values &amp;lt;- c()
# Create an empty vector to store MAE values
mae_values &amp;lt;- c()

# Compute validation RMSE
for (i in 1:length(models)) {
    
    # Retrieve the ith model from the list
    
    model &amp;lt;- models[[i]]
    
    # Generate predictions on homes_valid validation set
    
    pred_grid &amp;lt;- predict(object = model, newdata = homes_valid)
    
    # Compute validation RMSE and add to the
    
    rmse_values[i] &amp;lt;- rmse(actual = homes_valid$SalePrice, predicted = pred_grid)
    
    mae_values[i] &amp;lt;- mae(actual = homes_valid$SalePrice, predicted = pred_grid)
    
    
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Identifying the model with smallest validation set RMSE
best_model_RMSE &amp;lt;- models[[which.min(rmse_values)]]
# Print the model parameters of the best_model_RMSE
best_model_RMSE$control
# Compute test set RMSE on best_model_RMSE
pred &amp;lt;- predict(object = best_model_RMSE, newdata = homes_test)

rmse(actual = homes_test$SalePrice, predicted = pred)

# Identifying the model with smallest validation set MAE
best_model_MAE &amp;lt;- models[[which.min(mae_values)]]
# Print the model parameters of the best_model_MAE
best_model_MAE$control
# Compute test set MAE on best_model_MAE
pred &amp;lt;- predict(object = best_model_MAE, newdata = homes_test)

mae(actual = homes_test$SalePrice, predicted = pred)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/note/Thursday-22-11-2018_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is something wrong I think because the best tree after grid search has an &lt;code&gt;RMSE&lt;/code&gt; higher than the base tree. And then if we look at the RMSE values resulting from the grid search they are all the same so it seems like the tree doesn’t attempt any splits. WHY?
Another why?&lt;/p&gt;
&lt;p&gt;I think instead of doing this grid search which needs a proper study, I’m going to write up the full post and everything I’ve done with the Kaggle dataset. Better to talk about the interpretability and the next bits too.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Code from here until the end of the post was adapted from DataCamp’s course on Machine Learning with Trees.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why did error go up?</title>
      <link>/note/why-did-error-go-up/</link>
      <pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/why-did-error-go-up/</guid>
      <description>


&lt;div id=&#34;goals-for-todays-session-are&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goal’s for today’s session are:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Figure out why the &lt;code&gt;RMSE&lt;/code&gt; and &lt;code&gt;MSE&lt;/code&gt; went up when I chose an “improved” tree.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;goal-1-figure-out-why-the-rmse-and-mse-went-up-when-i-chose-an-improved-tree.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 1: Figure out why the &lt;code&gt;RMSE&lt;/code&gt; and &lt;code&gt;MSE&lt;/code&gt; went up when I chose an “improved” tree.&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;rmse&lt;/code&gt; for the base model &lt;code&gt;homes_model&lt;/code&gt; is 36182.1117 and the &lt;code&gt;mae&lt;/code&gt; is 25797.5518.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;rmse&lt;/code&gt; for the improved model &lt;code&gt;homes_model_opt&lt;/code&gt; is 40848.44 and the &lt;code&gt;mae&lt;/code&gt; is 28394.1.&lt;/p&gt;
&lt;p&gt;This shouldn’t be the case, or should it?&lt;/p&gt;
&lt;p&gt;Let’s think what an increase in these two types of performance metrics mean. In the &lt;code&gt;rmse&lt;/code&gt; case, yesterday we saw that this is a measure of how much the fit deviates from real values.
An increase of 4666.3268 means that the pruned model is a worse fit. WHY? Shouldn’t a smaller tree prevent overfitting?&lt;/p&gt;
&lt;p&gt;I’ve just posted the question on Stack Overflow because right now I don’t have any ideas… maybe there is nothing wrong with performance metrics going up? But that doesn’t make sense. Wasn’t the whole process of choosing the best &lt;code&gt;cp_index&lt;/code&gt; to choose a tree with improved performance?&lt;/p&gt;
&lt;p&gt;I am very confused. It doesn’t make sense. I am going to look at the documentation again to see if I can find anything on performance metrics.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An objective measure</title>
      <link>/note/an-objective-measure/</link>
      <pubDate>Tue, 20 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/an-objective-measure/</guid>
      <description>


&lt;div id=&#34;goals-for-todays-session-are&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goal’s for today’s session are:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Evaluate the &lt;code&gt;RMSE&lt;/code&gt; and &lt;code&gt;MSE&lt;/code&gt; in the base and improved tree.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;goal-1-evaluate-the-rmse-and-mse-in-the-base-and-improved-tree.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 1: Evaluate the &lt;code&gt;RMSE&lt;/code&gt; and &lt;code&gt;MSE&lt;/code&gt; in the base and improved tree.&lt;/h2&gt;
&lt;p&gt;Right. So we know from previous posts that we pick the smallest tree within one standard error of the lowest &lt;code&gt;xerror&lt;/code&gt;. Eggsellent. BUT. What can we use to objectively measure the difference between trees? We can use the error between a less pruned tree and the improved one.&lt;/p&gt;
&lt;p&gt;There are two measurements of error, the &lt;code&gt;rmse&lt;/code&gt; and the &lt;code&gt;mae&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The root mean square error (RMSE) or root mean standard deviation (RMSD) is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[RMSD =\sqrt {\frac {\sum _{i=1}^{n}({\hat {y}}_{i}-y_{i})^{2}}{n}}\]&lt;/span&gt;
Where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the sample size, and &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}_{i}\)&lt;/span&gt; are the predicted values for the dependent variable &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Without reading the definition, this looks like it it “punishing” the differences between the estimated values and actual values, then dividing it per observation and taking the square root so the units are the actual ones for the response variable.&lt;/p&gt;
&lt;p&gt;The other &lt;em&gt;performance metric&lt;/em&gt;, which is the actual name for these tree-measuring-metrics, is called the &lt;code&gt;MAE&lt;/code&gt;. Which I think stands for the mean absolute error is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[MAE ={\frac {\sum _{i=1}^{n}\left|y_{i}-\hat {y_i}\right|}{n}}\]&lt;/span&gt;
It seems like the difference here is that the &lt;code&gt;RMSE&lt;/code&gt; punishes differences while the &lt;code&gt;MAE&lt;/code&gt; doesn’t emphasise large differences as much.&lt;/p&gt;
&lt;p&gt;Now let’s calculate these performance metrics for the original tree and then the pruned tree.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:basic-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/note/Tuesday-20-11-2018_files/figure-html/basic-model-1.png&#34; alt=&#34;A first model suggests 9 splits with 10 nodes.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: A first model suggests 9 splits with 10 nodes.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;rmse&lt;/code&gt; for the base model &lt;code&gt;homes_model&lt;/code&gt; is 4.68910^{4} and the &lt;code&gt;mae&lt;/code&gt; is 3.16810^{4}.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:improved-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/note/Tuesday-20-11-2018_files/figure-html/improved-model-1.png&#34; alt=&#34;Improved model has 7 splits and 8 nodes.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Improved model has 7 splits and 8 nodes.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;rmse&lt;/code&gt; for the improved model &lt;code&gt;homes_model_opt&lt;/code&gt; is 4.99610^{4} and the &lt;code&gt;mae&lt;/code&gt; is 3.39110^{4}.&lt;/p&gt;
&lt;p&gt;Ugh oh. It has gone up. Something is wrong. Why?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The basics</title>
      <link>/note/the-basics/</link>
      <pubDate>Sat, 17 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/the-basics/</guid>
      <description>


&lt;div id=&#34;goals-for-todays-session-are&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goal’s for today’s session are:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Go over the difference between standard deviation, variance, and standard error.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Evaluate the &lt;code&gt;RMSE&lt;/code&gt; and &lt;code&gt;MSE&lt;/code&gt; in the base and improved tree.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;goal-1-go-over-the-difference-between-standard-deviation-variance-and-standard-error.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 1: Go over the difference between standard deviation, variance, and standard error.&lt;/h2&gt;
&lt;p&gt;So in a few machine learning algorithms, if not all of them, there are three terms that are contantly being used with “mean” and “median”: Standard deviation, variance, and standard error. I have probably studied these three terms and written their formulas too many times to mention here and you know what? If you asked me right now to explain the difference with every day examples, I don’t think I could. Over time they have become a mush and I realise the reason they don’t stick is because they are something I haven’t associated with something that makes sense to me.&lt;/p&gt;
&lt;p&gt;So today I will define these world-renowned measures of dispersion today and try to come up with examples from my every day life.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sample variance:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[s^2 = \frac{1}{N-1} \sum_{i=1}^N (x_i - \overline{x})^2\]&lt;/span&gt;
Where, &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the sample size; &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is the observation or piece of data, and &lt;span class=&#34;math inline&#34;&gt;\(\overline{x}\)&lt;/span&gt; is the data set mean.&lt;/p&gt;
&lt;p&gt;The differences between each point and the mean are squared to avoid negative differences cancelling positive ones. The variance units are the mean units squared.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: Today I went for my grocery shop so let’s work from there. Let’s say I add lemons (1.68), apples (1.9), grapes (1.4), bananas (.97), and pine nuts (5.4) to my receipt.&lt;/p&gt;
&lt;p&gt;Then I ask, I wonder what the average price of each ingredient was, so I add and divide to create a simple average.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grocery_bill&amp;lt;- c(1.68,1.9,1.4,0.97,5.4)
sum(grocery_bill)/length(grocery_bill)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.27&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;2.27, fabulous. Now, I want to see how much variation there is amongst each ingredient.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grocery_bill-mean(grocery_bill)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.59 -0.37 -0.87 -1.30  3.13&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems like the last two items, bananas and pine nuts varied quite a bit more from the mean. Let’s add the differences now because let’s say I want to show my husband how bigger the difference is when we add pine nuts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(grocery_bill-mean(grocery_bill))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They have cancelled out. I think this is what you do when you want the regression line to go through the origin.&lt;/p&gt;
&lt;p&gt;In order to take each difference without it cancelling out, we SQUARE. AHA. Ok. I think maybe this will help it stick.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum((grocery_bill-mean(grocery_bill))^2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12.7288&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great, now we divide by the 5 items.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sum((grocery_bill-mean(grocery_bill))^2))/5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.54576&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(grocery_bill)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.1822&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I also computed the variance with base R and they were different. Why? Because I forgot the &lt;span class=&#34;math inline&#34;&gt;\(N-1\)&lt;/span&gt; bit. Classic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sum((grocery_bill-mean(grocery_bill))^2))/4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.1822&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var(grocery_bill)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.1822&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now they are the same. But why do we substract &lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt;. It seems the answer is because we want to have an unbiased estimator and it’s called the &lt;em&gt;Bessel’s correction&lt;/em&gt; (?????).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sample standard deviation:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[s = \sqrt{\frac{1}{N-1} \sum_{i=1}^N (x_i -\overline{x})^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, the sample standard deviation is just the square root of the variance. It has the same units as the mean. On our groceries:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt((sum((grocery_bill-mean(grocery_bill))^2))/4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.783872&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(grocery_bill)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.783872&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is the same! Ok. Well done. So 1.78 Euros is how much each item of my grocery list on average deviates from the mean which was 2.27.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Standard error:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_{\bar {x}}\ \approx {\frac {s}{\sqrt {n}}}\]&lt;/span&gt;
So from what I can see it’s the sample standard deviation divided by the square root of the sample size. mmmm, but why?&lt;/p&gt;
&lt;p&gt;According to Wikipedia: &lt;em&gt;The standard error (SE) of a statistic (usually an estimate of a parameter) is the standard deviation of its sampling distribution…&lt;/em&gt;
mmmmm ok.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;or an estimate of that standard deviation.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Looking at the formula, this latter definition makes more sense.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If the parameter or the statistic is the mean, it is called the standard error of the mean (SEM).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;But actually, not. I don’t really get this, yet.&lt;/p&gt;
&lt;p&gt;Then it reads: &lt;em&gt;It can also be understood as the standard deviation of the error in the sample mean with respect to the true mean (or an estimate of that statistic).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I think I’m going to let this one sink for today because I gotta run.&lt;/p&gt;
&lt;p&gt;You know what, this was extremely basic but a helpful reminder I should do more often. Would be fun to compare the standard deviation between my grocery shop and my husband’s. I need a refresher like this with the median and distribution skewness.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Did the tree improve?</title>
      <link>/note/did-the-tree-improve/</link>
      <pubDate>Fri, 16 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/did-the-tree-improve/</guid>
      <description>


&lt;div id=&#34;goals-for-todays-session-are&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goal’s for today’s session are:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Show if the variable change made the tree more interpretable.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Splitting data
# Randomly assign rows to ids (1/2/3 represents train/valid/test)
# This will generate a vector of ids of length equal to the number of rows
# The train/valid/test split will be approximately 70% / 15% / 15% 
set.seed(1)
assignment &amp;lt;- sample(1:3, size = nrow(homes),
                     prob = c(0.7,0.15,0.15),
                     replace = TRUE)

# Create a train, validation and tests from the original data frame 
homes_train &amp;lt;- homes[assignment == 1, ]    # subset the homes data frame to training indices only
homes_valid &amp;lt;- homes[assignment == 2, ]  # subset the homes data frame to validation indices only
homes_test &amp;lt;- homes[assignment == 3, ]   # subset the homes data frame to test indices only&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Train the model
homes_model &amp;lt;- rpart(formula = SalePrice ~ ., 
                     data = homes_train, 
                     method = &amp;quot;anova&amp;quot;)

# Plot the basic model
rpart.plot(x = homes_model, type = 5, extra = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:basic-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/note/Fri-16-11-2018_files/figure-html/basic-model-1.png&#34; alt=&#34;A first model suggests 9 splits with 10 nodes.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: A first model suggests 9 splits with 10 nodes.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now we are going to look at the CP table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot the &amp;quot;CP Table&amp;quot;
plotcp(homes_model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/note/Fri-16-11-2018_files/figure-html/tuning-hyperparameters-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Print the &amp;quot;CP Table&amp;quot;
print(homes_model$cptable)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            CP nsplit rel error    xerror       xstd
## 1  0.44576120      0 1.0000000 1.0034376 0.09648934
## 2  0.11185759      1 0.5542388 0.5578944 0.05448931
## 3  0.07787835      2 0.4423812 0.4670323 0.05424358
## 4  0.03937406      3 0.3645029 0.4001065 0.03612001
## 5  0.02682084      4 0.3251288 0.3604073 0.03569390
## 6  0.02116148      6 0.2714871 0.3619070 0.04101147
## 7  0.01526856      7 0.2503256 0.3454768 0.03781928
## 8  0.01468395      8 0.2350571 0.3367953 0.03724153
## 9  0.01059506      9 0.2203731 0.3260323 0.03727571
## 10 0.01000000     10 0.2097781 0.3241648 0.03732242&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According the &lt;code&gt;cptable&lt;/code&gt;, the lowest &lt;code&gt;xerror&lt;/code&gt; is in row 10, to that we add the &lt;code&gt;xstd&lt;/code&gt; error and obtain: &lt;span class=&#34;math inline&#34;&gt;\(0.39529\)&lt;/span&gt;. We see that the smallest error with &lt;code&gt;xerror&lt;/code&gt; below that is the tree with 7 splits and 8 nodes. I will use that in the improved plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Retrieve optimal cp value based on cross-validated error
opt_index &amp;lt;- 7
cp_opt &amp;lt;- homes_model$cptable[opt_index, &amp;quot;CP&amp;quot;]

# Prune the model (to optimized cp value)
homes_model_opt &amp;lt;- prune(tree = homes_model, 
                         cp = cp_opt)
                          
# Plot the optimized model
 rpart.plot(x = homes_model_opt, type = 5, extra = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:improved-model&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/note/Fri-16-11-2018_files/figure-html/improved-model-1.png&#34; alt=&#34;Improved model has 7 splits and 8 nodes.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Improved model has 7 splits and 8 nodes.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Tomorrow, I will look to see if there are any objective measures of error, clue, there are! And apply them to compare both models.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Binning the neighborhood variable (for real now)</title>
      <link>/note/binning-the-neighborhood-variable-for-real-now/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/binning-the-neighborhood-variable-for-real-now/</guid>
      <description>


&lt;div id=&#34;goals-for-todays-session-are&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goal’s for today’s session are:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a new variable that captures what the variable is providing without so many levels.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;goal-1-create-a-new-variable-that-captures-what-the-variable-is-providing-without-so-many-levels.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 1: Create a new variable that captures what the variable is providing without so many levels.&lt;/h2&gt;
&lt;p&gt;As we saw yesterday from looking at the quantiles, we have 25 neighborhoods but very few houses are in the most expensive neighborhoods.&lt;/p&gt;
&lt;p&gt;I’m looking at the plot again and I realise there is something very odd with it. None of the homes costs more than 755,000 and yet the plot goes up to 30,000,000. I realised that I was adding the &lt;code&gt;SalePrice&lt;/code&gt; of individuals homes. Now here is another go with actual average &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/note/Thurs-15-11-2018_files/Mean_sale_price.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Much better. Now we can see that &lt;code&gt;NoRidge&lt;/code&gt; has the highest average &lt;code&gt;SalePrice&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, let’s look at this again in light of the quantiles,&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;gt; neigh_quant
# A tibble: 5 x 3
  names      x incomes      
  &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        
1 0%     34900 Lower        
2 25%   129975 Middle_Lower 
3 50%   163000 Lower        
4 75%   214000 Middle_Higher
5 100%  755000 Higher &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It seems like the top 8 neighborhoods would go into the &lt;code&gt;Higher&lt;/code&gt; bracket and none would go into the lowest.&lt;/p&gt;
&lt;p&gt;The way that I’m going to bin the neighborhoods is that I will create a variable called &lt;code&gt;Neighboord_type&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Houses that have neighborhoods with average &lt;code&gt;SalePrice&lt;/code&gt; above 200,000 will go into the level “Fancy”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Houses that have neighborhoods with average &lt;code&gt;SalePrice&lt;/code&gt; above 150,000 and below 200,000 will go into the level “Somewhat_fancy”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Houses that have neighborhoods with average &lt;code&gt;SalePrice&lt;/code&gt; above 100,000 and below 150,000 will go into the level “Not_fancy”.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The rest will go into the “OK” level. Because it’s ok if you don’t live in a fancy neighborhood.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And this is what it looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/note/Thurs-15-11-2018_files/New_var.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Job done I would say.&lt;/p&gt;
&lt;p&gt;Tomorrow I will do everything again with this new variable to see if the plot improves.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Binning the neighborhood variable</title>
      <link>/note/binning-the-neighborhood-variable/</link>
      <pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/binning-the-neighborhood-variable/</guid>
      <description>


&lt;div id=&#34;goals-for-todays-session-are&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goal’s for today’s session are:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Understand what information is the &lt;code&gt;neighboorhood&lt;/code&gt; variable providing&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a new variable that captures what the variable is providing without so many levels.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;goal-1-understand-what-information-is-the-neighboorhood-variable-providing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 1: Understand what information is the &lt;code&gt;neighboorhood&lt;/code&gt; variable providing&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levels(homes$Neighborhood)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Shows that the variable has 25 levels. Let’s see the relationship between each level and the response variable &lt;code&gt;SalesPrice&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Plotting the &lt;code&gt;neighboorhood&lt;/code&gt; variable shows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/note/Wed-14-11-2018_files/neigh_salesprice.png&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goal-2-create-a-new-variable-that-captures-what-the-variable-is-providing-without-so-many-levels.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 2: Create a new variable that captures what the variable is providing without so many levels.&lt;/h2&gt;
&lt;p&gt;Let’s say we want to create a variable with four levels, these would be the quantile cuts:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    0%    25%    50%    75%   100% 
 34900 129975 163000 214000 755000 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This shows for example that only very few neighborhoods will be in the last quantile (above &lt;code&gt;$755,000&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Tomorrow I will do the rest cause right now I gotta go.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Applying concepts learned to property prices</title>
      <link>/note/applying-concepts-learned-to-property-prices/</link>
      <pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/applying-concepts-learned-to-property-prices/</guid>
      <description>


&lt;div id=&#34;goals-for-todays-session-are&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Goal’s for today’s session are:&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Interpret the &lt;code&gt;cptable&lt;/code&gt; for the generic &lt;code&gt;homes_model&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Interpret the &lt;code&gt;rpart.plot&lt;/code&gt; associated to &lt;code&gt;homes_model&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Bin the &lt;code&gt;neighborhood&lt;/code&gt; variable and explain why I’m doing that.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;goal-1-interpret-the-cptable-for-the-generic-homes_model.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 1: Interpret the &lt;code&gt;cptable&lt;/code&gt; for the generic &lt;code&gt;homes_model&lt;/code&gt;.&lt;/h2&gt;
&lt;p&gt;With the &lt;code&gt;homes&lt;/code&gt; data as is (just removing missing values), I fited a model to predict &lt;code&gt;SalesPrice&lt;/code&gt; using &lt;code&gt;rpart()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;homes_model &amp;lt;- rpart(formula = SalePrice ~ ., 
                     data = homes_train, 
                     method = &amp;quot;anova&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the &lt;code&gt;cptable&lt;/code&gt; with &lt;code&gt;homes_model$cptable&lt;/code&gt;, I’ll apply the &lt;span class=&#34;math inline&#34;&gt;\(1-SE\)&lt;/span&gt; rule I explained yesterday:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/note/Tues-13-11-2018_files/cptable_rawhomes.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;First, we can see that the smallest cross-validation &lt;code&gt;xerror&lt;/code&gt; appears on row 11. Now let’s add 1 standard error &lt;code&gt;xstd&lt;/code&gt;: &lt;span class=&#34;math inline&#34;&gt;\(0.3074050+0.03541012=0.3428151\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The smallest tree with &lt;code&gt;xerror&lt;/code&gt; less than &lt;span class=&#34;math inline&#34;&gt;\(0.3428151\)&lt;/span&gt; is the tree appearing on row 8 with 8 splits and 9 nodes.&lt;/p&gt;
&lt;p&gt;Now, I look at the &lt;code&gt;plotcp(homes_model)&lt;/code&gt;, I’m confused to whether size of the tree refers to nodes or splits:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/note/Tues-13-11-2018_files/cpplot_rawhomes.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I think it refers to nodes because the &lt;code&gt;xerror&lt;/code&gt; for the tree with 9 nodes appears on the dotted line. A dotted line that intersects the 9-tree node must mean it’s the chosen one, right? :laughing:&lt;/p&gt;
&lt;p&gt;Now, let’s run the model again, but specifying the best model, with 8 splits.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Hard code best tree based on above explained analysis
opt_index &amp;lt;- 8
#Retrieve cp_opt
cp_opt &amp;lt;- homes_model$cptable[opt_index, &amp;quot;CP&amp;quot;]
# Prune the model (to optimized cp value)
homes_model_opt &amp;lt;- prune(tree = homes_model, 
                         cp = cp_opt)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I suppose hard coding the number 8 instead of writing the code to pick the best tree isn’t too elegant, but bear with me. Writing clean code is not the priority right now. :hankey:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goal-2-interpret-the-rpart.plot-associated-to-homes_model.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 2: Interpret the &lt;code&gt;rpart.plot&lt;/code&gt; associated to &lt;code&gt;homes_model&lt;/code&gt;.&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(x = homes_model_opt, type = 5, extra = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That code produces this plot:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/note/Tues-13-11-2018_files/rpartplot_rawhomes.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It has 8 splits and 9 nodes and we asked it to. Inside each terminal node is the number of observations which add up to 1025. Each split specifies the criteria for the observation being classified either into the left or right son.&lt;/p&gt;
&lt;p&gt;If we go back and look at the numeric variables that had the highest correlation with &lt;code&gt;SalesPrice&lt;/code&gt;, &lt;code&gt;OverallQual&lt;/code&gt; was leading followed by &lt;code&gt;GrLivArea&lt;/code&gt;. In the tree we can see that these variables appear further below &lt;code&gt;X1stFlrSF&lt;/code&gt; and &lt;code&gt;GarageArea&lt;/code&gt; appear later down.&lt;/p&gt;
&lt;p&gt;The plot shows the &lt;code&gt;neighborhood&lt;/code&gt; variable twice and in the second level. So it’s an important variable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goal-3-bin-the-neighborhood-variable-and-explain-why-im-doing-that.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal 3: Bin the &lt;code&gt;neighborhood&lt;/code&gt; variable and explain why I’m doing that.&lt;/h2&gt;
&lt;p&gt;So today I don’t think I’ll be able to the binning since my alloted time for this is almost up. But I will explain the why since it appears clearly on the graph.&lt;/p&gt;
&lt;p&gt;One of the advantages of regression trees is that they area easy to understand and visualise. Because the &lt;code&gt;neighborhood&lt;/code&gt; variable has so many levels, it is actually going against one of the reasons I picked it. The whole neighborhood variable is confusing. Explaining the key differences amongst neighborhoods is not straightforward using just the plot. So I’m going to explore what the relationship between those neighborhoods and price is tomorrow and see if I can create another variable that better reflects that relationship. Exciting! :sparkles:&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
