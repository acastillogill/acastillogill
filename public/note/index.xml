<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Ángela Castillo-Gill</title>
    <link>/note/</link>
    <description>Recent content in Notes on Ángela Castillo-Gill</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Ángela Castillo-Gill</copyright>
    <lastBuildDate>Thu, 13 Dec 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/note/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Checking model fit</title>
      <link>/note/checking-model-fit/</link>
      <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/checking-model-fit/</guid>
      <description>Data source
downloads_per_country &amp;lt;- r_downloads %&amp;gt;% group_by(country) %&amp;gt;% summarise(n = n()) %&amp;gt;% arrange(desc(n)) Now I’m going to give the countries full names, to do this I will use the ISO code to merge and obtain names.
iso_codes &amp;lt;- read_csv(here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;iso_countries.csv&amp;quot;)) downloads_per_country %&amp;lt;&amp;gt;% right_join(iso_codes, by = c(country = &amp;quot;iso&amp;quot;)) Which are the top ten countries in terms of downloads downloads_per_country %&amp;gt;% arrange(desc(n)) %&amp;gt;% filter(!</description>
    </item>
    
    <item>
      <title>Downloads per capita</title>
      <link>/note/downloads-per-capita/</link>
      <pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/downloads-per-capita/</guid>
      <description>Data source
downloads_per_country &amp;lt;- r_downloads %&amp;gt;% group_by(country) %&amp;gt;% summarise(n = n()) %&amp;gt;% arrange(desc(n)) Now I’m going to give the countries full names, to do this I will use the ISO code to merge and obtain names.
iso_codes &amp;lt;- read_csv(here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;iso_countries.csv&amp;quot;)) downloads_per_country %&amp;lt;&amp;gt;% right_join(iso_codes, by = c(country = &amp;quot;iso&amp;quot;)) Which are the top ten countries in terms of downloads downloads_per_country %&amp;gt;% arrange(desc(n)) %&amp;gt;% filter(!</description>
    </item>
    
    <item>
      <title>Nesting models</title>
      <link>/note/nesting-models/</link>
      <pubDate>Tue, 11 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/nesting-models/</guid>
      <description>Data source
downloads_per_country &amp;lt;- r_downloads %&amp;gt;% group_by(country) %&amp;gt;% summarise(n = n()) %&amp;gt;% arrange(desc(n)) Now I’m going to give the countries full names, to do this I will use the ISO code to merge and obtain names.
iso_codes &amp;lt;- read_csv(here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;iso_countries.csv&amp;quot;)) downloads_per_country %&amp;lt;&amp;gt;% right_join(iso_codes, by = c(country = &amp;quot;iso&amp;quot;)) Which are the top ten countries in terms of downloads downloads_per_country %&amp;gt;% arrange(desc(n)) %&amp;gt;% filter(!</description>
    </item>
    
    <item>
      <title>Adding population</title>
      <link>/note/adding-population/</link>
      <pubDate>Mon, 10 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/adding-population/</guid>
      <description>Data source
downloads_per_country &amp;lt;- r_downloads %&amp;gt;% group_by(country) %&amp;gt;% summarise(n = n()) %&amp;gt;% arrange(desc(n)) Now I’m going to give the countries full names, to do this I will use the ISO code to merge and obtain names.
iso_codes &amp;lt;- read_csv(here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;iso_countries.csv&amp;quot;)) downloads_per_country %&amp;lt;&amp;gt;% right_join(iso_codes, by = c(country = &amp;quot;iso&amp;quot;)) Which are the top ten countries in terms of downloads downloads_per_country %&amp;gt;% arrange(desc(n)) %&amp;gt;% filter(!</description>
    </item>
    
    <item>
      <title>Breaking up the HDI stats</title>
      <link>/note/breaking-up-the-hdi-stats/</link>
      <pubDate>Fri, 07 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/breaking-up-the-hdi-stats/</guid>
      <description>Data source
downloads_per_country &amp;lt;- r_downloads %&amp;gt;% group_by(country) %&amp;gt;% summarise(n = n()) %&amp;gt;% arrange(desc(n)) Now I’m going to give the countries full names, to do this I will use the ISO code to merge and obtain names.
iso_codes &amp;lt;- read_csv(here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;iso_countries.csv&amp;quot;)) downloads_per_country %&amp;lt;&amp;gt;% right_join(iso_codes, by = c(country = &amp;quot;iso&amp;quot;)) Which are the top ten countries in terms of downloads downloads_per_country %&amp;gt;% arrange(desc(n)) %&amp;gt;% filter(!</description>
    </item>
    
    <item>
      <title>Adding HDI</title>
      <link>/note/adding-hdi/</link>
      <pubDate>Thu, 06 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/adding-hdi/</guid>
      <description>Data source
downloads_per_country &amp;lt;- r_downloads %&amp;gt;% group_by(country) %&amp;gt;% summarise(n = n()) %&amp;gt;% arrange(desc(n)) Now I’m going to give the countries full names, to do this I will use the ISO code to merge and obtain names.
iso_codes &amp;lt;- read_csv(here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;iso_countries.csv&amp;quot;)) downloads_per_country %&amp;lt;&amp;gt;% right_join(iso_codes, by = c(country = &amp;quot;iso&amp;quot;)) Which are the top ten countries in terms of downloads downloads_per_country %&amp;gt;% arrange(desc(n)) %&amp;gt;% filter(!</description>
    </item>
    
    <item>
      <title>Describing outliers</title>
      <link>/note/describing-outliers/</link>
      <pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/describing-outliers/</guid>
      <description>Data source
downloads_per_country &amp;lt;- r_downloads %&amp;gt;% group_by(country) %&amp;gt;% summarise(n = n()) %&amp;gt;% arrange(desc(n)) Now I’m going to give the countries full names, to do this I will use the ISO code to merge and obtain names.
iso_codes &amp;lt;- read_csv(here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;iso_countries.csv&amp;quot;)) downloads_per_country %&amp;lt;&amp;gt;% right_join(iso_codes, by = c(country = &amp;quot;iso&amp;quot;)) Which are the top ten countries in terms of downloads downloads_per_country %&amp;gt;% arrange(desc(n)) %&amp;gt;% filter(!</description>
    </item>
    
    <item>
      <title>Adding variables</title>
      <link>/note/adding-variables/</link>
      <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/adding-variables/</guid>
      <description>Today I’ve added one variable internet_users which sums the total internet users per country. The point is to slowly add regressors or independent variables and see if they have a linear relationship with r-downloads. If yes, I’ll add them to a model.
Data source
downloads_per_country &amp;lt;- r_downloads %&amp;gt;% group_by(country) %&amp;gt;% summarise(n = n()) %&amp;gt;% arrange(desc(n)) Now I’m going to give the countries full names, to do this I will use the ISO code to merge and obtain names.</description>
    </item>
    
    <item>
      <title>New month, new post</title>
      <link>/note/new-month-new-post/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/new-month-new-post/</guid>
      <description>As a follow up to Friday’s post on the new packages and scripts I would like to make. I can report that I have learned the here package and was able to make a makefile too to organise all my projects. I have a good understanding now, I think. I’ll leave testthat for this Friday and roxygen2 for another rainy day.
It’s important to invest that time in learning things that will make my workflow more efficient.</description>
    </item>
    
    <item>
      <title>Learning Friday</title>
      <link>/note/learning-friday/</link>
      <pubDate>Fri, 30 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/learning-friday/</guid>
      <description>There are a bunch of packages that I’ve accumulated in the “to learn/later” basket but I never get around to them. Today, the last day of November 2018, I’m going to give myself two hours for some calm documentation reading because this time investment will bring much rewards in the future.
Let’s begin with a laundry list of the packages I want to learn:
here: brought to my attention by Jenny Brian.</description>
    </item>
    
    <item>
      <title>What next?</title>
      <link>/note/what-next/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/what-next/</guid>
      <description>Daily goals: Reflect on what post I would like do next, what tool will I use, what dataset will I use and why?  The last posts I haven’t planned properly in terms of what tool I’m going to use, why is it well-suited, and what dataset would I like to use. Don’t get me wrong, there are some positive aspects to this lacking of planning. I get to see lots of raw datasets where I can see the challenges of dealing with raw data.</description>
    </item>
    
    <item>
      <title>Finished!</title>
      <link>/note/finished-property-prices/</link>
      <pubDate>Wed, 28 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/finished-property-prices/</guid>
      <description> Daily goals: Daily goals were to finish the blogpost and I think I have. Tomorrow I will start with k-means clustering!   </description>
    </item>
    
    <item>
      <title>Do it again.</title>
      <link>/note/do-it-again/</link>
      <pubDate>Tue, 27 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/do-it-again/</guid>
      <description> Daily goals: Bin the neighbhorhood variable properly.  Goal 1: Bin the neighborhood variable properly. So job done than on that part. Now all I have to do it seems is write up the summary and then proofread. Let me see the whole thing again and decide.
  </description>
    </item>
    
    <item>
      <title>Editing property prices - part 2</title>
      <link>/note/editing-property-prices/</link>
      <pubDate>Mon, 26 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/editing-property-prices/</guid>
      <description> Daily goals: Bin the neighbhorhood variable properly.  Goal 1: Bin the neighborhood variable properly.   </description>
    </item>
    
    <item>
      <title>Editing property prices</title>
      <link>/note/editing-property-prices/</link>
      <pubDate>Sat, 24 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/editing-property-prices/</guid>
      <description>Daily goals: Edit the final blogpost.  Goal 1: Edit the final blogpost. So today I’ve had a session of ruthlessly cutting out a lot of the things I had written and I guess, a lot of the work I had done. In the end the most important thing is to make a much more readable blogpost that goes to the point. When I started writing that blogpost, I hadn’t really started this daily journal to think out loud about I was doing.</description>
    </item>
    
    <item>
      <title>Structure</title>
      <link>/note/structure/</link>
      <pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/structure/</guid>
      <description>Daily goals: Plan the structure of the final blogpost.  Goal 1: Plan the structure of the final blogpost. This is the second blogpost I’m going to be publishing. I want to set out a plan and know well what I’m going to include and what I’m not going to include.
The first step is to remind myself why I’m publishing this blog post. The reason is…
drumroll
I want to show how I applied the regression tree algorithm to a Kaggle dataset.</description>
    </item>
    
    <item>
      <title>The perfect balance</title>
      <link>/note/the-perfect-balance/</link>
      <pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/the-perfect-balance/</guid>
      <description>Daily goals: Discuss the proposed answer for yesterday’s question: why did the performance metrics go up? Talk about next steps for the property values post.  Goal 1: Discuss the proposed answer for yesterday’s question: why did the performance metrics go up? The rmse for the base model homes_model is 36595.3198 and the mae is 25784.2432.
The rmse for the improved model homes_model_opt is 40848.44 and the mae is 28394.</description>
    </item>
    
    <item>
      <title>Why did error go up?</title>
      <link>/note/why-did-error-go-up/</link>
      <pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/why-did-error-go-up/</guid>
      <description>Goal’s for today’s session are: Figure out why the RMSE and MSE went up when I chose an “improved” tree.  Goal 1: Figure out why the RMSE and MSE went up when I chose an “improved” tree. The rmse for the base model homes_model is 36182.1117 and the mae is 25797.5518.
The rmse for the improved model homes_model_opt is 40848.44 and the mae is 28394.1.
This shouldn’t be the case, or should it?</description>
    </item>
    
    <item>
      <title>An objective measure</title>
      <link>/note/an-objective-measure/</link>
      <pubDate>Tue, 20 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/an-objective-measure/</guid>
      <description>Goal’s for today’s session are: Evaluate the RMSE and MSE in the base and improved tree.  Goal 1: Evaluate the RMSE and MSE in the base and improved tree. Right. So we know from previous posts that we pick the smallest tree within one standard error of the lowest xerror. Eggsellent. BUT. What can we use to objectively measure the difference between trees? We can use the error between a less pruned tree and the improved one.</description>
    </item>
    
    <item>
      <title>The basics</title>
      <link>/note/the-basics/</link>
      <pubDate>Sat, 17 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/the-basics/</guid>
      <description>Goal’s for today’s session are: Go over the difference between standard deviation, variance, and standard error.
 Evaluate the RMSE and MSE in the base and improved tree.
  Goal 1: Go over the difference between standard deviation, variance, and standard error. So in a few machine learning algorithms, if not all of them, there are three terms that are contantly being used with “mean” and “median”: Standard deviation, variance, and standard error.</description>
    </item>
    
    <item>
      <title>Did the tree improve?</title>
      <link>/note/did-the-tree-improve/</link>
      <pubDate>Fri, 16 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/did-the-tree-improve/</guid>
      <description>Goal’s for today’s session are: Show if the variable change made the tree more interpretable.  #Splitting data # Randomly assign rows to ids (1/2/3 represents train/valid/test) # This will generate a vector of ids of length equal to the number of rows # The train/valid/test split will be approximately 70% / 15% / 15% set.seed(1) assignment &amp;lt;- sample(1:3, size = nrow(homes), prob = c(0.7,0.15,0.15), replace = TRUE) # Create a train, validation and tests from the original data frame homes_train &amp;lt;- homes[assignment == 1, ] # subset the homes data frame to training indices only homes_valid &amp;lt;- homes[assignment == 2, ] # subset the homes data frame to validation indices only homes_test &amp;lt;- homes[assignment == 3, ] # subset the homes data frame to test indices only # Train the model homes_model &amp;lt;- rpart(formula = SalePrice ~ .</description>
    </item>
    
    <item>
      <title>Binning the neighborhood variable (for real now)</title>
      <link>/note/binning-the-neighborhood-variable-for-real-now/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/binning-the-neighborhood-variable-for-real-now/</guid>
      <description>Goal’s for today’s session are: Create a new variable that captures what the variable is providing without so many levels.  Goal 1: Create a new variable that captures what the variable is providing without so many levels. As we saw yesterday from looking at the quantiles, we have 25 neighborhoods but very few houses are in the most expensive neighborhoods.
I’m looking at the plot again and I realise there is something very odd with it.</description>
    </item>
    
    <item>
      <title>Binning the neighborhood variable</title>
      <link>/note/binning-the-neighborhood-variable/</link>
      <pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/binning-the-neighborhood-variable/</guid>
      <description>Goal’s for today’s session are: Understand what information is the neighboorhood variable providing
 Create a new variable that captures what the variable is providing without so many levels.
  Goal 1: Understand what information is the neighboorhood variable providing levels(homes$Neighborhood) Shows that the variable has 25 levels. Let’s see the relationship between each level and the response variable SalesPrice.
Plotting the neighboorhood variable shows:
 Goal 2: Create a new variable that captures what the variable is providing without so many levels.</description>
    </item>
    
    <item>
      <title>Applying concepts learned to property prices</title>
      <link>/note/applying-concepts-learned-to-property-prices/</link>
      <pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/applying-concepts-learned-to-property-prices/</guid>
      <description>Goal’s for today’s session are: Interpret the cptable for the generic homes_model.
 Interpret the rpart.plot associated to homes_model.
 Bin the neighborhood variable and explain why I’m doing that.
  Goal 1: Interpret the cptable for the generic homes_model. With the homes data as is (just removing missing values), I fited a model to predict SalesPrice using rpart():
homes_model &amp;lt;- rpart(formula = SalePrice ~ ., data = homes_train, method = &amp;quot;anova&amp;quot;) Looking at the cptable with homes_model$cptable, I’ll apply the \(1-SE\) rule I explained yesterday:</description>
    </item>
    
    <item>
      <title>Hyperparameter laundry list</title>
      <link>/note/hyperparameter-laundry-list/</link>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/hyperparameter-laundry-list/</guid>
      <description>Goal’s for today’s session are: Get clarity on how to pick the best tree. Make a laundry list of the algorithms hyperparameters.  Goal 1: Get clarity on how to pick the best tree. So far, my confusion stems from the \(1-SE\) (“one standard error” rule) vs. the \(rel\&amp;gt;error\) \(+\) \(xstd\) \(&amp;lt;\) \(xerror\) rule.
These two threads have helped me understand WHY they are the same:
 What’s the deal with this whole 1-SE rule.</description>
    </item>
    
    <item>
      <title>Surrogate who?</title>
      <link>/note/surrogate-who/</link>
      <pubDate>Thu, 08 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/surrogate-who/</guid>
      <description>I hope the authors of the Rpart documentation don’t take it personally but they mention surrogate variables starting on page 11 and they don’t properly define what surrogate variables mean until page 18. Maybe it’s just me.
Anyway! So here is another response to a previous question: what is a surrogate variable?
So the Rpart algorithm deals with missing data. The way it does this is when it’s doing its thing and determining the next variable and split point and comes across an observation that does not have a that variable, it figures out which variables can act as a replacement for that variable.</description>
    </item>
    
    <item>
      <title>Choosing the best tree</title>
      <link>/note/choosing-the-best-tree/</link>
      <pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/choosing-the-best-tree/</guid>
      <description>According to user Harold Ship in this post, we should pick the tree with that keeps this relationship but has the tallest (that’s what he means with the lowest level, since each row represents a tree with more splits):
\(rel\&amp;gt;error\) \(+\) \(xstd\) \(&amp;lt;\) \(xerror\)</description>
    </item>
    
    <item>
      <title>Understanding output tables</title>
      <link>/note/understanding-output-tables/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/understanding-output-tables/</guid>
      <description>I’ve been able to answer quite a few of the questions that I asked before. For example, I don’t have to remove outliers because the algorithm is robust to outliers. No centering and scaling needs to be done to the variables. I can leave factors as factors, but the issue with with a lot of levels is that the tree can get quite wide which reduces its interpretability, which is after all, one of the advantages of using binary trees.</description>
    </item>
    
    <item>
      <title>Memorising tip</title>
      <link>/note/memorising-tip/</link>
      <pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/memorising-tip/</guid>
      <description>Today I went to a poetry competition and heard a useful tip for memorising a poem, or in my case, speeches. Sometimes when we can’t memorise certain part of a text, after trying many times, maybe whatever is written down there, should go on the final version. Memorising can be a tool for editing the final text. When you read the text aloud, it’s own cadence and rhythm will point out that that word shouldn’t be there.</description>
    </item>
    
    <item>
      <title>Pre-processing to what extent?</title>
      <link>/note/pre-processing-to-what-extent/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/pre-processing-to-what-extent/</guid>
      <description>The goal of today’s session was to learn how to preprocess the homes dataset I’ve been working with. The missing values have been filled and now I have a few questions about whether I should perform or not, additional pre-processing steps before I can implement the CART algorithm.
 Should I remove highly correlated variables? I have three and I don’t know what to do with them? Should I remove levels within factor variables with few observations?</description>
    </item>
    
  </channel>
</rss>