<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Notes on Ángela Castillo-Gill</title>
    <link>/note/</link>
    <description>Recent content in Notes on Ángela Castillo-Gill</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Ángela Castillo-Gill</copyright>
    <lastBuildDate>Sat, 17 Nov 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/note/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The basics</title>
      <link>/note/the-basics/</link>
      <pubDate>Sat, 17 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/the-basics/</guid>
      <description>Goal’s for today’s session are: Go over the difference between standard deviation, variance, and standard error.
 Evaluate the RMSE and MSE in the base and improved tree.
  Goal 1: Go over the difference between standard deviation, variance, and standard error. So in a few machine learning algorithms, if not all of them, there are three terms that are contantly being used with “mean” and “median”: Standard deviation, variance, and standard error.</description>
    </item>
    
    <item>
      <title>Did the tree improve?</title>
      <link>/note/did-the-tree-improve/</link>
      <pubDate>Fri, 16 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/did-the-tree-improve/</guid>
      <description>Goal’s for today’s session are: Show if the variable change made the tree more interpretable.  #Splitting data # Randomly assign rows to ids (1/2/3 represents train/valid/test) # This will generate a vector of ids of length equal to the number of rows # The train/valid/test split will be approximately 70% / 15% / 15% set.seed(1) assignment &amp;lt;- sample(1:3, size = nrow(homes), prob = c(0.7,0.15,0.15), replace = TRUE) # Create a train, validation and tests from the original data frame homes_train &amp;lt;- homes[assignment == 1, ] # subset the homes data frame to training indices only homes_valid &amp;lt;- homes[assignment == 2, ] # subset the homes data frame to validation indices only homes_test &amp;lt;- homes[assignment == 3, ] # subset the homes data frame to test indices only # Train the model homes_model &amp;lt;- rpart(formula = SalePrice ~ .</description>
    </item>
    
    <item>
      <title>Binning the neighborhood variable (for real now)</title>
      <link>/note/binning-the-neighborhood-variable-for-real-now/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/binning-the-neighborhood-variable-for-real-now/</guid>
      <description>Goal’s for today’s session are: Create a new variable that captures what the variable is providing without so many levels.  Goal 1: Create a new variable that captures what the variable is providing without so many levels. As we saw yesterday from looking at the quantiles, we have 25 neighborhoods but very few houses are in the most expensive neighborhoods.
I’m looking at the plot again and I realise there is something very odd with it.</description>
    </item>
    
    <item>
      <title>Binning the neighborhood variable</title>
      <link>/note/binning-the-neighborhood-variable/</link>
      <pubDate>Wed, 14 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/binning-the-neighborhood-variable/</guid>
      <description>Goal’s for today’s session are: Understand what information is the neighboorhood variable providing
 Create a new variable that captures what the variable is providing without so many levels.
  Goal 1: Understand what information is the neighboorhood variable providing levels(homes$Neighborhood) Shows that the variable has 25 levels. Let’s see the relationship between each level and the response variable SalesPrice.
Plotting the neighboorhood variable shows:
 Goal 2: Create a new variable that captures what the variable is providing without so many levels.</description>
    </item>
    
    <item>
      <title>Applying concepts learned to property prices</title>
      <link>/note/applying-concepts-learned-to-property-prices/</link>
      <pubDate>Tue, 13 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/applying-concepts-learned-to-property-prices/</guid>
      <description>Goal’s for today’s session are: Interpret the cptable for the generic homes_model.
 Interpret the rpart.plot associated to homes_model.
 Bin the neighborhood variable and explain why I’m doing that.
  Goal 1: Interpret the cptable for the generic homes_model. With the homes data as is (just removing missing values), I fited a model to predict SalesPrice using rpart():
homes_model &amp;lt;- rpart(formula = SalePrice ~ ., data = homes_train, method = &amp;quot;anova&amp;quot;) Looking at the cptable with homes_model$cptable, I’ll apply the \(1-SE\) rule I explained yesterday:</description>
    </item>
    
    <item>
      <title>Hyperparameter laundry list</title>
      <link>/note/hyperparameter-laundry-list/</link>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/hyperparameter-laundry-list/</guid>
      <description>Goal’s for today’s session are: Get clarity on how to pick the best tree. Make a laundry list of the algorithms hyperparameters.  Goal 1: Get clarity on how to pick the best tree. So far, my confusion stems from the \(1-SE\) (“one standard error” rule) vs. the \(rel\&amp;gt;error\) \(+\) \(xstd\) \(&amp;lt;\) \(xerror\) rule.
These two threads have helped me understand WHY they are the same:
 What’s the deal with this whole 1-SE rule.</description>
    </item>
    
    <item>
      <title>Surrogate who?</title>
      <link>/note/surrogate-who/</link>
      <pubDate>Thu, 08 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/surrogate-who/</guid>
      <description>I hope the authors of the Rpart documentation don’t take it personally but they mention surrogate variables starting on page 11 and they don’t properly define what surrogate variables mean until page 18. Maybe it’s just me.
Anyway! So here is another response to a previous question: what is a surrogate variable?
So the Rpart algorithm deals with missing data. The way it does this is when it’s doing its thing and determining the next variable and split point and comes across an observation that does not have a that variable, it figures out which variables can act as a replacement for that variable.</description>
    </item>
    
    <item>
      <title>Choosing the best tree</title>
      <link>/note/choosing-the-best-tree/</link>
      <pubDate>Wed, 07 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/choosing-the-best-tree/</guid>
      <description>According to user Harold Ship in this post, we should pick the tree with that keeps this relationship but has the tallest (that’s what he means with the lowest level, since each row represents a tree with more splits):
\(rel\&amp;gt;error\) \(+\) \(xstd\) \(&amp;lt;\) \(xerror\)</description>
    </item>
    
    <item>
      <title>Understanding output tables</title>
      <link>/note/understanding-output-tables/</link>
      <pubDate>Mon, 05 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/understanding-output-tables/</guid>
      <description>I’ve been able to answer quite a few of the questions that I asked before. For example, I don’t have to remove outliers because the algorithm is robust to outliers. No centering and scaling needs to be done to the variables. I can leave factors as factors, but the issue with with a lot of levels is that the tree can get quite wide which reduces its interpretability, which is after all, one of the advantages of using binary trees.</description>
    </item>
    
    <item>
      <title>Memorising tip</title>
      <link>/note/memorising-tip/</link>
      <pubDate>Sun, 04 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/memorising-tip/</guid>
      <description>Today I went to a poetry competition and heard a useful tip for memorising a poem, or in my case, speeches. Sometimes when we can’t memorise certain part of a text, after trying many times, maybe whatever is written down there, should go on the final version. Memorising can be a tool for editing the final text. When you read the text aloud, it’s own cadence and rhythm will point out that that word shouldn’t be there.</description>
    </item>
    
    <item>
      <title>Pre-processing to what extent?</title>
      <link>/note/pre-processing-to-what-extent/</link>
      <pubDate>Sat, 03 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/note/pre-processing-to-what-extent/</guid>
      <description>The goal of today’s session was to learn how to preprocess the homes dataset I’ve been working with. The missing values have been filled and now I have a few questions about whether I should perform or not, additional pre-processing steps before I can implement the CART algorithm.
 Should I remove highly correlated variables? I have three and I don’t know what to do with them? Should I remove levels within factor variables with few observations?</description>
    </item>
    
  </channel>
</rss>